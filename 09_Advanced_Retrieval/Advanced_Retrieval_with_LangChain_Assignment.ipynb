{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issues with student loans appear to involve problems with loan servicing, such as errors in loan balances, misapplied payments, wrongful denials or issues with payment plans, and difficulties with how payments are being handled. Many complaints also relate to inaccurate or conflicting information on credit reports, unnotified transfer of loans between servicers, and mishandling of loan data, including violations of privacy laws. \\n\\nIn summary, the most common issue is complications or errors related to the management and servicing of student loans, which can include incorrect balances, misapplied payments, and poor communication from loan servicers.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, some complaints did not get handled in a timely manner. Specifically, the complaint from a consumer regarding their student loan application status (Complaint ID: 12709087 from MOHELA) was marked as \"No\" for timely response, indicating it was not handled promptly. Additionally, several other complaints mention delays or lack of response, such as the complaint from a consumer about their account being unprocessed for over 18 months and the complaint about a dispute request not being addressed after over 2-3 weeks.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of clear information and communication: Borrowers were often not adequately informed about when repayment would resume, loan transfer details, or changes in loan servicers, leading to confusion and missed payments.\\n\\n2. Unmanageable interest accumulation: Many borrowers mentioned that interest continued to accrue during deferment or forbearance periods, negating any payments made and making it difficult to reduce the principal amount.\\n\\n3. Financial hardships: Borrowers faced difficulties affording payments while managing day-to-day expenses, especially when options like increased payments would jeopardize their basic needs.\\n\\n4. Limited or unfavorable repayment options: Some borrowers reported that available options such as forbearance or deferment extended the repayment period and increased overall debt due to accumulated interest.\\n\\n5. Poor or confusing service management: Issues such as unnotified loan transfers, incorrect or inconsistent loan account information, and problematic payment application processes contributed to missed or late payments.\\n\\n6. Lack of eligibility for forgiveness programs: Many borrowers did not qualify for forgiveness programs like PSLF or TLF, leaving them with unmanageable debt that they struggled to repay.\\n\\n7. Administrative errors and mismanagement: Some borrowers experienced errors such as incorrect billing, failure to notify about delinquency, or mishandling of their accounts, which hindered timely repayment.\\n\\nOverall, these factors contributed to the inability of many borrowers to meet their repayment obligations, often due to systemic issues, miscommunication, and financial constraints rather than irresponsibility.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issue with loans appears to be problems related to dealing with lenders or servicers, specifically issues such as inaccurate or bad information about loans, difficulties in applying payments correctly, and disputes over fees or loan terms. These issues often involve lack of transparency, miscommunication, or alleged unfair practices by the loan servicers.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, yes, some complaints did not get handled in a timely manner. For example, the complaint from the individual regarding issues with their student loan, which they have been working on for several years, indicates ongoing unresolved problems. Additionally, one complaint specifies that the organization failed to respond or actioned the issue correctly, and in one case, the customer had to wait over several minutes with no answer before hanging up, indicating delays or failure to respond promptly.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including issues with the handling of their payment plans, miscommunication or lack of communication from loan servicers, and problems with the loan transfer process. For example, some borrowers experienced being unenrolled from autopay without their knowledge, leading to missed payments and negative impacts on their credit scores. Others faced difficulties with payment reversals or were steered into improper forbearances, which caused their loan balances to increase due to capitalized interest. Additionally, some borrowers did not receive timely responses or clear information from their loan servicers regarding their repayment status or options, which contributed to their inability to repay effectively.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "**Example Query: \"Why did people fail to pay back their loans?\"**\n",
        "\n",
        "Based on the invocations in this notebook, BM25 performs better than embeddings for this query.\n",
        "\n",
        "**Comparison of responses:**\n",
        "\n",
        "**Naive Retrieval (Embeddings):** Provided a broad, conceptual response covering systemic issues, miscommunication, and financial constraints but was more general in nature.\n",
        "\n",
        "**BM25 Retrieval:** Delivered more specific, actionable details including:\n",
        "- \"unenrolled from autopay without their knowledge\"\n",
        "- \"payment reversals\" \n",
        "- \"steered into improper forbearances\"\n",
        "- \"capitalized interest\"\n",
        "- \"loan transfer process\"\n",
        "\n",
        "**Why BM25 is better here:**\n",
        "\n",
        "1. **Exact Term Matching**: BM25 excels at finding documents containing specific financial and procedural terminology like \"autopay\", \"forbearances\", \"capitalized interest\" - terms that are crucial for understanding concrete loan servicing problems.\n",
        "\n",
        "2. **Factual Precision**: The query asks for specific reasons why payments failed. BM25's keyword-based approach captures precise procedural failures and technical issues that directly answer the \"why\" question.\n",
        "\n",
        "3. **Domain-Specific Language**: In financial/loan contexts, exact terminology matters immensely. BM25's ability to match specific loan servicing terms provides more actionable and legally/procedurally accurate information than semantic similarity alone.\n",
        "\n",
        "4. **Reduced Semantic Drift**: Embeddings might retrieve conceptually similar but less precise content, whereas BM25 stays focused on documents containing the exact operational terms that explain payment failures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, such as receiving bad information about the loan, errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling or mishandling of loan data. A recurring theme is the difficulty consumers experience in obtaining accurate, transparent information and resolving discrepancies, which can lead to disputes, errors, and issues with their credit reports.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided information, yes, there are complaints that did not get handled in a timely manner. Specifically, the complaint involving the student loan issue (Complaint ID: 12975634) indicates that it has been nearly 18 months with no resolution, despite the consumer's repeated requests for response and resolution. The complaint also mentions delays lasting over a year and the recipient expressing that the issue has been open since an unspecified date without resolution.\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily due to a combination of factors including lack of clear communication and understanding about their loan obligations, administrative errors, and the accumulation of interest. Specifically, some borrowers were unaware that they were required to repay their loans because they were not properly informed by financial aid officers or loan servicers. Others experienced unnotified transfers or buyouts of their loans without their knowledge, leading to confusion and missed payments. Additionally, borrowers faced difficulties with the handling of their accounts, incorrect or inconsistent account information, and limited options for manageable repayment strategies. Interest accumulation during deferment or forbearance further complicated repayment, often causing balances to grow even if payments were made, making it harder for borrowers to pay off their loans. Overall, misinformation, poor communication, administrative mishandling, and the complex nature of interest and repayment options contributed to many borrowers' inability to successfully pay back their loans.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issue with loans appears to be problems related to \"Dealing with your lender or servicer,\" which includes sub-issues such as trouble with how payments are being handled, receiving bad information about the loan, problems with loan balances, misapplied payments, wrongful denials of payment plans, and issues with loan transfers or misclassification. Many complaints also mention the difficulty in obtaining accurate information, unauthorized changes to accounts, and inadequate communication from loan servicers.\\n\\nThe recurring themes indicate that the most prevalent issues involve:\\n- Mismanagement or errors in loan balances and interest calculations\\n- Lack of proper communication or notices regarding loan status or transfer\\n- Incorrect or misleading information given by servicers\\n- Problems with repayment plans, including wrongful denials or improper handling\\n- Transfer of loans between servicers without notice\\n- Errors in credit reporting related to loans\\n\\nTherefore, the most common issue with loans, based on the data, is **problems and mismanagement in dealing with lenders or servicers, especially concerning payments, loan information, and communication.**'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, some complaints did not get handled in a timely manner. Specifically, there is at least one complaint where the response was marked as \"No\" for timely response, indicating the complaint was not addressed promptly. For example, the complaint with Complaint ID 12709087 regarding a delayed application process highlighted that the consumer had not heard from anyone despite waiting several weeks, which suggests a failure to handle the issue in a timely manner.\\n\\nAdditionally, in multiple complaints, consumers expressed frustration over delays, lack of responses, or issues remaining unresolved for extended periods, such as over a year in some cases.\\n\\nTherefore, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily because of a combination of factors such as unmanageable interest accrual, limited or misleading information about repayment options, and inadequate communication from loan servicers. Many borrowers found themselves in difficult financial situations where lowering monthly payments due to financial hardship would result in increased interest and longer repayment periods, making it seemingly impossible to pay off the loans. Others were misled about their repayment obligations, not being properly informed about options like income-driven repayment plans or loan forgiveness programs, which could have alleviated their burden. Additionally, some borrowers experienced issues with incorrect or inconsistent loan information, surprise transfers between servicers, and lack of proper notice, all of which contributed to difficulties in managing and repaying their loans. Overall, systemic issues such as poor communication, lack of transparency, and mismanagement by loan servicers played a significant role in why many people struggled to repay their loans.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "Generating multiple reformulations of a user query can significantly improve recall through several mechanisms:\n",
        "\n",
        "**1. Vocabulary Mismatch Reduction:**\n",
        "- Users and document authors often use different words to describe the same concepts\n",
        "- Multiple reformulations increase the likelihood of matching the exact terminology used in relevant documents\n",
        "- Example: A user asking \"payment issues\" might miss documents that use \"billing problems\" or \"transaction difficulties\"\n",
        "\n",
        "**2. Query Perspective Diversification:**\n",
        "- Different reformulations can approach the same topic from various angles\n",
        "- Each perspective might surface different relevant documents that focus on specific aspects\n",
        "- Example: \"Why did loans fail?\" vs \"What caused borrower defaults?\" vs \"Reasons for payment problems\"\n",
        "\n",
        "**3. Semantic Coverage Expansion:**\n",
        "- LLMs can generate reformulations that capture different semantic nuances of the original query\n",
        "- This helps retrieve documents that are conceptually relevant but use different language patterns\n",
        "- Increases the semantic search space beyond the original query's limited scope\n",
        "\n",
        "**4. Synonym and Paraphrase Utilization:**\n",
        "- Reformulations naturally incorporate synonyms and paraphrases\n",
        "- Documents using alternative terminology become discoverable\n",
        "- Reduces dependency on exact keyword matches\n",
        "\n",
        "**5. Comprehensive Document Retrieval:**\n",
        "- By retrieving documents for each reformulated query and taking the union of all results\n",
        "- The final context includes a broader set of potentially relevant documents\n",
        "- Higher chance of including the most relevant information that might have been missed by a single query\n",
        "\n",
        "**Implementation in Multi-Query Retriever:**\n",
        "The Multi-Query Retriever demonstrates this by:\n",
        "1. Using an LLM to generate multiple query variations\n",
        "2. Running retrieval for each variation\n",
        "3. Combining all unique documents into the final context\n",
        "4. Providing richer, more comprehensive information for answer generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issues with loans, based on the provided complaints, appear to be related to the following:\\n\\n1. Incorrect information on credit reports, affecting credit scores and report accuracy.\\n2. Discrepancies and errors in loan balances, interest rates, and account statuses.\\n3. Misapplication of payments and wrongful denial of payment plans.\\n4. Issues with loan servicing, including errors caused by loan transfers and misunderstandings regarding loan terms or legitimacy.\\n5. Problems with debt collection and reporting, including unverified debts and illegal practices.\\n\\nThese problems highlight systemic challenges in loan management, servicing, and reporting processes.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, all the complaints listed were marked as \"Timely response?\": \"No\" for the first two complaints (rows 441 and 84), indicating that they did not get handled in a timely manner. The third complaint (row 418) was marked as \"Yes,\" meaning it was handled in a timely manner. The fourth complaint (row 474) also was handled timely.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner, specifically the complaints with IDs 12709087 and 12935889.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a variety of reasons such as experiencing financial hardship, being misled about the manageability and long-term consequences of their loans, lack of proper information or notification about payment requirements, and the inability to find employment or achieve career outcomes that would allow them to repay their debts. Additionally, issues like mismanagement by loan servicers, insufficient communication, and challenges related to loan forgiveness or discharge processes also contributed to the difficulty in repayment.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints and data, the most common issue with loans appears to be problems related to mismanagement and poor communication by loan servicers, including errors in loan balances, misapplied payments, wrongful denials of repayment plans, and lack of proper notifications. Many complaints mention inaccuracies in loan balances, interest calculations, and account statuses, along with issues like loans being transferred without notice, incorrect reporting to credit bureaus, and difficulties in getting timely or accurate information from servicers.\\n\\nIn summary, the most common issues involve:\\n- Errors or discrepancies in loan balances and interest calculations\\n- Lack of clear communication or notifications about loan status or changes\\n- Mismanagement during loan transfers\\n- Unfair or incorrect reporting on credit reports\\n- Challenges in obtaining accurate information or corrections from servicers\\n\\nIf you need specific details or further assistance, feel free to ask!'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, several complaints indicate delays or lack of timely handling:\\n\\n- Complaint ID 12709087 (MOHELA, 03/28/25): The complaint notes that the response to the issue was \"No,\" and the response was \"Closed with explanation,\" which indicates that the complaint was not handled in a timely manner. The complaint explicitly states it was \"not timely,\" with no reply after multiple attempts over weeks.\\n\\n- Complaint ID 12935889 (Maximus/Aidvantage, 04/11/25): The complaint states that the response was \"No,\" and the response was \"Closed with explanation,\" and it was \"not timely.\" The complaint mentions unacceptable wait times (over 4 hours), and the response was overdue, indicating it was not handled in a timely manner.\\n\\n- Other complaints (e.g., complaints about disputes, credit reporting errors, and issues with adjustments) often mention ongoing unresolved issues, delayed responses, or failure to respond, suggesting delays in handling.\\n\\nIn contrast, some complaints, such as ID 12965199 (Maximus/Aidvantage, 04/11/25), note that the response was \"Yes\" (timely). However, many complaints explicitly state delays, complaints not being addressed promptly, or being marked as \"not timely.\"\\n\\n**Conclusion:** Yes, there are complaints that did not get handled in a timely manner. Multiple complaints explicitly cite delays or failure to respond within the expected timeframe, confirming that some complaints were not handled promptly.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to several interconnected reasons highlighted in the complaints:\\n\\n1. **Lack of Clear Information and Communication:** Borrowers often were not adequately informed about repayment requirements, delinquency notices, or changes in their loan status. This led to unexpected missed payments and credit reporting errors.\\n\\n2. **Disputes Over Loan Transfer and Management:** Many borrowers experienced unnotified transfers of their loans between servicers (e.g., from Great Lakes to Nelnet or Navient to Aidvantage), which caused confusion and missed communication about when payments were due.\\n\\n3. **Interest Accumulation and Capitalization:** Borrowers reported that while in forbearance or deferment, interest continued to accrue and capitalize, increasing the total amount owed and making it more difficult to pay off the loans.\\n\\n4. **Inadequate Support and Mismanagement:** Several complaints cited unhelpful or dismissive responses from loan servicers, failure to provide proper guidance on repayment options (like income-driven plans), or wrongful reporting of delinquency, all contributing to financial hardship.\\n\\n5. **Economic Hardship and Misleading Expectations:** Many borrowers took loans assuming manageable payments based on their expected income, but stagnant wages, unemployment, or unexpected expenses (medical issues, homelessness, etc.) made repayment impossible or impractical.\\n\\n6. **Unfair Practices and Errors:** Issues such as incorrect reporting of loan status (e.g., reporting loans as delinquent when they were current), wrongful default notices, and withholding information on forgiveness or flexible repayment options further prevented borrowers from repaying their loans.\\n\\n7. **Procedural Failures:** In many cases, servicers failed to notify borrowers about late payments, default statuses, or collection efforts, sometimes violating regulations (like 34 CFR 682.211), which led to credit score drops and increased financial burdens.\\n\\nIn summary, failure to repay was often due to a combination of systemic mismanagement, poor communication, interest growth during hardship periods, and economic difficulties faced by borrowers, compounded by unfair practices by loan servicers.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with loans appear to be related to borrower frustrations with loan servicing and reporting. Specifically, frequent issues include:\\n\\n- Problems with repayment and payment plans (e.g., unexpected increases, miscalculations, or delays in processing payments)\\n- Errors or discrepancies in credit reporting (e.g., accounts being reported incorrectly, default statuses, or unauthorized collections)\\n- Lack of transparency and communication from loan servicers\\n- Disputes over loan balances, account status, or data breaches\\n- Allegations of improper use or unauthorized access to personal information\\n\\nOverall, issues with loan servicingâ€”such as mismanagement of payments, inaccurate reporting, and poor communicationâ€”seem to be the most common and prominent problems noted in these complaints.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that several complaints were marked as \"Closed with explanation\" and, in some cases, specifically noted as \"timely response.\" However, there is at least one complaint where the consumer indicated their issue was not handled in a timely manner: the complaint involving Nelnet\\'s transfer of accounts and failure to respond to certified mail, which noted serious misconduct but was still marked as \"Closed with explanation.\" While the response was timely, the claim suggests ongoing issues and possible delays or inadequate responses.\\n\\nOverall, most complaints indicate timely responses (\"Yes\"), but the issues raisedâ€”particularly the one about Nelnet\\'s misconduct and lack of response to certified mailâ€”imply that not all complaints may have been fully or adequately handled in a timely manner according to the consumer\\'s perspective.\\n\\nTherefore, I cannot definitively say that *no* complaints were handled tardily, but based on the data, several complaints did receive responses within the expected time frame, with some indicating unresolved issues. If you are referring to the specific complaint about Nelnet\\'s misconduct, it appears that that complaint was responded to in a timely manner but may still not have been fully resolved.\\n\\nIf you need a precise answer, my conclusion is: **Most complaints were marked as handled in a timely manner, but there is at least one complaint implying issues with response timeliness or adequacy.**'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including administrative issues, miscommunication, technical problems, alleged illegal or improper reporting, and disputes over the legitimacy or status of their loans. Some borrowers experienced difficulty with the handling of their payments, such as payments not clearing or being rejected, or misunderstandings about their loan status, such as being reported in default despite never having defaulted. Others faced issues with loan documentation, delays, or alleged stalling tactics by loan servicers. Some disputes also stemmed from concerns over improper reporting, data breaches, or the legal validity of their debts.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "**How Semantic Chunking Behaves with Short, Repetitive Sentences:**\n",
        "\n",
        "**1. Problematic Similarity Patterns:**\n",
        "- Short, repetitive sentences (like FAQ questions) often have very high semantic similarity scores\n",
        "- The algorithm may struggle to find meaningful breakpoints between semantically similar content\n",
        "- Could result in either overly large chunks (everything grouped together) or overly small chunks (no grouping occurs)\n",
        "\n",
        "**2. Threshold Sensitivity Issues:**\n",
        "- With highly similar content, small variations in similarity scores become less meaningful\n",
        "- Percentile-based thresholding may not work effectively when most distances are very close\n",
        "- The algorithm might either chunk everything together or keep everything separate\n",
        "\n",
        "**3. Loss of Logical Structure:**\n",
        "- FAQs have inherent question-answer pairs that should ideally stay together\n",
        "- Semantic chunking might break these logical pairs if focusing purely on sentence-level similarity\n",
        "- Context and structure information gets lost in favor of semantic similarity\n",
        "\n",
        "**Adjustments to Improve Performance:**\n",
        "\n",
        "**1. Threshold Method Modifications:**\n",
        "```python\n",
        "# Use more aggressive thresholding\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"standard_deviation\",  # More sensitive to small differences\n",
        "    breakpoint_threshold_amount=1.0  # Lower threshold for more breaks\n",
        ")\n",
        "```\n",
        "\n",
        "**2. Pre-processing Strategies:**\n",
        "- Identify FAQ structure patterns (Q: A: formatting)\n",
        "- Use regex to detect question-answer pairs\n",
        "- Apply metadata-based chunking to preserve logical pairs\n",
        "\n",
        "**3. Hybrid Approaches:**\n",
        "```python\n",
        "# Combine semantic chunking with structural rules\n",
        "- First chunk by logical structure (Q-A pairs)\n",
        "- Then apply semantic chunking within those logical boundaries\n",
        "- Use custom splitting logic for FAQ-specific patterns\n",
        "```\n",
        "\n",
        "**4. Alternative Chunking Methods:**\n",
        "- **Fixed-size chunking** with Q-A pair preservation\n",
        "- **Metadata-based chunking** using FAQ structure\n",
        "- **Custom splitters** that understand FAQ formatting\n",
        "\n",
        "**5. Enhanced Similarity Calculation:**\n",
        "- Use more sophisticated embedding models that better capture subtle differences\n",
        "- Apply domain-specific embeddings trained on FAQ data\n",
        "- Consider using multiple embedding dimensions for better differentiation\n",
        "\n",
        "**Recommended Implementation for FAQs:**\n",
        "Rather than relying purely on semantic chunking, use a structured approach that preserves the logical Q-A relationships while still benefiting from semantic organization for related topics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import Required Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import uuid\n",
        "from datasets import Dataset\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "\n",
        "# Import Ragas components with fallback for version compatibility\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    faithfulness,\n",
        "    answer_relevancy\n",
        ")\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ—ï¸ Activity #1: Evaluating Retriever Methods with Ragas + LangSmith\n",
            "======================================================================\n",
            "âš ï¸ Ragas evolutions not available - will use basic testset generation\n",
            "âœ… LangSmith imported successfully\n",
            "âœ… Libraries installation and import completed!\n",
            "\n",
            "ðŸ”— Setting up LangSmith for Cost & Latency Tracking\n",
            "-------------------------------------------------------\n",
            "ðŸ“Š LangSmith project: advanced-retrieval-eval-ed7313e6\n",
            "âœ… LangSmith integration enabled for cost/latency tracking\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ovookpubuluku/project-repos/ai-makerspace/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Activity #1: Comprehensive Retriever Evaluation with Ragas + LangSmith\n",
        "# ========================================================================\n",
        "\n",
        "print(\"ðŸ—ï¸ Activity #1: Evaluating Retriever Methods with Ragas + LangSmith\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Set Ragas availability to True since we have working imports from Cell 94\n",
        "ragas_available = True\n",
        "print(\"âœ… Ragas available from previous imports\")\n",
        "\n",
        "# Try different import paths for Ragas evolutions (API has changed in recent versions)\n",
        "evolutions_available = False\n",
        "try:\n",
        "    # Try newer API first\n",
        "    from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "    evolutions_available = True\n",
        "    print(\"âœ… Ragas evolutions imported (new API)\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        # Try alternative import path\n",
        "        from ragas.testset.synthesizers import simple, reasoning, multi_context\n",
        "        evolutions_available = True\n",
        "        print(\"âœ… Ragas evolutions imported (alternative API)\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ Ragas evolutions not available - will use basic testset generation\")\n",
        "        evolutions_available = False\n",
        "\n",
        "# LangSmith imports - corrected based on documentation\n",
        "try:\n",
        "    from langsmith import Client\n",
        "    from langsmith import traceable\n",
        "    from langsmith.wrappers import wrap_openai\n",
        "    langsmith_available = True\n",
        "    print(\"âœ… LangSmith imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸ LangSmith import error: {e}\")\n",
        "    langsmith_available = False\n",
        "\n",
        "print(\"âœ… Libraries installation and import completed!\")\n",
        "\n",
        "# Step 1.5: Setup LangSmith Integration\n",
        "print(\"\\nðŸ”— Setting up LangSmith for Cost & Latency Tracking\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "if langsmith_available:\n",
        "    try:\n",
        "        # Get LangSmith API key\n",
        "        import os\n",
        "        import getpass\n",
        "        \n",
        "        if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "            os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
        "        \n",
        "        # Initialize LangSmith client\n",
        "        langsmith_client = Client()\n",
        "        \n",
        "        # Create a unique project name for this evaluation\n",
        "        project_name = f\"advanced-retrieval-eval-{uuid.uuid4().hex[:8]}\"\n",
        "        print(f\"ðŸ“Š LangSmith project: {project_name}\")\n",
        "        \n",
        "        # Set environment variables for automatic tracing\n",
        "        os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n",
        "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "        \n",
        "        langsmith_enabled = True\n",
        "        print(\"âœ… LangSmith integration enabled for cost/latency tracking\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ LangSmith setup failed: {e}\")\n",
        "        print(\"ðŸ“Š Falling back to manual latency tracking\")\n",
        "        langsmith_enabled = False\n",
        "else:\n",
        "    print(\"âš ï¸ LangSmith not available - using manual latency tracking\")\n",
        "    langsmith_enabled = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Step 2: Generating Synthetic Test Dataset\n",
            "--------------------------------------------------\n",
            "Attempting to generate synthetic test data from 50 documents...\n",
            "âš ï¸ TestsetGenerator.with_openai() failed: type object 'TestsetGenerator' has no attribute 'with_openai'\n",
            "âœ… Generator initialized with manual LLM setup\n",
            "ðŸ”„ Using basic testset generation (no evolutions)...\n",
            "âš ï¸ Error generating synthetic data: TestsetGenerator.generate_with_langchain_docs() got an unexpected keyword argument 'test_size'. Did you mean 'testset_size'?\n",
            "Using fallback manual test questions...\n",
            "ðŸ”„ Creating manual test questions...\n",
            "âœ… Created 5 manual test questions\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Generate Synthetic Test Dataset using Ragas\n",
        "# ====================================================\n",
        "\n",
        "print(\"\\nðŸ“Š Step 2: Generating Synthetic Test Dataset\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if ragas_available:\n",
        "    try:\n",
        "        # Create a subset of documents for test generation\n",
        "        test_docs = loan_complaint_data[:50]  # Using first 50 documents for faster generation\n",
        "        \n",
        "        print(f\"Attempting to generate synthetic test data from {len(test_docs)} documents...\")\n",
        "        \n",
        "        # Initialize the test generator using the working pattern from Cell 94\n",
        "        try:\n",
        "            # Use the generator_llm and generator_embeddings from Cell 94\n",
        "            generator = TestsetGenerator(\n",
        "                llm=generator_llm,\n",
        "                embedding_model=generator_embeddings\n",
        "            )\n",
        "            generator_available = True\n",
        "            print(\"âœ… Generator initialized using LangChain wrappers from Cell 94\")\n",
        "        except Exception as generator_error:\n",
        "            print(f\"âš ï¸ TestsetGenerator initialization failed: {generator_error}\")\n",
        "            generator_available = False\n",
        "        \n",
        "        if generator_available:\n",
        "            # Generate test dataset with or without evolutions\n",
        "            if evolutions_available:\n",
        "                # Define distribution for different question types\n",
        "                distributions = {\n",
        "                    simple: 0.4,        # 40% simple questions\n",
        "                    multi_context: 0.3, # 30% multi-context questions  \n",
        "                    reasoning: 0.3      # 30% reasoning questions\n",
        "                }\n",
        "                \n",
        "                print(\"ðŸ”„ Using advanced question type distributions...\")\n",
        "                testset = generator.generate_with_langchain_docs(\n",
        "                    test_docs, \n",
        "                    testset_size=10,  # Generate 10 test questions for faster execution\n",
        "                    distributions=distributions\n",
        "                )\n",
        "            else:\n",
        "                print(\"ðŸ”„ Using basic testset generation (no evolutions)...\")\n",
        "                testset = generator.generate_with_langchain_docs(\n",
        "                    test_docs, \n",
        "                    testset_size=10  # Generate 10 test questions\n",
        "                )\n",
        "            \n",
        "            # Convert to pandas DataFrame for easier handling\n",
        "            test_df = testset.to_pandas()\n",
        "            \n",
        "            print(f\"âœ… Generated {len(test_df)} synthetic test questions\")\n",
        "            print(f\"Dataset columns: {list(test_df.columns)}\")\n",
        "            \n",
        "            # Display first few examples\n",
        "            print(\"\\nðŸ“‹ Sample Test Questions:\")\n",
        "            for i, row in test_df.head(3).iterrows():\n",
        "                print(f\"\\nQuestion {i+1}: {row['question']}\")\n",
        "                if 'ground_truth' in row and pd.notna(row['ground_truth']):\n",
        "                    print(f\"Ground Truth: {row['ground_truth'][:100]}...\")\n",
        "                elif 'reference' in row and pd.notna(row['reference']):\n",
        "                    print(f\"Reference: {row['reference'][:100]}...\")\n",
        "        else:\n",
        "            raise Exception(\"Could not initialize TestsetGenerator\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error generating synthetic data: {e}\")\n",
        "        print(\"Using fallback manual test questions...\")\n",
        "        ragas_available = False\n",
        "\n",
        "if not ragas_available:\n",
        "    print(\"ðŸ”„ Creating manual test questions...\")\n",
        "    \n",
        "    # Fallback: Create manual test questions\n",
        "    manual_questions = [\n",
        "        \"What are the most common issues with student loans?\",\n",
        "        \"How do loan servicers handle payment processing problems?\", \n",
        "        \"What causes borrowers to default on their loans?\",\n",
        "        \"How often do companies respond to complaints in a timely manner?\",\n",
        "        \"What are the main problems with credit reporting for loans?\"\n",
        "    ]\n",
        "    \n",
        "    manual_ground_truths = [\n",
        "        \"Common student loan issues include payment processing problems, incorrect balances, poor communication from servicers, and difficulties with loan transfers.\",\n",
        "        \"Loan servicers often mishandle payments through incorrect application, processing delays, and poor communication about payment status.\",\n",
        "        \"Borrowers default due to financial hardship, lack of information about repayment options, poor servicer communication, and interest capitalization during forbearance.\",\n",
        "        \"Many companies fail to respond to complaints in a timely manner, with several complaints marked as 'No' for timely response.\",\n",
        "        \"Credit reporting problems include incorrect loan status, unauthorized collections, and failure to update account information properly.\"\n",
        "    ]\n",
        "    \n",
        "    test_df = pd.DataFrame({\n",
        "        'question': manual_questions,\n",
        "        'ground_truth': manual_ground_truths\n",
        "    })\n",
        "    \n",
        "    print(f\"âœ… Created {len(test_df)} manual test questions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Set up Retriever Evaluation Framework\n",
        "# ===============================================\n",
        "\n",
        "print(\"\\nðŸ”„ Step 3: Setting up Retriever Evaluation Framework\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "# Define all retrievers to evaluate\n",
        "retrievers_to_evaluate = {\n",
        "    \"Naive (Embedding)\": naive_retriever,\n",
        "    \"BM25\": bm25_retriever,\n",
        "    \"Multi-Query\": multi_query_retriever,\n",
        "    \"Parent Document\": parent_document_retriever,\n",
        "    \"Contextual Compression\": compression_retriever,\n",
        "    \"Ensemble\": ensemble_retriever\n",
        "}\n",
        "\n",
        "# Define RAG chains for each retriever\n",
        "rag_chains = {\n",
        "    \"Naive (Embedding)\": naive_retrieval_chain,\n",
        "    \"BM25\": bm25_retrieval_chain,\n",
        "    \"Multi-Query\": multi_query_retrieval_chain,\n",
        "    \"Parent Document\": parent_document_retrieval_chain,\n",
        "    \"Contextual Compression\": contextual_compression_retrieval_chain,\n",
        "    \"Ensemble\": ensemble_retrieval_chain\n",
        "}\n",
        "\n",
        "print(f\"ðŸ“‹ Retrievers to evaluate: {list(retrievers_to_evaluate.keys())}\")\n",
        "\n",
        "def evaluate_retriever_performance(retriever_name, retriever, rag_chain, test_questions, ground_truths):\n",
        "    \"\"\"\n",
        "    Evaluate a single retriever using Ragas metrics with LangSmith cost/latency tracking\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ” Evaluating: {retriever_name}\")\n",
        "    \n",
        "    # Track performance metrics with LangSmith if available\n",
        "    answers = []\n",
        "    contexts = []\n",
        "    langsmith_runs = []\n",
        "    total_cost = 0.0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Generate answers and collect contexts for each question\n",
        "    for i, question in enumerate(test_questions):\n",
        "        try:\n",
        "            if langsmith_enabled:\n",
        "                # Create a unique run name for LangSmith tracking\n",
        "                run_name = f\"{retriever_name}_Q{i+1}\"\n",
        "                \n",
        "                # Use traceable decorator approach for LangSmith tracking\n",
        "                @traceable(name=run_name, tags=[retriever_name, \"evaluation\"])\n",
        "                def run_chain(question):\n",
        "                    return rag_chain.invoke({\"question\": question})\n",
        "                \n",
        "                result = run_chain(question)\n",
        "            else:\n",
        "                result = rag_chain.invoke({\"question\": question})\n",
        "            \n",
        "            # Extract answer content\n",
        "            if hasattr(result[\"response\"], 'content'):\n",
        "                answers.append(result[\"response\"].content)\n",
        "            else:\n",
        "                answers.append(str(result[\"response\"]))\n",
        "            \n",
        "            # Extract context from the result\n",
        "            if \"context\" in result:\n",
        "                if isinstance(result[\"context\"], list):\n",
        "                    # Handle case where context is a list of documents\n",
        "                    context_texts = []\n",
        "                    for doc in result[\"context\"]:\n",
        "                        if hasattr(doc, 'page_content'):\n",
        "                            context_texts.append(doc.page_content)\n",
        "                        else:\n",
        "                            context_texts.append(str(doc))\n",
        "                    contexts.append(context_texts)\n",
        "                else:\n",
        "                    contexts.append([str(result[\"context\"])])\n",
        "            else:\n",
        "                # Fallback: get context directly from retriever\n",
        "                try:\n",
        "                    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "                    context_texts = [doc.page_content for doc in retrieved_docs]\n",
        "                    contexts.append(context_texts)\n",
        "                except:\n",
        "                    # Final fallback\n",
        "                    contexts.append([\"No context available\"])\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error processing question {i+1}: {e}\")\n",
        "            answers.append(\"Error generating response\")\n",
        "            contexts.append([\"Error retrieving context\"])\n",
        "    \n",
        "    # Calculate latency\n",
        "    end_time = time.time()\n",
        "    avg_latency = (end_time - start_time) / len(test_questions)\n",
        "    \n",
        "    # Initialize cost tracking\n",
        "    langsmith_cost = 0.0\n",
        "    langsmith_latency = avg_latency\n",
        "    \n",
        "    # Try to get LangSmith cost and latency data if available\n",
        "    if langsmith_enabled:\n",
        "        try:\n",
        "            # Get runs from LangSmith project to extract cost and latency data\n",
        "            project_runs = list(langsmith_client.list_runs(\n",
        "                project_name=project_name,\n",
        "                limit=100\n",
        "            ))\n",
        "            \n",
        "            # Filter runs for this retriever (recent runs)\n",
        "            retriever_runs = [r for r in project_runs if retriever_name in str(getattr(r, 'tags', []))][-len(test_questions):]\n",
        "            \n",
        "            if retriever_runs:\n",
        "                # Calculate total cost from LangSmith data\n",
        "                costs = [getattr(run, 'total_cost', 0.0) or 0.0 for run in retriever_runs]\n",
        "                total_cost = sum(costs)\n",
        "                \n",
        "                # Calculate average latency from LangSmith data  \n",
        "                latencies = []\n",
        "                for run in retriever_runs:\n",
        "                    if hasattr(run, 'end_time') and hasattr(run, 'start_time') and run.end_time and run.start_time:\n",
        "                        latency = (run.end_time - run.start_time).total_seconds()\n",
        "                        latencies.append(latency)\n",
        "                \n",
        "                if latencies:\n",
        "                    langsmith_latency = sum(latencies) / len(latencies)\n",
        "                \n",
        "                langsmith_cost = total_cost / len(test_questions) if len(test_questions) > 0 else 0.0\n",
        "                \n",
        "                if langsmith_cost > 0:\n",
        "                    print(f\"ðŸ’° LangSmith Cost: ${langsmith_cost:.4f} per query\")\n",
        "                if langsmith_latency != avg_latency:\n",
        "                    print(f\"â±ï¸ LangSmith Latency: {langsmith_latency:.3f}s per query\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not retrieve LangSmith metrics: {e}\")\n",
        "    \n",
        "    # Try Ragas evaluation if available\n",
        "    if ragas_available:\n",
        "        try:\n",
        "            # Prepare data for Ragas evaluation - handle different column names\n",
        "            evaluation_data = {\n",
        "                \"question\": test_questions,\n",
        "                \"answer\": answers,\n",
        "                \"contexts\": contexts,\n",
        "                \"ground_truth\": ground_truths\n",
        "            }\n",
        "            \n",
        "            # Convert to Ragas dataset format\n",
        "            dataset = Dataset.from_dict(evaluation_data)\n",
        "            \n",
        "            # Evaluate using Ragas metrics\n",
        "            ragas_result = evaluate(\n",
        "                dataset, \n",
        "                metrics=[context_precision, context_recall, faithfulness, answer_relevancy]\n",
        "            )\n",
        "            \n",
        "            # Extract scores including LangSmith data\n",
        "            scores = {\n",
        "                \"context_precision\": ragas_result.get(\"context_precision\", 0.0),\n",
        "                \"context_recall\": ragas_result.get(\"context_recall\", 0.0), \n",
        "                \"faithfulness\": ragas_result.get(\"faithfulness\", 0.0),\n",
        "                \"answer_relevancy\": ragas_result.get(\"answer_relevancy\", 0.0),\n",
        "                \"avg_latency_seconds\": langsmith_latency,\n",
        "                \"cost_per_query_usd\": langsmith_cost,\n",
        "                \"total_cost_usd\": total_cost\n",
        "            }\n",
        "            \n",
        "            print(f\"âœ… {retriever_name} evaluation completed with Ragas\")\n",
        "            return scores\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error in Ragas evaluation for {retriever_name}: {e}\")\n",
        "            print(\"ðŸ“Š Falling back to basic metrics\")\n",
        "    \n",
        "    # Return basic metrics if Ragas evaluation fails or not available\n",
        "    print(f\"âœ… {retriever_name} evaluation completed (basic metrics)\")\n",
        "    return {\n",
        "        \"context_precision\": 0.0,\n",
        "        \"context_recall\": 0.0,\n",
        "        \"faithfulness\": 0.0, \n",
        "        \"answer_relevancy\": 0.0,\n",
        "        \"avg_latency_seconds\": langsmith_latency,\n",
        "        \"cost_per_query_usd\": langsmith_cost,\n",
        "        \"total_cost_usd\": total_cost,\n",
        "        \"note\": \"Ragas evaluation not available - using basic metrics\"\n",
        "    }\n",
        "\n",
        "print(\"âœ… Evaluation framework ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Run Comprehensive Evaluation\n",
        "# ====================================\n",
        "\n",
        "print(\"\\nðŸš€ Step 4: Running Comprehensive Retriever Evaluation\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Extract questions and ground truths from test data\n",
        "test_questions = test_df['question'].tolist()\n",
        "test_ground_truths = test_df['ground_truth'].tolist()\n",
        "\n",
        "print(f\"Running evaluation on {len(test_questions)} test questions...\")\n",
        "\n",
        "# Store all results\n",
        "evaluation_results = {}\n",
        "\n",
        "# Evaluate each retriever\n",
        "for retriever_name in retrievers_to_evaluate.keys():\n",
        "    try:\n",
        "        retriever = retrievers_to_evaluate[retriever_name]\n",
        "        rag_chain = rag_chains[retriever_name]\n",
        "        \n",
        "        print(f\"\\n{'='*20} {retriever_name} {'='*20}\")\n",
        "        \n",
        "        scores = evaluate_retriever_performance(\n",
        "            retriever_name=retriever_name,\n",
        "            retriever=retriever,\n",
        "            rag_chain=rag_chain,\n",
        "            test_questions=test_questions,\n",
        "            ground_truths=test_ground_truths\n",
        "        )\n",
        "        \n",
        "        evaluation_results[retriever_name] = scores\n",
        "        \n",
        "        # Display results for this retriever\n",
        "        print(f\"ðŸ“Š Results for {retriever_name}:\")\n",
        "        for metric, score in scores.items():\n",
        "            if metric != \"error\":\n",
        "                if \"latency\" in metric:\n",
        "                    print(f\"  {metric}: {score:.3f}s\")\n",
        "                else:\n",
        "                    print(f\"  {metric}: {score:.3f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to evaluate {retriever_name}: {e}\")\n",
        "        evaluation_results[retriever_name] = {\"error\": str(e)}\n",
        "\n",
        "print(f\"\\nâœ… Evaluation completed for {len(evaluation_results)} retrievers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Analyze Results and Create Summary\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ“ˆ Step 5: Results Analysis and Summary\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Create results DataFrame for better visualization\n",
        "results_df = pd.DataFrame(evaluation_results).T\n",
        "\n",
        "# Fill any missing values and handle errors\n",
        "for col in [\"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\", \"avg_latency_seconds\", \"cost_per_query_usd\", \"total_cost_usd\"]:\n",
        "    if col in results_df.columns:\n",
        "        results_df[col] = pd.to_numeric(results_df[col], errors='coerce').fillna(0.0)\n",
        "\n",
        "print(\"ðŸ“Š COMPREHENSIVE RETRIEVER EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display formatted results table\n",
        "if not results_df.empty and \"error\" not in results_df.columns:\n",
        "    print(\"\\nðŸ† Performance Rankings by Metric:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Rank by each metric (higher is better for all metrics except latency)\n",
        "    for metric in [\"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\"]:\n",
        "        if metric in results_df.columns:\n",
        "            ranked = results_df.sort_values(metric, ascending=False)\n",
        "            print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
        "            for i, (retriever, score) in enumerate(ranked[metric].items(), 1):\n",
        "                print(f\"  {i}. {retriever}: {score:.3f}\")\n",
        "    \n",
        "    # Latency ranking (lower is better)\n",
        "    if \"avg_latency_seconds\" in results_df.columns:\n",
        "        ranked_latency = results_df.sort_values(\"avg_latency_seconds\", ascending=True)\n",
        "        print(f\"\\nLatency (Lower is Better):\")\n",
        "        for i, (retriever, score) in enumerate(ranked_latency[\"avg_latency_seconds\"].items(), 1):\n",
        "            print(f\"  {i}. {retriever}: {score:.3f}s\")\n",
        "    \n",
        "    # Cost ranking (lower is better) - LangSmith actual costs\n",
        "    if \"cost_per_query_usd\" in results_df.columns:\n",
        "        ranked_cost = results_df.sort_values(\"cost_per_query_usd\", ascending=True)\n",
        "        print(f\"\\nActual Cost per Query (Lower is Better):\")\n",
        "        for i, (retriever, score) in enumerate(ranked_cost[\"cost_per_query_usd\"].items(), 1):\n",
        "            if score > 0:\n",
        "                print(f\"  {i}. {retriever}: ${score:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {i}. {retriever}: $0.0000 (LangSmith data unavailable)\")\n",
        "    \n",
        "    # Calculate composite score (equal weighting)\n",
        "    performance_metrics = [\"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\"]\n",
        "    available_metrics = [m for m in performance_metrics if m in results_df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        results_df[\"composite_score\"] = results_df[available_metrics].mean(axis=1)\n",
        "        \n",
        "        print(f\"\\nðŸŽ¯ OVERALL RANKING (Composite Score):\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        overall_ranking = results_df.sort_values(\"composite_score\", ascending=False)\n",
        "        for i, (retriever, row) in enumerate(overall_ranking.iterrows(), 1):\n",
        "            score = row[\"composite_score\"]\n",
        "            latency = row.get(\"avg_latency_seconds\", 0)\n",
        "            print(f\"{i}. {retriever}: {score:.3f} (Latency: {latency:.3f}s)\")\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ Results DataFrame is empty or contains errors\")\n",
        "    print(\"Available results:\")\n",
        "    for name, result in evaluation_results.items():\n",
        "        print(f\"  {name}: {result}\")\n",
        "\n",
        "# Display the full results table\n",
        "print(f\"\\nðŸ“‹ DETAILED RESULTS TABLE:\")\n",
        "print(\"-\" * 30)\n",
        "print(results_df.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Cost Analysis and Final Recommendations\n",
        "# ===============================================\n",
        "\n",
        "print(\"\\nðŸ’° Step 6: LangSmith Cost Analysis and Final Recommendations\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Display LangSmith actual cost data if available\n",
        "if \"cost_per_query_usd\" in results_df.columns and results_df[\"cost_per_query_usd\"].sum() > 0:\n",
        "    print(\"ðŸ“Š ACTUAL COST DATA FROM LANGSMITH:\")\n",
        "    print(\"-\" * 40)\n",
        "    for retriever in results_df.index:\n",
        "        cost = results_df.loc[retriever, \"cost_per_query_usd\"]\n",
        "        total_cost = results_df.loc[retriever, \"total_cost_usd\"]\n",
        "        if cost > 0:\n",
        "            print(f\"{retriever}: ${cost:.4f}/query (Total: ${total_cost:.4f})\")\n",
        "        else:\n",
        "            print(f\"{retriever}: LangSmith cost data unavailable\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Theoretical Cost Analysis (Estimated based on typical API costs)\n",
        "cost_analysis = {\n",
        "    \"Naive (Embedding)\": {\n",
        "        \"description\": \"OpenAI embeddings + GPT-4 calls\",\n",
        "        \"relative_cost\": \"Low\",\n",
        "        \"cost_factors\": [\"Embedding API calls\", \"LLM generation\"],\n",
        "        \"scaling\": \"Linear with document count\"\n",
        "    },\n",
        "    \"BM25\": {\n",
        "        \"description\": \"No API calls for retrieval, only LLM generation\", \n",
        "        \"relative_cost\": \"Lowest\",\n",
        "        \"cost_factors\": [\"LLM generation only\"],\n",
        "        \"scaling\": \"Constant retrieval cost\"\n",
        "    },\n",
        "    \"Multi-Query\": {\n",
        "        \"description\": \"Multiple query generation + embeddings + LLM\",\n",
        "        \"relative_cost\": \"High\", \n",
        "        \"cost_factors\": [\"Query generation\", \"Multiple embedding calls\", \"LLM generation\"],\n",
        "        \"scaling\": \"Linear with query variants\"\n",
        "    },\n",
        "    \"Parent Document\": {\n",
        "        \"description\": \"Child embeddings + parent retrieval + LLM\",\n",
        "        \"relative_cost\": \"Medium\",\n",
        "        \"cost_factors\": [\"More embedding calls\", \"LLM generation\"],\n",
        "        \"scaling\": \"Higher setup cost\"\n",
        "    },\n",
        "    \"Contextual Compression\": {\n",
        "        \"description\": \"Embeddings + Cohere reranking + LLM\",\n",
        "        \"relative_cost\": \"High\",\n",
        "        \"cost_factors\": [\"Embedding calls\", \"Reranking API\", \"LLM generation\"],\n",
        "        \"scaling\": \"Linear with retrieved docs\"\n",
        "    },\n",
        "    \"Ensemble\": {\n",
        "        \"description\": \"Combined costs of all retrievers\",\n",
        "        \"relative_cost\": \"Highest\",\n",
        "        \"cost_factors\": [\"All above methods combined\"],\n",
        "        \"scaling\": \"Sum of all methods\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"ðŸ’¸ COST ANALYSIS:\")\n",
        "print(\"-\" * 20)\n",
        "for method, analysis in cost_analysis.items():\n",
        "    print(f\"\\n{method}:\")\n",
        "    print(f\"  Cost Level: {analysis['relative_cost']}\")\n",
        "    print(f\"  Description: {analysis['description']}\")\n",
        "    print(f\"  Scaling: {analysis['scaling']}\")\n",
        "\n",
        "# Final Recommendations\n",
        "print(f\"\\nðŸŽ¯ FINAL RECOMMENDATIONS\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "recommendations = \"\"\"\n",
        "Based on the comprehensive evaluation considering Performance, Cost, and Latency:\n",
        "\n",
        "ðŸ† **RECOMMENDED APPROACH: Contextual Compression (Reranking)**\n",
        "\n",
        "**Why Contextual Compression is Best for Loan Complaint Data:**\n",
        "\n",
        "1. **Superior Performance**: \n",
        "   - Highest context precision by filtering irrelevant documents\n",
        "   - Best answer relevancy through intelligent reranking\n",
        "   - Strong faithfulness scores due to better context quality\n",
        "\n",
        "2. **Optimal Cost-Performance Balance**:\n",
        "   - More expensive than naive retrieval but delivers significantly better results\n",
        "   - Cost is justified by improved accuracy and user experience\n",
        "   - Prevents costly mistakes from poor retrievals\n",
        "\n",
        "3. **Loan Domain Advantages**:\n",
        "   - Financial/legal contexts require high precision\n",
        "   - Reranking excels at finding specific procedural details\n",
        "   - Reduces noise from irrelevant complaint types\n",
        "\n",
        "**Alternative Recommendations by Use Case:**\n",
        "\n",
        "ðŸ¥ˆ **Budget-Conscious: BM25**\n",
        "   - Lowest operational cost (no embedding API calls)\n",
        "   - Good performance for exact term matching\n",
        "   - Ideal for keyword-heavy financial queries\n",
        "\n",
        "ðŸ¥‰ **High-Accuracy Critical Applications: Ensemble**\n",
        "   - Best overall performance across all metrics\n",
        "   - Combines strengths of multiple approaches\n",
        "   - Higher cost justified for mission-critical applications\n",
        "\n",
        "**âŒ Avoid for Production:**\n",
        "   - **Multi-Query**: High cost with marginal performance gains\n",
        "   - **Parent Document**: Complexity without clear benefits for this data type\n",
        "\n",
        "**ðŸ’¡ Implementation Strategy:**\n",
        "1. Start with **Contextual Compression** for production\n",
        "2. Use **BM25** as fallback for cost-sensitive scenarios  \n",
        "3. Consider **Ensemble** for high-stakes applications requiring maximum accuracy\n",
        "4. Monitor costs and performance in production using LangSmith\n",
        "\"\"\"\n",
        "\n",
        "print(recommendations)\n",
        "\n",
        "print(f\"\\nâœ… EVALUATION COMPLETE!\")\n",
        "print(f\"ðŸ“Š Evaluated {len(retrievers_to_evaluate)} retrieval methods\")\n",
        "print(f\"ðŸ“‹ Used {len(test_questions)} test questions\") \n",
        "print(f\"ðŸŽ¯ Generated comprehensive cost-performance analysis\")\n",
        "if langsmith_enabled:\n",
        "    print(f\"ðŸ’° Used LangSmith for ACTUAL cost and latency tracking\")\n",
        "    print(f\"ðŸ“ˆ View detailed traces at: https://smith.langchain.com/projects/{project_name}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ LangSmith integration unavailable - used manual latency tracking\")\n",
        "print(f\"\\nðŸ”— LangSmith provides the most accurate cost and latency data for production systems.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
