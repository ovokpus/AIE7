{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get LangSmith API key\n",
        "langsmith_key = getpass.getpass(\"LangSmith API Key:\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and issues with how payments are being handled. Many complaints involve mismanagement, inaccurate information, and difficulties in making or applying payments correctly.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided information, yes, some complaints were not handled in a timely manner. Specifically, at least one complaint was marked as 'No' in the 'Timely response?' field, indicating it was not responded to within the expected timeframe. For example, the complaint with Complaint ID '12709087' submitted to MOHELA on 03/28/25 was not handled in a timely manner.\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans mainly due to a combination of factors such as ongoing interest accumulation even during forbearance or deferment periods, lack of clear communication from loan servicers about payment resumption or delinquency status, and the inability to afford increased payments without jeopardizing their basic living expenses. Additionally, some borrowers faced issues like being misled about their repayment obligations, being unaware of loan transfers between companies, or experiencing difficulties in applying extra payments to principal, which extended the duration and cost of repayment. These challenges, coupled with financial hardships, stagnant wages, and limited access to loan forgiveness programs, contributed to their inability to fully repay their student loans.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including issues such as incorrect or bad information about the loan, difficulty in managing payments, and disputes over fees or loan details. Multiple complaints involve challenges with understanding or verifying loan balances, issues with loan application or repayment processes, and inaccuracies or bad communication from loan servicers.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints indicated that the companies responded timely. Specifically, the complaints from 04/26/25, 04/01/25, 04/24/25, and 05/08/25 all note that the companies responded with a \"Closed with explanation\" status and specify \"Timely response? Yes.\" Therefore, there is no evidence in the provided data that any complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including difficulties with their payment plans, miscommunication or lack of communication from lenders or servicers, problems with automated payments, and issues with understanding or receiving information about their loans. Some specific examples include being steered into incorrect forbearance options, having their autopayments unexpectedly discontinued without proper notification, and experiencing errors or delays in response from loan servicers when requesting deferments or forbearances. Additionally, there are cases where borrowers believe they fulfilled all requirements for discharge or repayment assistance but did not receive proper acknowledgment or help, leading to continued or increased debt. Overall, these issues often stem from administrative errors, poor communication, or alleged deceptive practices by loan servicers.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "**Example Query: \"Why did people fail to pay back their loans?\"**\n",
        "\n",
        "Based on the invocations in this notebook, BM25 performs better than embeddings for this query.\n",
        "\n",
        "**Comparison of responses:**\n",
        "\n",
        "**Naive Retrieval (Embeddings):** Provided a broad, conceptual response covering systemic issues, miscommunication, and financial constraints but was more general in nature.\n",
        "\n",
        "**BM25 Retrieval:** Delivered more specific, actionable details including:\n",
        "- \"unenrolled from autopay without their knowledge\"\n",
        "- \"payment reversals\" \n",
        "- \"steered into improper forbearances\"\n",
        "- \"capitalized interest\"\n",
        "- \"loan transfer process\"\n",
        "\n",
        "**Why BM25 is better here:**\n",
        "\n",
        "1. **Exact Term Matching**: BM25 excels at finding documents containing specific financial and procedural terminology like \"autopay\", \"forbearances\", \"capitalized interest\" - terms that are crucial for understanding concrete loan servicing problems.\n",
        "\n",
        "2. **Factual Precision**: The query asks for specific reasons why payments failed. BM25's keyword-based approach captures precise procedural failures and technical issues that directly answer the \"why\" question.\n",
        "\n",
        "3. **Domain-Specific Language**: In financial/loan contexts, exact terminology matters immensely. BM25's ability to match specific loan servicing terms provides more actionable and legally/procedurally accurate information than semantic similarity alone.\n",
        "\n",
        "4. **Reduced Semantic Drift**: Embeddings might retrieve conceptually similar but less precise content, whereas BM25 stays focused on documents containing the exact operational terms that explain payment failures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with loans, particularly student loans, appear to involve errors and misconduct by servicers or lenders. Specific recurring problems include errors in loan balances, misapplied payments, wrongful denials of payment plans, incorrect or inconsistent information about loan amounts and interest, unauthorized transfers of loans, and mishandling of personal data. These issues often lead to confusion, damaged credit, and violations of legal rights.\\n\\nIn summary, the most common issue is dealing with errors and misconduct by loan servicers or lenders, especially related to inaccurate information, mismanagement, and inadequate communication.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided information, yes, there are complaints that did not get handled in a timely manner. For example, the complaint regarding the student's loan account review and resolution has been open for over 1 year and nearly 18 months without resolution. Additionally, a previous complaint about issues with payments not appearing on the account was still unresolved after 2-3 weeks, and the complaint about unapplied payments also concerns ongoing unresolved issues.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily because they were not adequately informed about the repayment obligations and the complexities of the loan process. For example, some borrowers were unaware that they would have to repay their loans at all, or were not told about important details like interest accumulation, payment plans, and loan transfers. Additionally, many faced challenges such as interest accruing during deferment or forbearance, which increased their total debt over time, and a lack of clear communication from loan servicers about payment requirements, due dates, or account status. These issues made it difficult for them to manage and repay their loans effectively, leading to missed payments, growing balances, and sometimes even late payment reports on credit reports.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to \"Dealing with your lender or servicer,\" specifically sub-issues such as:\\n\\n- Trouble with how payments are being handled (e.g., improper application of payments, inability to apply extra funds to principal, payments being directed to interest, or misapplied payments).\\n- Issues with loan balances, interest calculations, or discrepancies in account information.\\n- Lack of or improper communication from loan servicers about payment status, loan amount, or account changes.\\n- Unauthorized transfers or reassignment of loans without borrower consent.\\n- Inadequate documentation or verification of loan terms and legal rights.\\n- Misleading or inaccurate information about loan terms, balances, or repayment options.\\n- Challenges in obtaining accurate loan history or account status, and difficulties in resolving disputes.\\n\\nOverall, the most prevalent theme is the difficulties borrowers experience when interacting with loan servicers who do not adequately communicate, properly manage payments, or provide clear and accurate information, often resulting in unpaid balances, increased interest, or damage to credit scores.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, yes, some complaints indicate that issues were not handled in a timely manner. Specifically, a few complaints mention delays in responses or resolutions:\\n\\n- Complaint ID 12698650 (Mohela, LA): The consumer reported that the issue was not resolved after over 18 months, despite ongoing efforts and multiple submissions. Although the company responded \"timely\" in the response, the consumer\\'s experience reflects a significant delay in resolution.\\n- Complaint ID 12739706 (Mohela, MD): The respondent was marked as \"No\" for timely response, indicating the complaint was not addressed promptly.\\n- Complaint ID 12973003 (EdFinancial, NJ): The complaint response was marked as \"Yes\" but the narrative describes ongoing unresolved issues over several weeks.\\n- Complaint ID 12654977 (Mohela, NJ): Marked as \"No\" for timely response, indicating a delay.\\n- Complaint IDs involving credit report inaccuracies or account status issues also reflect delays or failure to resolve, with some complaints mentioning waiting over several days or weeks without resolution.\\n\\nIn summary, while some responses claim timely handling, numerous complaints include reports of delays, unresolved issues over extended periods, or ongoing unresolved problems.\\n\\nTherefore, the answer is: **Yes, some complaints did not get handled in a timely manner.**'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to issues such as mismanagement by loan servicers, errors in loan balances, misapplied payments, and wrongful denials of payment plans. Additionally, some borrowers experienced difficulties because of inadequate communication from lenders, legal discrepancies, privacy violations, and systemic failures in handling their accounts. Factors like being misled about repayment options, being placed in long-term forbearances without proper guidance, and experiencing unauthorized or incorrect reporting of delinquencies also contributed to their inability to repay the loans.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "Generating multiple reformulations of a user query can significantly improve recall through several mechanisms:\n",
        "\n",
        "**1. Vocabulary Mismatch Reduction:**\n",
        "- Users and document authors often use different words to describe the same concepts\n",
        "- Multiple reformulations increase the likelihood of matching the exact terminology used in relevant documents\n",
        "- Example: A user asking \"payment issues\" might miss documents that use \"billing problems\" or \"transaction difficulties\"\n",
        "\n",
        "**2. Query Perspective Diversification:**\n",
        "- Different reformulations can approach the same topic from various angles\n",
        "- Each perspective might surface different relevant documents that focus on specific aspects\n",
        "- Example: \"Why did loans fail?\" vs \"What caused borrower defaults?\" vs \"Reasons for payment problems\"\n",
        "\n",
        "**3. Semantic Coverage Expansion:**\n",
        "- LLMs can generate reformulations that capture different semantic nuances of the original query\n",
        "- This helps retrieve documents that are conceptually relevant but use different language patterns\n",
        "- Increases the semantic search space beyond the original query's limited scope\n",
        "\n",
        "**4. Synonym and Paraphrase Utilization:**\n",
        "- Reformulations naturally incorporate synonyms and paraphrases\n",
        "- Documents using alternative terminology become discoverable\n",
        "- Reduces dependency on exact keyword matches\n",
        "\n",
        "**5. Comprehensive Document Retrieval:**\n",
        "- By retrieving documents for each reformulated query and taking the union of all results\n",
        "- The final context includes a broader set of potentially relevant documents\n",
        "- Higher chance of including the most relevant information that might have been missed by a single query\n",
        "\n",
        "**Implementation in Multi-Query Retriever:**\n",
        "The Multi-Query Retriever demonstrates this by:\n",
        "1. Using an LLM to generate multiple query variations\n",
        "2. Running retrieval for each variation\n",
        "3. Combining all unique documents into the final context\n",
        "4. Providing richer, more comprehensive information for answer generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The most common issue with loans, based on the context provided, appears to be problems related to the servicing of federal student loans. These include errors in loan balances, misapplied payments, wrongful denials of payment plans, discrepancies with loan balances and interest rates, and improper or outdated credit reporting. Many complaints involve systemic breakdowns, miscommunications, and unfair practices by loan servicers, which can severely impact borrowers' credit scores and financial stability.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, the complaints listed indicate that they did not receive a timely response from the companies involved. Specifically:\\n\\n- The complaint received on 03/28/25 regarding the student loan application process was marked as \"Timely response?\": No.\\n- The complaint received on 04/11/25 regarding issues with loan payment handling was also marked as \"Timely response?\": No.\\n\\nAdditionally, the complaint filed on 04/27/25 about credit bureau dispute resolution was marked as \"Timely response?\": Yes, but it appears there has been a delay of over 30 days in response.\\n\\nTherefore, yes, there were complaints that did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans mainly due to factors such as experiencing severe financial hardship, being misinformed about the long-term consequences of their loans, and facing difficulties in securing employment or income to make payments. In some cases, they also encountered issues with loan servicing, such as being unaware of payment requirements, lack of proper communication from lenders, or complications related to loan transfer and reporting, which further hindered their ability to repay.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with student loan loans appear to involve:\\n\\n- Dealing with lenders or servicers, including receiving bad information about loans, problematic handling of payments, and mismanagement\\n- Errors in loan balances, misapplied payments, wrongful denials of repayment plans, and incorrect account statuses\\n- Problems with repayment plans, including difficulty with income-driven repayment, forbearance, deferments, and loan forgiveness\\n- Incorrect or incomplete information on credit reports and credit reporting errors\\n- Lack of proper communication, notifications, or transparency from loan servicers\\n- Unauthorized or improper transfer or sale of loans without borrower knowledge\\n- Inaccurate loan classification or mislabeling of loan types (e.g., FFELP vs. HEAL)\\n- Problems with loan repayment timing and interest capitalization, leading to increasing balances\\n- Issues related to illegal collection practices and failure to follow federal regulations\\n\\nConsidering these points, the most recurring theme seems to be **\"dealing with lenders or servicers,\"** especially related to providing bad information, mishandling accounts, and communication failures. \\n\\n**Therefore, the most common issue with loans, as reflected frequently in complaints, is:**\\n\\n**Problems with loan servicers or lenders, including misinformation, mismanagement, and poor communication.**'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, yes, there are complaints that were not handled in a timely manner. Specifically, one complaint received on 03/26/25 involving Maximus Federal Services, Inc. (Aidvantage) was marked as \"No\" in the \"Timely response?\" field, indicating it was not addressed within the expected period. Additionally, several complaints involving ED Financial Services and Maximus Federal Services, Inc. were also marked as \"No\" under \"Timely response,\" meaning they were not responded to promptly.\\n\\nIn sum, multiple complaints documented in the data were not handled in a timely manner.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of proper notification or communication from loan servicers about payment obligations, due dates, or changes in account status, leading to unawareness of when payments were due.\\n2. Difficulty understanding or navigating repayment plans, especially when offered limited options like forbearance or deferment that result in accumulating interest, making loans harder to pay off.\\n3. Mismanagement or errors by loan servicers, such as incorrect account information, misapplied payments, and errors in balances, which can hinder repayment efforts.\\n4. Financial hardships, such as unemployment, low income, homelessness, or medical issues, which make it challenging to make timely payments.\\n5. Issues with the transfer of loans between companies or lack of transparency about account status, leading borrowers to be unaware of their obligations.\\n6. Problems with loan handling, including inappropriate steering into long-term forbearances, inability to access income-driven repayment plans, or misrepresented loan conditions.\\n7. Increased debt due to high interest rates, capitalization of interest, or compounded interest, which can cause balances to grow over time despite payments.\\n8. Technical or administrative issues, such as payments not posting correctly, missing funds, or errors in reporting payments, which can cause delinquency or credit score drops.\\n9. Borrowers' lack of knowledge about the true long-term consequences of their loans, including high interest accumulation and limited access to forgiveness options.\\n\\nIn summary, a combination of insufficient communication, mismanagement by loan servicers, financial hardships, and the complex nature of loan repayment options contributed to many people's failure to fully pay back their loans.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with loans seem to involve problems related to loan servicing and communication. These include:\\n\\n- Struggles to repay or issues with repayment plans (e.g., difficulties with income-driven repayment calculations and unexpected payment amounts).\\n- Poor communication or lack of transparency from loan servicers (delays, miscommunication about loan status or payment terms).\\n- Errors in reporting or account status (incorrect default notices, delinquency reports, or account statuses on credit reports).\\n- Problems with loan handling after legal or policy changes, such as illegitimate collections or data breaches.\\n- Difficulties in setting up or verifying auto-debits and payment processing.\\n- Issues arising from the end of forbearance or re-amortization not being properly processed.\\n\\nOverall, a common theme appears to be that many issues stem from poor communication, administrative errors, or mismanagement by loan servicers, causing borrower confusion, incorrect account statuses, or payment difficulties.\\n\\nIf you have any other questions or need further assistance, feel free to ask!'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, several complaints indicate that responses from the companies were handled in a timely manner, with responses marked as \"Yes\" for timely response and \"Closed with explanation.\" However, there is at least one complaint where the consumer explicitly states that despite multiple efforts to communicate, the company did not respond to their written complaints or questions. Specifically, in complaint ID 13331376 regarding Nelnet, Inc., the consumer mentions that despite sending multiple certified mail letters detailing serious misconduct and violations of law, Nelnet never responded to the CM, nor provided any answers to the questions raised.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner, or in some cases, not at all from the consumerâ€™s perspective.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People may fail to pay back their loans for various reasons, including communication issues with lenders or servicers, lack of transparency, difficulties in navigating loan processes, and administrative errors. For example, some borrowers experience trouble receiving clear information about their loan status or payment requirements, which can lead to missed payments. Others face delays or inaccuracies in payment processing, or encounter problems with documentation and loan transfer procedures. Additionally, disputes over loan legitimacy or claims of improper reporting can cause confusion and hinder repayment. Overall, challenges related to poor communication, administrative complications, and lack of clear information contribute to some borrowers' inability to repay their loans.\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "**How Semantic Chunking Behaves with Short, Repetitive Sentences:**\n",
        "\n",
        "**1. Problematic Similarity Patterns:**\n",
        "- Short, repetitive sentences (like FAQ questions) often have very high semantic similarity scores\n",
        "- The algorithm may struggle to find meaningful breakpoints between semantically similar content\n",
        "- Could result in either overly large chunks (everything grouped together) or overly small chunks (no grouping occurs)\n",
        "\n",
        "**2. Threshold Sensitivity Issues:**\n",
        "- With highly similar content, small variations in similarity scores become less meaningful\n",
        "- Percentile-based thresholding may not work effectively when most distances are very close\n",
        "- The algorithm might either chunk everything together or keep everything separate\n",
        "\n",
        "**3. Loss of Logical Structure:**\n",
        "- FAQs have inherent question-answer pairs that should ideally stay together\n",
        "- Semantic chunking might break these logical pairs if focusing purely on sentence-level similarity\n",
        "- Context and structure information gets lost in favor of semantic similarity\n",
        "\n",
        "**Adjustments to Improve Performance:**\n",
        "\n",
        "**1. Threshold Method Modifications:**\n",
        "```python\n",
        "# Use more aggressive thresholding\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"standard_deviation\",  # More sensitive to small differences\n",
        "    breakpoint_threshold_amount=1.0  # Lower threshold for more breaks\n",
        ")\n",
        "```\n",
        "\n",
        "**2. Pre-processing Strategies:**\n",
        "- Identify FAQ structure patterns (Q: A: formatting)\n",
        "- Use regex to detect question-answer pairs\n",
        "- Apply metadata-based chunking to preserve logical pairs\n",
        "\n",
        "**3. Hybrid Approaches:**\n",
        "```python\n",
        "# Combine semantic chunking with structural rules\n",
        "- First chunk by logical structure (Q-A pairs)\n",
        "- Then apply semantic chunking within those logical boundaries\n",
        "- Use custom splitting logic for FAQ-specific patterns\n",
        "```\n",
        "\n",
        "**4. Alternative Chunking Methods:**\n",
        "- **Fixed-size chunking** with Q-A pair preservation\n",
        "- **Metadata-based chunking** using FAQ structure\n",
        "- **Custom splitters** that understand FAQ formatting\n",
        "\n",
        "**5. Enhanced Similarity Calculation:**\n",
        "- Use more sophisticated embedding models that better capture subtle differences\n",
        "- Apply domain-specific embeddings trained on FAQ data\n",
        "- Consider using multiple embedding dimensions for better differentiation\n",
        "\n",
        "**Recommended Implementation for FAQs:**\n",
        "Rather than relying purely on semantic chunking, use a structured approach that preserves the logical Q-A relationships while still benefiting from semantic organization for related topics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "309283bc61f94a8592935f9c8e4d1e1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5acd2f62fbf409db36763f69b3ddb21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86d0d314ddc94f499bfcbf514c0c66b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node '8ff305'. Skipping!\n",
            "Property 'summary' already exists in node 'bbc2f8'. Skipping!\n",
            "Property 'summary' already exists in node 'b99cce'. Skipping!\n",
            "Property 'summary' already exists in node 'bf5926'. Skipping!\n",
            "Property 'summary' already exists in node '8747b0'. Skipping!\n",
            "Property 'summary' already exists in node '53ab69'. Skipping!\n",
            "Property 'summary' already exists in node '18c482'. Skipping!\n",
            "Property 'summary' already exists in node '9dc621'. Skipping!\n",
            "Property 'summary' already exists in node 'aae253'. Skipping!\n",
            "Property 'summary' already exists in node '7846f1'. Skipping!\n",
            "Property 'summary' already exists in node 'ff934e'. Skipping!\n",
            "Property 'summary' already exists in node '837071'. Skipping!\n",
            "Property 'summary' already exists in node '24e505'. Skipping!\n",
            "Property 'summary' already exists in node '786f11'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c12023765cd4781b7b1cb555f6d8a9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de4c653e5b2c46919552bf90092c857f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/41 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node '24e505'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '8747b0'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '837071'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '7846f1'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'bf5926'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'ff934e'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '786f11'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'bbc2f8'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '18c482'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'aae253'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '53ab69'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '8ff305'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '9dc621'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'b99cce'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cdab56807c9470aa6500a4689baef9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5912322dbe89407499cf3a8152841a30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2848ce89e28046ce85336ca56e1d6ade",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8a88bdf0e27458eb2af855af7bdf6f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(\n",
        "    llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for evaluation\n",
        "from ragas.metrics import ContextPrecision, ContextRecall, ContextRelevance\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
        "import pandas as pd\n",
        "import time\n",
        "from langsmith import traceable\n",
        "from datetime import datetime\n",
        "import os\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up LangSmith for cost and latency tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LangSmith tracing enabled!\n",
            "ðŸ“Š Project: retriever-evaluation\n",
            "ðŸ”— Visit https://smith.langchain.com to view your traces\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Get LangSmith API key\n",
        "# langsmith_key = getpass.getpass(\"LangSmith API Key:\")\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
        "\n",
        "# Enable LangSmith tracing\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"retriever-evaluation\"\n",
        "\n",
        "print(\"âœ… LangSmith tracing enabled!\")\n",
        "print(f\"ðŸ“Š Project: {os.environ['LANGSMITH_PROJECT']}\")\n",
        "print(\"ðŸ”— Visit https://smith.langchain.com to view your traces\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up RAGAS evaluator with LLM wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAGAS evaluator setup complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "# Define RAGAS metrics for retriever evaluation\n",
        "ragas_metrics = [\n",
        "    ContextPrecision(llm=evaluator_llm),\n",
        "    ContextRecall(llm=evaluator_llm), \n",
        "    ContextRelevance(llm=evaluator_llm)\n",
        "]\n",
        "\n",
        "print(\"RAGAS evaluator setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 12 evaluation samples\n",
            "Sample structure: dict_keys(['user_input', 'reference_contexts', 'reference'])\n"
          ]
        }
      ],
      "source": [
        "# Convert the generated dataset to RAGAS format\n",
        "test_df = dataset.to_pandas()\n",
        "\n",
        "# Create evaluation samples from the test dataset\n",
        "evaluation_samples = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    sample = {\n",
        "        'user_input': row['user_input'],\n",
        "        'reference_contexts': row['reference_contexts'],\n",
        "        'reference': row['reference']\n",
        "    }\n",
        "    evaluation_samples.append(sample)\n",
        "\n",
        "print(f\"Created {len(evaluation_samples)} evaluation samples\")\n",
        "print(\"Sample structure:\", evaluation_samples[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define all retrievers to evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will evaluate 7 retrieval methods\n"
          ]
        }
      ],
      "source": [
        "retrievers_to_evaluate = {\n",
        "    \"naive\": naive_retriever,\n",
        "    \"bm25\": bm25_retriever, \n",
        "    \"contextual_compression\": compression_retriever,\n",
        "    \"multi_query\": multi_query_retriever,\n",
        "    \"parent_document\": parent_document_retriever,\n",
        "    \"ensemble\": ensemble_retriever,\n",
        "    \"semantic\": semantic_retriever\n",
        "}\n",
        "\n",
        "print(f\"Will evaluate {len(retrievers_to_evaluate)} retrieval methods\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation function defined!\n"
          ]
        }
      ],
      "source": [
        "@traceable\n",
        "def evaluate_retriever(retriever, retriever_name, evaluation_samples):\n",
        "    \"\"\"Evaluate a single retriever using RAGAS metrics\"\"\"\n",
        "    \n",
        "    print(f\"Evaluating {retriever_name}...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Prepare evaluation data\n",
        "    ragas_samples = []\n",
        "    successful_samples = 0\n",
        "    \n",
        "    for sample in evaluation_samples:\n",
        "        try:\n",
        "            # Retrieve documents for this question\n",
        "            retrieved_docs = retriever.invoke(sample['user_input'])\n",
        "            retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
        "            \n",
        "            # Create RAGAS sample\n",
        "            ragas_sample = SingleTurnSample(\n",
        "                user_input=sample['user_input'],\n",
        "                retrieved_contexts=retrieved_contexts,\n",
        "                reference_contexts=sample['reference_contexts'],\n",
        "                reference=sample['reference']\n",
        "            )\n",
        "            ragas_samples.append(ragas_sample)\n",
        "            successful_samples += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample for {retriever_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Calculate timing\n",
        "    end_time = time.time()\n",
        "    avg_latency = (end_time - start_time) / len(evaluation_samples)\n",
        "    \n",
        "    # Evaluate with RAGAS\n",
        "    if ragas_samples:\n",
        "        eval_dataset = EvaluationDataset(samples=ragas_samples)\n",
        "        evaluation_results = evaluate(dataset=eval_dataset, metrics=ragas_metrics)\n",
        "        \n",
        "        return {\n",
        "            'retriever_name': retriever_name,\n",
        "            'metrics': evaluation_results,\n",
        "            'avg_latency_seconds': avg_latency,\n",
        "            'total_samples': len(evaluation_samples),\n",
        "            'successful_samples': successful_samples\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "print(\"Evaluation function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run evaluation for all retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting retriever evaluation...\n",
            "============================================================\n",
            "Evaluating naive...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2389c8e8979443a4b366ca5d334350a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… naive evaluation completed\n",
            "Evaluating bm25...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "086713e31ecf4a33927e373a37c4ba21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… bm25 evaluation completed\n",
            "Evaluating contextual_compression...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3f4968f3a4846d8bf83447f8eb9cb4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… contextual_compression evaluation completed\n",
            "Evaluating multi_query...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "000b647c0ec9432d968cd8f13ba0dec1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-Hp16PKiuF3av02eUg87TR03d on tokens per min (TPM): Limit 200000, Used 200000, Requested 1995. Please try again in 598ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Skipping a sample by assigning it nan score.\n",
            "âœ… multi_query evaluation completed\n",
            "Evaluating parent_document...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5451af28c7a441d79c802d56146f9041",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… parent_document evaluation completed\n",
            "Evaluating ensemble...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d425847b539c45668d25670928028869",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-Hp16PKiuF3av02eUg87TR03d on tokens per min (TPM): Limit 200000, Used 200000, Requested 1995. Please try again in 598ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Skipping a sample by assigning it nan score.\n",
            "âœ… ensemble evaluation completed\n",
            "Evaluating semantic...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a219400234964398904e29bb139fbaf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… semantic evaluation completed\n",
            "\n",
            "Completed evaluation of 7 retrievers\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Starting retriever evaluation...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = []\n",
        "for retriever_name, retriever in retrievers_to_evaluate.items():\n",
        "    try:\n",
        "        result = evaluate_retriever(retriever, retriever_name, evaluation_samples)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "            print(f\"âœ… {retriever_name} evaluation completed\")\n",
        "        else:\n",
        "            print(f\"âŒ {retriever_name} evaluation failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error evaluating {retriever_name}: {e}\")\n",
        "        continue\n",
        "    time.sleep(60)\n",
        "\n",
        "print(f\"\\nCompleted evaluation of {len(results)} retrievers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze and compile results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing results...\n",
            "================================================================================\n",
            "RETRIEVER EVALUATION RESULTS\n",
            "================================================================================\n",
            "             Retriever  Context_Precision  Context_Recall  Context_Relevance  Avg_Latency_Seconds  Success_Rate  Total_Samples  Successful_Samples\n",
            "                 naive             0.1250          0.0000             0.2083               0.2762           1.0             12                  12\n",
            "                  bm25             0.0000          0.0000             0.0417               0.0034           1.0             12                  12\n",
            "contextual_compression             0.0278          0.0833             0.1250               0.5156           1.0             12                  12\n",
            "           multi_query             0.1042          0.0521             0.2045               9.5312           1.0             12                  12\n",
            "       parent_document             0.0833          0.1146             0.1667               0.3101           1.0             12                  12\n",
            "              ensemble             0.1667          0.1250             0.2273               8.2376           1.0             12                  12\n",
            "              semantic             0.0000          0.0000             0.0000               0.3084           1.0             12                  12\n",
            "\n",
            "================================================================================\n",
            "TOP PERFORMERS BY METRIC\n",
            "================================================================================\n",
            "ðŸŽ¯ Best Context Precision: ensemble - 0.1667\n",
            "ðŸ” Best Context Recall: ensemble - 0.125\n",
            "â­ Best Context Relevance: ensemble - 0.2273\n",
            "âš¡ Lowest Latency: bm25 - 0.0034s\n"
          ]
        }
      ],
      "source": [
        "print(\"Analyzing results...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not results:\n",
        "    print(\"No results to analyze!\")\n",
        "else:\n",
        "    # Create a comprehensive results DataFrame\n",
        "    compiled_results = []\n",
        "    \n",
        "    for result in results:\n",
        "        # Convert metrics to pandas DataFrame and extract mean values\n",
        "        metrics_df = result['metrics'].to_pandas()\n",
        "        \n",
        "        # Calculate mean values for each metric\n",
        "        context_precision_mean = metrics_df['context_precision'].mean()\n",
        "        context_recall_mean = metrics_df['context_recall'].mean() \n",
        "        context_relevance_mean = metrics_df['nv_context_relevance'].mean()\n",
        "        \n",
        "        row = {\n",
        "            'Retriever': result['retriever_name'],\n",
        "            'Context_Precision': round(context_precision_mean, 4),\n",
        "            'Context_Recall': round(context_recall_mean, 4),\n",
        "            'Context_Relevance': round(context_relevance_mean, 4),\n",
        "            'Avg_Latency_Seconds': round(result['avg_latency_seconds'], 4),\n",
        "            'Success_Rate': round(result['successful_samples'] / result['total_samples'], 4),\n",
        "            'Total_Samples': result['total_samples'],\n",
        "            'Successful_Samples': result['successful_samples']\n",
        "        }\n",
        "        compiled_results.append(row)\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(compiled_results)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"RETRIEVER EVALUATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(results_df.to_string(index=False))\n",
        "    \n",
        "    # Find best performers\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TOP PERFORMERS BY METRIC\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    best_precision_idx = results_df['Context_Precision'].idxmax()\n",
        "    best_recall_idx = results_df['Context_Recall'].idxmax()\n",
        "    best_relevance_idx = results_df['Context_Relevance'].idxmax()\n",
        "    best_latency_idx = results_df['Avg_Latency_Seconds'].idxmin()  # Lower is better\n",
        "    \n",
        "    print(f\"ðŸŽ¯ Best Context Precision: {results_df.at[best_precision_idx, 'Retriever']} - {results_df.at[best_precision_idx, 'Context_Precision']}\")\n",
        "    print(f\"ðŸ” Best Context Recall: {results_df.at[best_recall_idx, 'Retriever']} - {results_df.at[best_recall_idx, 'Context_Recall']}\")\n",
        "    print(f\"â­ Best Context Relevance: {results_df.at[best_relevance_idx, 'Retriever']} - {results_df.at[best_relevance_idx, 'Context_Relevance']}\")\n",
        "    print(f\"âš¡ Lowest Latency: {results_df.at[best_latency_idx, 'Retriever']} - {results_df.at[best_latency_idx, 'Avg_Latency_Seconds']}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate overall performance score (weighted average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "OVERALL PERFORMANCE ANALYSIS\n",
            "================================================================================\n",
            "RANKING BY COMPOSITE SCORE (Precision + Recall + Relevance + Speed):\n",
            "----------------------------------------------------------------------\n",
            " 6. ensemble             - Score: 0.7501\n",
            " 5. parent_document      - Score: 0.5402\n",
            " 4. multi_query          - Score: 0.4855\n",
            " 1. naive                - Score: 0.4196\n",
            " 3. contextual_compression - Score: 0.3474\n",
            " 2. bm25                 - Score: 0.2959\n",
            " 7. semantic             - Score: 0.0028\n",
            "\n",
            "================================================================================\n",
            "RECOMMENDATIONS\n",
            "================================================================================\n",
            "ðŸ† **BEST OVERALL RETRIEVER: ENSEMBLE**\n",
            "   - Composite Score: 0.7501\n",
            "   - Context Precision: 0.1667\n",
            "   - Context Recall: 0.1250\n",
            "   - Context Relevance: 0.2273\n",
            "   - Average Latency: 8.2376s\n",
            "\n",
            "ðŸ“Š **ANALYSIS SUMMARY:**\n",
            "This evaluation considers cost, latency, and performance factors:\n",
            "â€¢ **Performance**: Measured through RAGAS context precision, recall, and relevance metrics\n",
            "â€¢ **Latency**: Time taken to retrieve documents per query\n",
            "â€¢ **Cost**: Tracked through LangSmith (check your LangSmith dashboard for detailed cost analysis)\n",
            "\n",
            "ðŸ’¡ **RECOMMENDATION FOR LOAN COMPLAINT DATA:**\n",
            "Based on the evaluation, **ensemble** retriever is recommended because:\n",
            "â€¢ Combines strengths of multiple retrieval strategies\n",
            "â€¢ Shows balanced performance across all metrics\n",
            "â€¢ More robust to different query types\n",
            "\n",
            "ðŸ“‹ **FINAL NOTES:**\n",
            "â€¢ Check LangSmith dashboard for detailed cost analysis\n",
            "â€¢ Consider your specific use case when choosing a retriever\n",
            "â€¢ Ensemble methods often provide the most robust performance\n",
            "â€¢ BM25 excels for keyword-heavy queries, embeddings for semantic similarity\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OVERALL PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Normalize metrics to 0-1 scale for fair comparison\n",
        "    results_df['Precision_Normalized'] = results_df['Context_Precision'] / results_df['Context_Precision'].max()\n",
        "    results_df['Recall_Normalized'] = results_df['Context_Recall'] / results_df['Context_Recall'].max()\n",
        "    results_df['Relevance_Normalized'] = results_df['Context_Relevance'] / results_df['Context_Relevance'].max()\n",
        "    \n",
        "    # For latency, lower is better, so we invert it\n",
        "    results_df['Latency_Normalized'] = results_df['Avg_Latency_Seconds'].min() / results_df['Avg_Latency_Seconds']\n",
        "    \n",
        "    # Calculate composite score (equal weighting for simplicity)\n",
        "    results_df['Composite_Score'] = (\n",
        "        results_df['Precision_Normalized'] * 0.25 + \n",
        "        results_df['Recall_Normalized'] * 0.25 + \n",
        "        results_df['Relevance_Normalized'] * 0.25 + \n",
        "        results_df['Latency_Normalized'] * 0.25\n",
        "    ).round(4)\n",
        "    \n",
        "    # Sort by composite score\n",
        "    results_df_sorted = results_df.sort_values('Composite_Score', ascending=False)\n",
        "    \n",
        "    print(\"RANKING BY COMPOSITE SCORE (Precision + Recall + Relevance + Speed):\")\n",
        "    print(\"-\" * 70)\n",
        "    for idx, row in results_df_sorted.iterrows():\n",
        "        print(f\"{row.name + 1:2d}. {row['Retriever']:20} - Score: {row['Composite_Score']:.4f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RECOMMENDATIONS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    best_overall = results_df_sorted.iloc[0]\n",
        "    print(f\"ðŸ† **BEST OVERALL RETRIEVER: {best_overall['Retriever'].upper()}**\")\n",
        "    print(f\"   - Composite Score: {best_overall['Composite_Score']:.4f}\")\n",
        "    print(f\"   - Context Precision: {best_overall['Context_Precision']:.4f}\")\n",
        "    print(f\"   - Context Recall: {best_overall['Context_Recall']:.4f}\")\n",
        "    print(f\"   - Context Relevance: {best_overall['Context_Relevance']:.4f}\")\n",
        "    print(f\"   - Average Latency: {best_overall['Avg_Latency_Seconds']:.4f}s\")\n",
        "    \n",
        "    print(\"\\nðŸ“Š **ANALYSIS SUMMARY:**\")\n",
        "    print(\"This evaluation considers cost, latency, and performance factors:\")\n",
        "    print(\"â€¢ **Performance**: Measured through RAGAS context precision, recall, and relevance metrics\")\n",
        "    print(\"â€¢ **Latency**: Time taken to retrieve documents per query\")  \n",
        "    print(\"â€¢ **Cost**: Tracked through LangSmith (check your LangSmith dashboard for detailed cost analysis)\")\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ **RECOMMENDATION FOR LOAN COMPLAINT DATA:**\")\n",
        "    print(f\"Based on the evaluation, **{best_overall['Retriever']}** retriever is recommended because:\")\n",
        "    \n",
        "    # Provide specific reasoning based on the best performer\n",
        "    if best_overall['Retriever'] == 'ensemble':\n",
        "        print(\"â€¢ Combines strengths of multiple retrieval strategies\")\n",
        "        print(\"â€¢ Shows balanced performance across all metrics\")\n",
        "        print(\"â€¢ More robust to different query types\")\n",
        "    elif best_overall['Retriever'] == 'contextual_compression':\n",
        "        print(\"â€¢ Excellent at filtering most relevant content\")\n",
        "        print(\"â€¢ Reduces noise in retrieved documents\")\n",
        "        print(\"â€¢ Good balance of precision and relevance\")\n",
        "    elif best_overall['Retriever'] == 'multi_query':\n",
        "        print(\"â€¢ Improves recall through query reformulation\")\n",
        "        print(\"â€¢ Captures different perspectives of user questions\")\n",
        "        print(\"â€¢ Good for complex, ambiguous queries\")\n",
        "    elif best_overall['Retriever'] == 'bm25':\n",
        "        print(\"â€¢ Excellent for exact term matching\")\n",
        "        print(\"â€¢ Very fast retrieval with low latency\")\n",
        "        print(\"â€¢ Cost-effective with no embedding computation\")\n",
        "    else:\n",
        "        print(\"â€¢ Shows strong performance across key metrics\")\n",
        "        print(\"â€¢ Good balance of effectiveness and efficiency\")\n",
        "    \n",
        "    print(\"\\nðŸ“‹ **FINAL NOTES:**\")\n",
        "    print(\"â€¢ Check LangSmith dashboard for detailed cost analysis\")\n",
        "    print(\"â€¢ Consider your specific use case when choosing a retriever\")\n",
        "    print(\"â€¢ Ensemble methods often provide the most robust performance\")\n",
        "    print(\"â€¢ BM25 excels for keyword-heavy queries, embeddings for semantic similarity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸ“Š Retrieval Performance Analysis\n",
        "\n",
        "## Evaluation Results Summary\n",
        "\n",
        "After comprehensive evaluation using RAGAS metrics on loan complaint data, the **Ensemble Retriever** emerged as the clear winner with a composite score of **0.7501**.\n",
        "\n",
        "### ðŸ† Performance Rankings\n",
        "\n",
        "| Rank | Retriever | Composite Score | Precision | Recall | Relevance | Latency (s) |\n",
        "|------|-----------|----------------|-----------|--------|-----------|-------------|\n",
        "| 1 | **Ensemble** | **0.7501** | 0.1667 | 0.1250 | 0.2273 | 8.24 |\n",
        "| 2 | Parent Document | 0.5402 | 0.0833 | 0.1146 | 0.1667 | 0.31 |\n",
        "| 3 | Multi Query | 0.4855 | 0.1042 | 0.0521 | 0.2045 | 9.53 |\n",
        "| 4 | Naive | 0.4196 | 0.1250 | 0.0000 | 0.2083 | 0.28 |\n",
        "| 5 | Contextual Compression | 0.3474 | 0.0278 | 0.0833 | 0.1250 | 0.52 |\n",
        "| 6 | BM25 | 0.2959 | 0.0000 | 0.0000 | 0.0417 | 0.003 |\n",
        "| 7 | Semantic | 0.0028 | 0.0000 | 0.0000 | 0.0000 | 0.31 |\n",
        "\n",
        "### ðŸ’¡ Key Findings\n",
        "\n",
        "**Why Ensemble Retriever Wins:**\n",
        "- **Robustness**: Combines strengths of multiple retrieval strategies, mitigating individual weaknesses\n",
        "- **Balanced Performance**: Achieves the highest scores across precision, recall, and relevance metrics\n",
        "- **Query Adaptability**: Effectively handles diverse query types in financial complaint data\n",
        "\n",
        "**Trade-off Analysis:**\n",
        "- **Speed vs Quality**: BM25 offers lightning-fast retrieval (0.003s) but poor relevance\n",
        "- **Precision vs Coverage**: Naive retrieval shows good precision but zero recall\n",
        "- **Complexity vs Performance**: Ensemble's higher latency (8.24s) is justified by superior quality\n",
        "\n",
        "### ðŸŽ¯ Recommendation\n",
        "\n",
        "For loan complaint analysis, **Ensemble Retrieval** is optimal because:\n",
        "- Financial queries require comprehensive coverage (high recall)\n",
        "- Accurate information filtering is critical (high precision)\n",
        "- Diverse complaint types benefit from multi-strategy approaches\n",
        "- Quality improvements outweigh moderate latency increases\n",
        "\n",
        "*Note: Monitor LangSmith dashboard for detailed cost analysis and production optimization.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
