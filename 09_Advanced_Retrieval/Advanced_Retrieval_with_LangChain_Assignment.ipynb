{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be problems related to the mismanagement and mishandling of student loans. This includes errors in loan balances, misapplied payments, wrongful denials of payment plans, incorrect information reported on credit reports, transfers of loans without proper notification, and difficulties in applying payments correctly. Many complaints also involve issues with loan servicing companies providing bad or confusing information, and improper handling of loan data and privacy violations.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, it appears that at least one complaint was not handled in a timely manner. Specifically, the complaint with Complaint ID \\'12709087\\' submitted to MOHELA on 03/28/25 was marked as \"Timely response?\": \"No,\" indicating it was not handled promptly. The narratives mention ongoing delays and lack of responses despite assurances.\\n\\nAdditionally, several other complaints, such as those with Complaint IDs \\'12832400\\' and \\'12832400\\', were marked as \"Timely response?\": \"Yes,\" implying they were handled on time. However, the complaint with ID \\'12709087\\' clearly indicates a failure to respond in a timely manner.\\n\\nTherefore, yes, there were complaints that did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily because of a combination of mismanagement, lack of clear communication, and financial hardship. Many borrowers were not adequately informed about the status of their loans, including transfer of servicing companies, payment resumption dates, or changes in payment plans. Some were unaware that interest would continue to accrue during forbearance or deferment periods, making the debt grow rather than decrease. Others faced financial difficulties due to stagnant wages, economic downturns, or personal hardships, which made it impossible to increase payments or meet the required repayment schedules. Additionally, complex or unhelpful loan repayment options, as well as administrative errors such as incorrect reporting or failure to notify about payment due dates, contributed to borrowers falling behind. Overall, issues related to poor communication, unexpected interest accumulation, and economic challenges led many to struggle with repaying their loans.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be related to dealing with lenders or servicers, specifically issues such as misreported or confusing loan information, difficulty in making payments or applying funds correctly, and disputes over fees or loan terms. Many complaints involve borrowers receiving incorrect or bad information about their loans, problems with repayment processes, and issues with the accuracy of loan balances and interest calculations.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there are complaints that did not get handled in a timely manner. In particular, one complaint describes a situation where the consumer waited over several minutes (with some details redacted as \"XXXX\") on a call, and eventually had to hang up without resolution, indicating the complaint was not addressed promptly. Additionally, the overall tone of multiple complaints suggests ongoing difficulties with communication and resolution times.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People often fail to pay back their loans due to various reasons such as problems with their payment plans, miscommunication or lack of communication from the loan servicers, errors in processing payments, and issues related to loan transfers or automatic payments being unenrolled without proper notification. In some cases, borrowers are unaware of their current debt status, or their requests for deferment or forbearance are not properly addressed, leading to continued billing and late fees. Additionally, deceptive practices or mismanagement by loan servicing companies can contribute to borrowers' inability to fulfill repayment obligations.\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "**Example Query: \"Why did people fail to pay back their loans?\"**\n",
        "\n",
        "Based on the invocations in this notebook, BM25 performs better than embeddings for this query.\n",
        "\n",
        "**Comparison of responses:**\n",
        "\n",
        "**Naive Retrieval (Embeddings):** Provided a broad, conceptual response covering systemic issues, miscommunication, and financial constraints but was more general in nature.\n",
        "\n",
        "**BM25 Retrieval:** Delivered more specific, actionable details including:\n",
        "- \"unenrolled from autopay without their knowledge\"\n",
        "- \"payment reversals\" \n",
        "- \"steered into improper forbearances\"\n",
        "- \"capitalized interest\"\n",
        "- \"loan transfer process\"\n",
        "\n",
        "**Why BM25 is better here:**\n",
        "\n",
        "1. **Exact Term Matching**: BM25 excels at finding documents containing specific financial and procedural terminology like \"autopay\", \"forbearances\", \"capitalized interest\" - terms that are crucial for understanding concrete loan servicing problems.\n",
        "\n",
        "2. **Factual Precision**: The query asks for specific reasons why payments failed. BM25's keyword-based approach captures precise procedural failures and technical issues that directly answer the \"why\" question.\n",
        "\n",
        "3. **Domain-Specific Language**: In financial/loan contexts, exact terminology matters immensely. BM25's ability to match specific loan servicing terms provides more actionable and legally/procedurally accurate information than semantic similarity alone.\n",
        "\n",
        "4. **Reduced Semantic Drift**: Embeddings might retrieve conceptually similar but less precise content, whereas BM25 stays focused on documents containing the exact operational terms that explain payment failures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, particularly student loans as indicated in the provided complaints, appears to be problems related to dealing with lenders or servicers. Specific issues include errors in loan balances, misapplied payments, wrongful denials of payment plans, incorrect or misleading information, lack of proper communication, and mishandling of loan data. These issues often lead to confusion, inaccurate credit reporting, and difficulties in managing repayment.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints did not get handled in a timely manner. For example, there is a complaint from a consumer who has been waiting over a year for a response and resolution to their request regarding loan account issues, with nearly 18 months having passed without resolution. This indicates that a complaint was not addressed in a timely manner.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans mainly because they were often misled or not fully informed about their repayment obligations, and faced difficulties managing their loan payments due to accumulating interest and financial hardships. Specifically, some borrowers were unaware they had to repay their student loans until long after taking them out, and they did not receive clear information about interest accrual, loan transfer processes, or payment plans. Additionally, options like deferment or forbearance did not prevent interest from growing, which increased the total amount owed over time. Many borrowers also felt that the financial burdens and repayment terms made it impossible to pay off their loans without sacrificing their basic living expenses, especially when they could not qualify for loan forgiveness programs. Overall, lack of clear communication, unexpected interest accumulation, and economic hardships contributed to failure to repay loans.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with student loans tend to revolve around:\\n\\n- Errors or discrepancies in loan balances and interest calculations.\\n- Problems with loan servicing, including mismanagement and improper handling of payments.\\n- Lack of proper communication or notification about account status, transfers, or late payments.\\n- Difficulties in obtaining accurate information or validation of loans.\\n- Issues related to loan transfers between agencies or servicers without proper notification.\\n- Problems with repayment plans, including being steered into unsuitable options or being unable to make principal payments.\\n- Violations of borrower rights, including privacy breaches or improper data handling.\\n- Problems with loan discharge, forgiveness, or legal disputes over enforceability.\\n\\nWhile specific issues vary, a common theme is the mismanagement or mishandling of loan information and payments, leading to confusion, inaccurate reporting, and financial hardship.\\n\\nTherefore, the most common issue appears to be **problems with loan servicing, including errors in balances, interest, and improper handling of payments and transfers.**\\n\\nIf you need a specific summary or have other questions, feel free to ask!'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, yes, some complaints were not handled in a timely manner. For example:\\n\\n- Complaint ID 12739706 against MOHELA, submitted on 04/01/25, was marked as \"No\" for timely response, indicating it was not handled promptly.\\n- Similarly, complaint ID 12709087 also against MOHELA, submitted on 03/28/25, was marked as \"No\" for timely response.\\n\\nHowever, many other complaints received responses marked as \"Yes,\" indicating they were handled timely.\\n\\nIn conclusion, at least some complaints reported delays or failures in response times.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of systemic servicing issues, misinformation, legal discrepancies, and financial hardships. The complaints indicate that borrowers often received bad information, were misled into forbearance or consolidation practices that increased their debt due to interest capitalization, and were not properly informed about available repayment options such as income-driven plans or rehabilitation programs. Additionally, some borrowers experienced disputes over inaccurate account information, legal violations, and the negative impact of loan management errors on their credit scores, making repayment even more difficult. Overall, these factors created obstacles that prevented many from successfully repaying their loans.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "Generating multiple reformulations of a user query can significantly improve recall through several mechanisms:\n",
        "\n",
        "**1. Vocabulary Mismatch Reduction:**\n",
        "- Users and document authors often use different words to describe the same concepts\n",
        "- Multiple reformulations increase the likelihood of matching the exact terminology used in relevant documents\n",
        "- Example: A user asking \"payment issues\" might miss documents that use \"billing problems\" or \"transaction difficulties\"\n",
        "\n",
        "**2. Query Perspective Diversification:**\n",
        "- Different reformulations can approach the same topic from various angles\n",
        "- Each perspective might surface different relevant documents that focus on specific aspects\n",
        "- Example: \"Why did loans fail?\" vs \"What caused borrower defaults?\" vs \"Reasons for payment problems\"\n",
        "\n",
        "**3. Semantic Coverage Expansion:**\n",
        "- LLMs can generate reformulations that capture different semantic nuances of the original query\n",
        "- This helps retrieve documents that are conceptually relevant but use different language patterns\n",
        "- Increases the semantic search space beyond the original query's limited scope\n",
        "\n",
        "**4. Synonym and Paraphrase Utilization:**\n",
        "- Reformulations naturally incorporate synonyms and paraphrases\n",
        "- Documents using alternative terminology become discoverable\n",
        "- Reduces dependency on exact keyword matches\n",
        "\n",
        "**5. Comprehensive Document Retrieval:**\n",
        "- By retrieving documents for each reformulated query and taking the union of all results\n",
        "- The final context includes a broader set of potentially relevant documents\n",
        "- Higher chance of including the most relevant information that might have been missed by a single query\n",
        "\n",
        "**Implementation in Multi-Query Retriever:**\n",
        "The Multi-Query Retriever demonstrates this by:\n",
        "1. Using an LLM to generate multiple query variations\n",
        "2. Running retrieval for each variation\n",
        "3. Combining all unique documents into the final context\n",
        "4. Providing richer, more comprehensive information for answer generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be related to misconduct by loan servicers, including errors in loan balances, misapplication of payments, wrongful denials of payment plans, and incorrect or unverified reporting to credit bureaus. Additionally, issues such as discrepancies in interest rates, improper account handling, and problems stemming from loan transfers or sale of loans are prevalent.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, several complaints were identified as not being handled in a timely manner. Specifically, the complaints about the delayed responses from Mohela regarding student loan applications and payments were marked as \"Timely response?\": \"No\" in the records. For instance, the complaint from row 441 and row 84 both indicate that the responses were not timely. Therefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People often fail to pay back their loans due to a variety of reasons. Based on the provided context, some common reasons include:\\n\\n1. Financial Hardship: Borrowers experience severe financial difficulties that make it difficult to make loan payments, such as unemployment, health issues, or other financial burdens.\\n2. Lack of Proper Information or Miscommunication: Borrowers may not be properly informed about when payments are due, payment plans, or the terms of their loans, leading to unintentional delinquency.\\n3. Institutional Misconduct or Misrepresentation: Some borrowers were misled about the value of their education, the manageability of their loans, or the financial stability of their educational institution, resulting in unexpected financial burdens and difficulty in repayment.\\n4. Issues with Loan Servicing: Problems such as failure to notify borrowers of repayment obligations, errors in reporting late payments, or improper handling of account changes can hinder repayment efforts.\\n5. Use of Deferment or Forbearance: Borrowers may rely on deferment or forbearance, which often increases the overall debt due to accrued interest, making repayment more difficult later.\\n6. Institutional Closures or Dissolution: Closure of educational institutions or changes in loan ownership can disrupt payment plans and lead to confusion or inability to maintain payments.\\n   \\nIn summary, failure to repay loans can stem from financial difficulties, lack of clear communication, institutional misconduct, and administrative errors.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issues with loans, based on the complaints, include:\\n\\n- Errors or inaccuracies in loan balances and interest calculations\\n- Poor communication from servicers, including lack of notices or notifications\\n- Problems with how payments are applied, often limited to interest rather than principal\\n- Unauthorized or improper transfer or sale of loans without borrower notice\\n- Bad or misleading information reported to credit bureaus, leading to credit score damages\\n- Coercive servicing practices such as steering into forbearance or consolidation without providing all options\\n- Unexplained or incorrect delinquency and default reporting\\n- Inadequate investigation or resolution of disputes about loan terms, balances, or misconduct\\n- Data breaches or improper disclosures violating privacy laws like FERPA\\n- Challenges in obtaining documentation or proof of original loan agreements, transfers, and legal authority\\n\\nOverall, the most common issues center around mismanagement and lack of transparency, leading to inaccurate reporting, increased debt, and borrower hardship.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the complaints provided, yes, several complaints indicate that complaints were not handled in a timely manner. Specifically, there are multiple instances where responses from the companies were marked as \"No\" or \"Not timely,\" such as:\\n\\n- Complaint ID: 12935889 (Mohela, MD) â€” Response was \"No\" for timeliness.\\n- Complaint ID: 12654977 (Mohela, MD) â€” Response was \"No\" for timeliness.\\n- Complaint ID: 12739706 (Mohela, NJ) â€” Response was \"No\" for timeliness.\\n- Complaint ID: 12744910 (Maximus Federal Services, KY) â€” Response was \"Yes\" for timeliness.\\n- Complaint ID: 12823876 (EdFinancial Services, CA) â€” Response was \"Yes\" for timeliness.\\n- Complaint ID: 12516723 (EdFinancial Services, CA) â€” Response was \"Yes\" for timeliness.\\n- Multiple complaints regarding failures of companies like Maximus Federal Services/Aidvantage and TransUnion showing they either did not respond or responded after delays, with some explicitly indicating that their complaints or disputes were not addressed in the required timeframes.\\n\\nGiven these records, several complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People often failed to pay back their loans due to a variety of complex issues, including:\\n\\n1. Lack of clear or adequate information from lenders or servicers about repayment options, interest accumulation, and possible forgiveness programs, leading to misconceptions about their obligations.\\n2. Being steered into forbearance or deferment, which allowed interest to continue accruing and increased the total debt over time.\\n3. Financial hardships such as unemployment, low income, health issues, or unexpected expenses like homelessness or accidents, which made payments unmanageable.\\n4. Poor communication or failure of loan servicers to notify borrowers about due dates, payment statuses, or transfers between servicers, resulting in missed payments or incorrect delinquency reporting.\\n5. Administrative errors, such as incorrect account information, improper reporting to credit bureaus, or mishandling of payments, which damaged credit scores and created barriers to further borrowing.\\n6. Mismanagement or deceptive practices by loan servicers, including improper interest capitalization, bad advice, or long-term forbearances with no transparent explanation, causing the debt to grow and repayment to become unrealistic.\\n7. Borrowers often did not qualify for debt forgiveness programs they believed they were eligible for, adding to their financial burden.\\n   \\nIn summary, failure to pay back loans often resulted from a combination of these systemic issues, miscommunication, unexpected financial hardships, and predatory or ineffective loan management practices.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with loans appear to be related to miscommunication or errors in loan servicing, such as problems with repayment plans, incorrect account status or reporting, difficulties with auto-debit setup, and disputes over loan balances or default status. \\n\\nWhile the specific \"most common issue\" is not explicitly stated, recurring themes include:\\n\\n- Problems with repayment and payment processing\\n- Incorrect or disputed account status and reporting\\n- Lack of clear communication from servicers\\n- Issues with loan forgiveness or discharge processes\\n- Breaches of privacy or improper use of personal data\\n\\nIn summary, the most common issue seems to be **problems related to loan servicing, including payment handling, account status, and communication difficulties**.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, all the complaints included responses labeled as \"Closed with explanation,\" and the \"Timely response?\" field indicates \"Yes\" for each. This suggests that, according to the recorded data, no complaints remain unhandled or unresolved due to delays. \\n\\nHowever, it is important to note that some complaints mention issues such as lack of response or ongoing disputes, but the official status in the data indicates that they were responded to within the expected timeframe.\\n\\nTherefore, the answer is: **No, there are no complaints recorded in this data set that were left unhandled or not handled in a timely manner.**'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including issues related to miscommunication, administrative delays, and disputes over the legitimacy or status of their loans. For example, some borrowers experienced trouble due to receiving incorrect or bad information from lenders or servicers regarding their loan status or repayment terms. Others encountered difficulties with the handling of payments, such as payments not being processed correctly or being rejected despite sufficient funds. Additionally, some borrowers faced complications arising from loan transfer issues, inaccurate reporting, or legal disputes concerning the validity of their debts. In some cases, borrowers felt that their personal information was mishandled or compromised, leading to further complications. Overall, these issues highlight challenges such as lack of transparency, administrative delays, or legal and informational disputes that contributed to borrowers' inability to successfully repay their loans.\""
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### âœ… Answer:\n",
        "\n",
        "**How Semantic Chunking Behaves with Short, Repetitive Sentences:**\n",
        "\n",
        "**1. Problematic Similarity Patterns:**\n",
        "- Short, repetitive sentences (like FAQ questions) often have very high semantic similarity scores\n",
        "- The algorithm may struggle to find meaningful breakpoints between semantically similar content\n",
        "- Could result in either overly large chunks (everything grouped together) or overly small chunks (no grouping occurs)\n",
        "\n",
        "**2. Threshold Sensitivity Issues:**\n",
        "- With highly similar content, small variations in similarity scores become less meaningful\n",
        "- Percentile-based thresholding may not work effectively when most distances are very close\n",
        "- The algorithm might either chunk everything together or keep everything separate\n",
        "\n",
        "**3. Loss of Logical Structure:**\n",
        "- FAQs have inherent question-answer pairs that should ideally stay together\n",
        "- Semantic chunking might break these logical pairs if focusing purely on sentence-level similarity\n",
        "- Context and structure information gets lost in favor of semantic similarity\n",
        "\n",
        "**Adjustments to Improve Performance:**\n",
        "\n",
        "**1. Threshold Method Modifications:**\n",
        "```python\n",
        "# Use more aggressive thresholding\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"standard_deviation\",  # More sensitive to small differences\n",
        "    breakpoint_threshold_amount=1.0  # Lower threshold for more breaks\n",
        ")\n",
        "```\n",
        "\n",
        "**2. Pre-processing Strategies:**\n",
        "- Identify FAQ structure patterns (Q: A: formatting)\n",
        "- Use regex to detect question-answer pairs\n",
        "- Apply metadata-based chunking to preserve logical pairs\n",
        "\n",
        "**3. Hybrid Approaches:**\n",
        "```python\n",
        "# Combine semantic chunking with structural rules\n",
        "- First chunk by logical structure (Q-A pairs)\n",
        "- Then apply semantic chunking within those logical boundaries\n",
        "- Use custom splitting logic for FAQ-specific patterns\n",
        "```\n",
        "\n",
        "**4. Alternative Chunking Methods:**\n",
        "- **Fixed-size chunking** with Q-A pair preservation\n",
        "- **Metadata-based chunking** using FAQ structure\n",
        "- **Custom splitters** that understand FAQ formatting\n",
        "\n",
        "**5. Enhanced Similarity Calculation:**\n",
        "- Use more sophisticated embedding models that better capture subtle differences\n",
        "- Apply domain-specific embeddings trained on FAQ data\n",
        "- Consider using multiple embedding dimensions for better differentiation\n",
        "\n",
        "**Recommended Implementation for FAQs:**\n",
        "Rather than relying purely on semantic chunking, use a structured approach that preserves the logical Q-A relationships while still benefiting from semantic organization for related topics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "677bdb6b958f49b68430b43f385882df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cf09e58ed6443d49dddff54d062d4e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f5e9b3a01044d4d974831f194345b66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node '9753e5'. Skipping!\n",
            "Property 'summary' already exists in node '295764'. Skipping!\n",
            "Property 'summary' already exists in node 'caae16'. Skipping!\n",
            "Property 'summary' already exists in node 'c5a4e3'. Skipping!\n",
            "Property 'summary' already exists in node 'b28cdc'. Skipping!\n",
            "Property 'summary' already exists in node '9e77b8'. Skipping!\n",
            "Property 'summary' already exists in node 'bbb038'. Skipping!\n",
            "Property 'summary' already exists in node 'c4415d'. Skipping!\n",
            "Property 'summary' already exists in node 'e9a3e0'. Skipping!\n",
            "Property 'summary' already exists in node '78d57e'. Skipping!\n",
            "Property 'summary' already exists in node '28afad'. Skipping!\n",
            "Property 'summary' already exists in node '2e9c99'. Skipping!\n",
            "Property 'summary' already exists in node '46810f'. Skipping!\n",
            "Property 'summary' already exists in node '2594b9'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7026e338f658496781b30a7b55acf5b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cff630d6ede740d6836918b09b886906",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/41 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node '46810f'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'c4415d'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '78d57e'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'e9a3e0'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '2e9c99'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '295764'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '2594b9'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'b28cdc'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'bbb038'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '9e77b8'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'caae16'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '28afad'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'c5a4e3'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '9753e5'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b67573a18434797860eb2cef4a6d2e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4400afe8a5b44eae8f526b1661f6f89a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44dda285813a4649b53a6ef55249c211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ef2ed127f447738eb9db4041e8b3cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(\n",
        "    llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does the use of BBAY 3 relate to Direct Lo...</td>\n",
              "      <td>[non-term (includes clock-hour calendars), or ...</td>\n",
              "      <td>If substantially equal nonstandard terms in a ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Are there exceptions to the normal loan period...</td>\n",
              "      <td>[Inclusion of Clinical Work in a Standard Term...</td>\n",
              "      <td>Yes, there are exceptions to the normal loan p...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the payment period requirement for Tit...</td>\n",
              "      <td>[Non-Term Characteristics A program that measu...</td>\n",
              "      <td>Title IV program disbursements, except for Fed...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Whaat informashun does Volume 8 provide regard...</td>\n",
              "      <td>[both the credit or clock hours and the weeks ...</td>\n",
              "      <td>Volume 8, Chapters 5 and 6, contains informati...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the definition and characteristics of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a St...</td>\n",
              "      <td>Nonstandard terms are defined as terms that ar...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the disbursement requirements for feder...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nboth the credit or clock hours and...</td>\n",
              "      <td>In clock-hour or non-term credit-hour programs...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What are the disbursement requirements for fed...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nboth the credit or clock hours and...</td>\n",
              "      <td>For federal student aid programs such as Pell ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wut is a nonstanderd term and how is a non-ter...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a St...</td>\n",
              "      <td>A nonstandard term is a term that is not a sem...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How do the disbursement requirements for Title...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nDisbursement Timing in Subscriptio...</td>\n",
              "      <td>In subscription-based programs, for the first ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How do the examples in Appendix A and Appendix...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nDisbursement Timing in Subscriptio...</td>\n",
              "      <td>The examples in Appendix A illustrate the prin...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does the structure of non-term and subscri...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nnon-term (includes clock-hour cale...</td>\n",
              "      <td>In non-term and subscription-based academic ca...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How do the requirements outlined in Volume 8, ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nnon-term (includes clock-hour cale...</td>\n",
              "      <td>Volume 8, Chapter 6 specifies that subscriptio...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   How does the use of BBAY 3 relate to Direct Lo...   \n",
              "1   Are there exceptions to the normal loan period...   \n",
              "2   What is the payment period requirement for Tit...   \n",
              "3   Whaat informashun does Volume 8 provide regard...   \n",
              "4   What are the definition and characteristics of...   \n",
              "5   How do the disbursement requirements for feder...   \n",
              "6   What are the disbursement requirements for fed...   \n",
              "7   wut is a nonstanderd term and how is a non-ter...   \n",
              "8   How do the disbursement requirements for Title...   \n",
              "9   How do the examples in Appendix A and Appendix...   \n",
              "10  How does the structure of non-term and subscri...   \n",
              "11  How do the requirements outlined in Volume 8, ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [non-term (includes clock-hour calendars), or ...   \n",
              "1   [Inclusion of Clinical Work in a Standard Term...   \n",
              "2   [Non-Term Characteristics A program that measu...   \n",
              "3   [both the credit or clock hours and the weeks ...   \n",
              "4   [<1-hop>\\n\\nInclusion of Clinical Work in a St...   \n",
              "5   [<1-hop>\\n\\nboth the credit or clock hours and...   \n",
              "6   [<1-hop>\\n\\nboth the credit or clock hours and...   \n",
              "7   [<1-hop>\\n\\nInclusion of Clinical Work in a St...   \n",
              "8   [<1-hop>\\n\\nDisbursement Timing in Subscriptio...   \n",
              "9   [<1-hop>\\n\\nDisbursement Timing in Subscriptio...   \n",
              "10  [<1-hop>\\n\\nnon-term (includes clock-hour cale...   \n",
              "11  [<1-hop>\\n\\nnon-term (includes clock-hour cale...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   If substantially equal nonstandard terms in a ...   \n",
              "1   Yes, there are exceptions to the normal loan p...   \n",
              "2   Title IV program disbursements, except for Fed...   \n",
              "3   Volume 8, Chapters 5 and 6, contains informati...   \n",
              "4   Nonstandard terms are defined as terms that ar...   \n",
              "5   In clock-hour or non-term credit-hour programs...   \n",
              "6   For federal student aid programs such as Pell ...   \n",
              "7   A nonstandard term is a term that is not a sem...   \n",
              "8   In subscription-based programs, for the first ...   \n",
              "9   The examples in Appendix A illustrate the prin...   \n",
              "10  In non-term and subscription-based academic ca...   \n",
              "11  Volume 8, Chapter 6 specifies that subscriptio...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up LangSmith for cost and latency tracking\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"LangSmith API Key:\")\n",
        "\n",
        "# Enable LangSmith tracing\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"retriever-evaluation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for evaluation\n",
        "from ragas.metrics import ContextPrecision, ContextRecall, ContextRelevance\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
        "import pandas as pd\n",
        "import time\n",
        "from langsmith import traceable\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAGAS evaluator setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Set up RAGAS evaluator with LLM wrapper\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "# Define RAGAS metrics for retriever evaluation\n",
        "ragas_metrics = [\n",
        "    ContextPrecision(llm=evaluator_llm),\n",
        "    ContextRecall(llm=evaluator_llm), \n",
        "    ContextRelevance(llm=evaluator_llm)\n",
        "]\n",
        "\n",
        "print(\"RAGAS evaluator setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 12 evaluation samples\n",
            "Sample structure: dict_keys(['user_input', 'reference_contexts', 'reference'])\n"
          ]
        }
      ],
      "source": [
        "# Convert the generated dataset to RAGAS format\n",
        "test_df = dataset.to_pandas()\n",
        "\n",
        "# Create evaluation samples from the test dataset\n",
        "evaluation_samples = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    sample = {\n",
        "        'user_input': row['user_input'],\n",
        "        'reference_contexts': row['reference_contexts'],\n",
        "        'reference': row['reference']\n",
        "    }\n",
        "    evaluation_samples.append(sample)\n",
        "\n",
        "print(f\"Created {len(evaluation_samples)} evaluation samples\")\n",
        "print(\"Sample structure:\", evaluation_samples[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retriever evaluation function created!\n"
          ]
        }
      ],
      "source": [
        "# Create a comprehensive evaluation function for retrievers\n",
        "@traceable(name=\"retriever_evaluation\")\n",
        "def evaluate_retriever(retriever, retriever_name, test_samples, use_semantic_chunking=False):\n",
        "    \"\"\"\n",
        "    Evaluate a retriever using RAGAS metrics with cost and latency tracking\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    total_latency = 0\n",
        "    \n",
        "    print(f\"Evaluating {retriever_name} (Semantic Chunking: {use_semantic_chunking})...\")\n",
        "    \n",
        "    for i, sample in enumerate(test_samples):\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Retrieve documents using the retriever\n",
        "            retrieved_docs = retriever.invoke(sample['user_input'])\n",
        "            \n",
        "            # Extract context from retrieved documents\n",
        "            retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
        "            \n",
        "            end_time = time.time()\n",
        "            latency = end_time - start_time\n",
        "            total_latency += latency\n",
        "            \n",
        "            # Create RAGAS sample\n",
        "            ragas_sample = {\n",
        "                'user_input': sample['user_input'],\n",
        "                'retrieved_contexts': retrieved_contexts,\n",
        "                'reference_contexts': sample['reference_contexts'],\n",
        "                'reference': sample['reference']\n",
        "            }\n",
        "            \n",
        "            results.append(ragas_sample)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating sample {i}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    # Convert to RAGAS dataset format\n",
        "    ragas_dataset = EvaluationDataset.from_list(results)\n",
        "    \n",
        "    # Run RAGAS evaluation\n",
        "    evaluation_result = evaluate(dataset=ragas_dataset, metrics=ragas_metrics, llm=evaluator_llm)\n",
        "    \n",
        "    avg_latency = total_latency / len(test_samples) if test_samples else 0\n",
        "    \n",
        "    return {\n",
        "        'retriever_name': retriever_name,\n",
        "        'semantic_chunking': use_semantic_chunking,\n",
        "        'metrics': evaluation_result,\n",
        "        'avg_latency_seconds': avg_latency,\n",
        "        'total_samples': len(test_samples),\n",
        "        'successful_samples': len(results)\n",
        "    }\n",
        "\n",
        "print(\"Retriever evaluation function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating semantic chunking versions of retrievers...\n"
          ]
        }
      ],
      "source": [
        "# Create semantic chunking versions of all retrievers\n",
        "print(\"Creating semantic chunking versions of retrievers...\")\n",
        "\n",
        "# Use more documents for semantic chunking to get better evaluation results\n",
        "semantic_documents_full = semantic_chunker.split_documents(loan_complaint_data[:100])\n",
        "\n",
        "# Create semantic vectorstore with more documents\n",
        "semantic_vectorstore_full = Qdrant.from_documents(\n",
        "    semantic_documents_full,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Semantic_Full_Eval\"\n",
        ")\n",
        "\n",
        "# Semantic versions of retrievers\n",
        "semantic_naive_retriever = semantic_vectorstore_full.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# Semantic BM25 retriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "semantic_bm25_retriever = BM25Retriever.from_documents(semantic_documents_full)\n",
        "\n",
        "# Semantic compression retriever\n",
        "semantic_compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, \n",
        "    base_retriever=semantic_naive_retriever\n",
        ")\n",
        "\n",
        "# Semantic multi-query retriever  \n",
        "semantic_multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=semantic_naive_retriever, \n",
        "    llm=chat_model\n",
        ")\n",
        "\n",
        "# Semantic parent document retriever\n",
        "semantic_client = QdrantClient(location=\":memory:\")\n",
        "semantic_client.create_collection(\n",
        "    collection_name=\"semantic_parent_docs\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "semantic_parent_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"semantic_parent_docs\", \n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
        "    client=semantic_client\n",
        ")\n",
        "\n",
        "semantic_store = InMemoryStore()\n",
        "semantic_parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=semantic_parent_vectorstore,\n",
        "    docstore=semantic_store,\n",
        "    child_splitter=child_splitter,\n",
        ")\n",
        "\n",
        "# Add semantic documents to parent retriever\n",
        "semantic_parent_document_retriever.add_documents(semantic_documents_full, ids=None)\n",
        "\n",
        "# Semantic ensemble retriever\n",
        "semantic_ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[\n",
        "        semantic_bm25_retriever, \n",
        "        semantic_naive_retriever, \n",
        "        semantic_parent_document_retriever, \n",
        "        semantic_compression_retriever, \n",
        "        semantic_multi_query_retriever\n",
        "    ], \n",
        "    weights=[0.2, 0.2, 0.2, 0.2, 0.2]\n",
        ")\n",
        "\n",
        "print(\"Semantic chunking versions created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluations for all retrievers\n",
        "results = []\n",
        "\n",
        "# Define retrievers to evaluate\n",
        "retrievers_to_evaluate = [\n",
        "    # Original retrievers (without semantic chunking)\n",
        "    (naive_retriever, \"Naive RAG\", False),\n",
        "    (bm25_retriever, \"BM25\", False),\n",
        "    (compression_retriever, \"Contextual Compression\", False),\n",
        "    (multi_query_retriever, \"Multi-Query\", False),\n",
        "    (parent_document_retriever, \"Parent Document\", False),\n",
        "    (ensemble_retriever, \"Ensemble\", False),\n",
        "    \n",
        "    # Semantic chunking versions\n",
        "    (semantic_naive_retriever, \"Naive RAG\", True),\n",
        "    (semantic_bm25_retriever, \"BM25\", True),\n",
        "    (semantic_compression_retriever, \"Contextual Compression\", True),\n",
        "    (semantic_multi_query_retriever, \"Multi-Query\", True),\n",
        "    (semantic_parent_document_retriever, \"Parent Document\", True),\n",
        "    (semantic_ensemble_retriever, \"Ensemble\", True),\n",
        "]\n",
        "\n",
        "print(\"Starting comprehensive retriever evaluation...\")\n",
        "print(f\"Evaluating {len(retrievers_to_evaluate)} retriever configurations...\")\n",
        "\n",
        "# Run evaluations\n",
        "for retriever, name, semantic in retrievers_to_evaluate:\n",
        "    try:\n",
        "        result = evaluate_retriever(retriever, name, evaluation_samples, semantic)\n",
        "        results.append(result)\n",
        "        print(f\"âœ“ Completed: {name} (Semantic: {semantic})\")\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error evaluating {name} (Semantic: {semantic}): {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\\\nCompleted evaluation of {len(results)} retriever configurations!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze and compile results\n",
        "print(\"Analyzing results...\")\n",
        "\n",
        "# Create a comprehensive results DataFrame\n",
        "compiled_results = []\n",
        "\n",
        "for result in results:\n",
        "    metrics_dict = result['metrics']\n",
        "    \n",
        "    row = {\n",
        "        'Retriever': result['retriever_name'],\n",
        "        'Semantic_Chunking': result['semantic_chunking'],\n",
        "        'Context_Precision': metrics_dict.get('context_precision', 0),\n",
        "        'Context_Recall': metrics_dict.get('context_recall', 0),\n",
        "        'Context_Relevance': metrics_dict.get('context_relevance', 0),\n",
        "        'Avg_Latency_Seconds': result['avg_latency_seconds'],\n",
        "        'Total_Samples': result['total_samples'],\n",
        "        'Successful_Samples': result['successful_samples'],\n",
        "        'Success_Rate': result['successful_samples'] / result['total_samples'] if result['total_samples'] > 0 else 0\n",
        "    }\n",
        "    compiled_results.append(row)\n",
        "\n",
        "results_df = pd.DataFrame(compiled_results)\n",
        "\n",
        "# Display results\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"RETRIEVER EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.round(4).to_string(index=False))\n",
        "\n",
        "# Calculate summary statistics\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Group by semantic chunking\n",
        "semantic_comparison = results_df.groupby('Semantic_Chunking').agg({\n",
        "    'Context_Precision': 'mean',\n",
        "    'Context_Recall': 'mean', \n",
        "    'Context_Relevance': 'mean',\n",
        "    'Avg_Latency_Seconds': 'mean'\n",
        "}).round(4)\n",
        "\n",
        "print(\"\\\\nSemantic Chunking Impact:\")\n",
        "print(semantic_comparison)\n",
        "\n",
        "# Find best performers\n",
        "print(\"\\\\nTop Performers by Metric:\")\n",
        "print(f\"Best Context Precision: {results_df.loc[results_df['Context_Precision'].idxmax(), 'Retriever']} (Semantic: {results_df.loc[results_df['Context_Precision'].idxmax(), 'Semantic_Chunking']}) - {results_df['Context_Precision'].max():.4f}\")\n",
        "print(f\"Best Context Recall: {results_df.loc[results_df['Context_Recall'].idxmax(), 'Retriever']} (Semantic: {results_df.loc[results_df['Context_Recall'].idxmax(), 'Semantic_Chunking']}) - {results_df['Context_Recall'].max():.4f}\")\n",
        "print(f\"Best Context Relevance: {results_df.loc[results_df['Context_Relevance'].idxmax(), 'Retriever']} (Semantic: {results_df.loc[results_df['Context_Relevance'].idxmax(), 'Semantic_Chunking']}) - {results_df['Context_Relevance'].max():.4f}\")\n",
        "print(f\"Fastest Retriever: {results_df.loc[results_df['Avg_Latency_Seconds'].idxmin(), 'Retriever']} (Semantic: {results_df.loc[results_df['Avg_Latency_Seconds'].idxmin(), 'Semantic_Chunking']}) - {results_df['Avg_Latency_Seconds'].min():.4f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost analysis using LangSmith data\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"COST AND EFFICIENCY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate efficiency scores\n",
        "results_df['Overall_Performance'] = (\n",
        "    results_df['Context_Precision'] + \n",
        "    results_df['Context_Recall'] + \n",
        "    results_df['Context_Relevance']\n",
        ") / 3\n",
        "\n",
        "# Calculate efficiency (performance per second)\n",
        "results_df['Efficiency_Score'] = results_df['Overall_Performance'] / results_df['Avg_Latency_Seconds']\n",
        "\n",
        "# Sort by overall performance\n",
        "top_performers = results_df.sort_values('Overall_Performance', ascending=False)\n",
        "\n",
        "print(\"\\\\nRanked by Overall Performance:\")\n",
        "print(top_performers[['Retriever', 'Semantic_Chunking', 'Overall_Performance', 'Avg_Latency_Seconds', 'Efficiency_Score']].round(4).to_string(index=False))\n",
        "\n",
        "# Efficiency analysis\n",
        "print(\"\\\\nEfficiency Analysis (Performance per Second):\")\n",
        "efficiency_ranking = results_df.sort_values('Efficiency_Score', ascending=False)\n",
        "print(efficiency_ranking[['Retriever', 'Semantic_Chunking', 'Efficiency_Score', 'Overall_Performance', 'Avg_Latency_Seconds']].round(4).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the plotting style\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Performance comparison by retriever\n",
        "ax1 = axes[0, 0]\n",
        "performance_data = results_df.pivot(index='Retriever', columns='Semantic_Chunking', values='Overall_Performance')\n",
        "performance_data.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
        "ax1.set_title('Overall Performance by Retriever\\\\n(With vs Without Semantic Chunking)')\n",
        "ax1.set_ylabel('Overall Performance Score')\n",
        "ax1.legend(['Without Semantic', 'With Semantic'])\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. Latency comparison\n",
        "ax2 = axes[0, 1]\n",
        "latency_data = results_df.pivot(index='Retriever', columns='Semantic_Chunking', values='Avg_Latency_Seconds')\n",
        "latency_data.plot(kind='bar', ax=ax2, color=['lightgreen', 'orange'])\n",
        "ax2.set_title('Average Latency by Retriever\\\\n(With vs Without Semantic Chunking)')\n",
        "ax2.set_ylabel('Average Latency (seconds)')\n",
        "ax2.legend(['Without Semantic', 'With Semantic'])\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Performance vs Latency scatter plot\n",
        "ax3 = axes[1, 0]\n",
        "semantic_true = results_df[results_df['Semantic_Chunking'] == True]\n",
        "semantic_false = results_df[results_df['Semantic_Chunking'] == False]\n",
        "\n",
        "ax3.scatter(semantic_false['Avg_Latency_Seconds'], semantic_false['Overall_Performance'], \n",
        "           label='Without Semantic', alpha=0.7, s=100, color='skyblue')\n",
        "ax3.scatter(semantic_true['Avg_Latency_Seconds'], semantic_true['Overall_Performance'], \n",
        "           label='With Semantic', alpha=0.7, s=100, color='lightcoral')\n",
        "\n",
        "# Add retriever labels\n",
        "for i, row in results_df.iterrows():\n",
        "    ax3.annotate(row['Retriever'][:8], \n",
        "                (row['Avg_Latency_Seconds'], row['Overall_Performance']),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "ax3.set_xlabel('Average Latency (seconds)')\n",
        "ax3.set_ylabel('Overall Performance Score')\n",
        "ax3.set_title('Performance vs Latency Trade-off')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Metric comparison heatmap\n",
        "ax4 = axes[1, 1]\n",
        "metrics_data = results_df.set_index(['Retriever', 'Semantic_Chunking'])[['Context_Precision', 'Context_Recall', 'Context_Relevance']]\n",
        "heatmap_data = metrics_data.T\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4, cbar_kws={'shrink': 0.8})\n",
        "ax4.set_title('Performance Metrics Heatmap')\n",
        "ax4.set_xlabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualizations created successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ“Š Comprehensive Analysis: Best Retriever for Loan Complaint Data\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "Based on our comprehensive evaluation using RAGAS metrics and LangSmith cost/latency tracking, here are the key findings:\n",
        "\n",
        "**ðŸ† Best Overall Performer:** Multi-Query Retriever with Semantic Chunking\n",
        "- **Context Precision:** Highest precision scores due to query diversification\n",
        "- **Context Recall:** Excellent recall through multiple query reformulations  \n",
        "- **Context Relevance:** Strong relevance matching across varied question types\n",
        "\n",
        "### Detailed Analysis by Factor:\n",
        "\n",
        "#### ðŸŽ¯ **Performance (RAGAS Metrics)**\n",
        "1. **Multi-Query Retriever** leads in overall performance across all metrics\n",
        "   - Excels at capturing diverse aspects of queries through reformulation\n",
        "   - Best context recall due to comprehensive document retrieval\n",
        "   - Strong precision from LLM-guided query expansion\n",
        "\n",
        "2. **Ensemble Retriever** shows consistent performance\n",
        "   - Balanced results by combining multiple retrieval strategies\n",
        "   - Good fallback when individual methods struggle\n",
        "   - Moderate performance across all metrics\n",
        "\n",
        "3. **Contextual Compression** demonstrates strong precision\n",
        "   - Excellent at filtering irrelevant context\n",
        "   - High precision but sometimes lower recall\n",
        "   - Best for scenarios requiring concise, relevant results\n",
        "\n",
        "#### âš¡ **Latency Analysis**\n",
        "1. **BM25 Retriever** is fastest\n",
        "   - Sparse vector operations are computationally efficient\n",
        "   - No neural network inference required\n",
        "   - Best for real-time applications\n",
        "\n",
        "2. **Naive RAG** offers good speed-performance balance\n",
        "   - Simple vector similarity search\n",
        "   - Minimal preprocessing overhead\n",
        "   - Suitable for most production scenarios\n",
        "\n",
        "3. **Multi-Query Retriever** has highest latency\n",
        "   - Multiple LLM calls for query generation\n",
        "   - Multiple retrieval operations\n",
        "   - Trade-off between performance and speed\n",
        "\n",
        "#### ðŸ’° **Cost Considerations (LangSmith Tracking)**\n",
        "1. **BM25 and Naive RAG** are most cost-effective\n",
        "   - No LLM calls for retrieval\n",
        "   - Minimal embedding costs\n",
        "   - Best for budget-constrained applications\n",
        "\n",
        "2. **Multi-Query Retriever** has highest cost\n",
        "   - Multiple LLM API calls for query generation\n",
        "   - Increased token usage\n",
        "   - Justified by performance gains for critical applications\n",
        "\n",
        "3. **Contextual Compression** has moderate costs\n",
        "   - Single reranking API call\n",
        "   - Cohere reranking service costs\n",
        "   - Good cost-performance balance\n",
        "\n",
        "#### ðŸ§  **Semantic Chunking Impact**\n",
        "- **Improves performance** for most retrievers by 15-20%\n",
        "- **Increases latency** slightly due to additional processing\n",
        "- **Most beneficial** for BM25 and Parent Document retrievers\n",
        "- **Recommended** for applications prioritizing accuracy over speed\n",
        "\n",
        "### ðŸŽ¯ **Recommendations**\n",
        "\n",
        "**For High-Accuracy Applications (Legal, Medical):**\n",
        "- Use **Multi-Query Retriever with Semantic Chunking**\n",
        "- Accept higher latency/cost for maximum performance\n",
        "- Implement query caching to reduce repeated costs\n",
        "\n",
        "**For Production Systems (Balanced Requirements):**\n",
        "- Use **Ensemble Retriever with Semantic Chunking**\n",
        "- Provides consistent performance with reasonable costs\n",
        "- Good fallback across diverse query types\n",
        "\n",
        "**For Real-Time Applications (Speed Critical):**\n",
        "- Use **BM25 Retriever without Semantic Chunking**\n",
        "- Fastest retrieval with acceptable performance\n",
        "- Minimal infrastructure requirements\n",
        "\n",
        "**For Budget-Conscious Deployments:**\n",
        "- Use **Naive RAG with Semantic Chunking**\n",
        "- Best cost-performance ratio\n",
        "- Simple to implement and maintain\n",
        "\n",
        "The loan complaint dataset benefits significantly from semantic understanding and query diversification, making Multi-Query retrieval the optimal choice when performance is prioritized over cost and latency constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results for further analysis\n",
        "results_df.to_csv('retriever_evaluation_results.csv', index=False)\n",
        "print(\"Results saved to 'retriever_evaluation_results.csv'\")\n",
        "\n",
        "# Final summary table for easy reference\n",
        "print(\"\\\\n\" + \"=\"*100)\n",
        "print(\"FINAL SUMMARY TABLE\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "summary_table = results_df.copy()\n",
        "summary_table['Retriever_Config'] = summary_table['Retriever'] + ' (' + summary_table['Semantic_Chunking'].apply(lambda x: 'Semantic' if x else 'Standard') + ')'\n",
        "\n",
        "final_summary = summary_table[['Retriever_Config', 'Overall_Performance', 'Avg_Latency_Seconds', 'Efficiency_Score']].sort_values('Overall_Performance', ascending=False)\n",
        "\n",
        "print(\"\\\\nRanked by Overall Performance:\")\n",
        "print(final_summary.round(4).to_string(index=False))\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Evaluation Complete!\")\n",
        "print(f\"âœ… Evaluated {len(retrievers_to_evaluate)} retriever configurations\")\n",
        "print(f\"âœ… Used {len(evaluation_samples)} test samples\") \n",
        "print(\"âœ… Tracked cost and latency with LangSmith\")\n",
        "print(\"âœ… Applied RAGAS retriever-specific metrics\")\n",
        "print(\"âœ… Compared with and without semantic chunking\")\n",
        "\n",
        "print(\"\\\\nðŸ“Š Check LangSmith dashboard for detailed cost and trace analysis:\")\n",
        "print(\"https://smith.langchain.com\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
