{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced RAG Evaluation: The Great Chunking Debate\n",
        "\n",
        "## When Size Doesn't Matter (But Semantic Coherence Does)\n",
        "\n",
        "In the rapidly evolving landscape of Retrieval-Augmented Generation (RAG), one fundamental question continues to challenge practitioners: **How should we divide our knowledge into digestible pieces?** This notebook ventures into the heart of this question by conducting a rigorous empirical comparison between two fundamentally different approaches to document chunking.\n",
        "\n",
        "The conventional wisdom suggests that splitting text at arbitrary character boundaries‚Äîwhile computationally efficient‚Äîmay fracture the semantic coherence that makes information truly useful. Yet, does this intuition hold up under scrutiny? Can semantic-aware chunking strategies deliver measurable improvements that justify their additional complexity?\n",
        "\n",
        "## The Experimental Design\n",
        "\n",
        "This investigation implements and evaluates two competing paradigms:\n",
        "\n",
        "### üîß **Baseline System**: The Pragmatic Approach\n",
        "- **Strategy**: RecursiveCharacterTextSplitter with fixed boundaries\n",
        "- **Philosophy**: Simple, fast, and widely adopted\n",
        "- **Characteristics**: 1000-character chunks with 200-character overlap\n",
        "\n",
        "### üß† **Advanced System**: The Semantic Pioneer  \n",
        "- **Strategy**: Jaccard similarity-based sentence grouping\n",
        "- **Philosophy**: Preserve meaning boundaries, optimize for coherence\n",
        "- **Characteristics**: Variable-sized chunks respecting semantic relationships\n",
        "\n",
        "## The Stakes\n",
        "\n",
        "Both systems face the same rigorous evaluation battery using **five comprehensive Ragas metrics**:\n",
        "- **Faithfulness** - Does the system hallucinate or stay grounded?\n",
        "- **Answer Relevancy** - Does it actually answer what was asked?\n",
        "- **Context Precision** - Is the retrieved information truly relevant?\n",
        "- **Context Recall** - Does it find all the necessary pieces?\n",
        "- **Answer Correctness** - Is the final response accurate?\n",
        "\n",
        "This comparison will reveal not just which approach performs better, but *why* certain chunking strategies succeed or fail in different dimensions of RAG performance. The results may challenge our assumptions about the trade-offs between computational efficiency and semantic intelligence in information retrieval systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup Dependencies and API Keys\n",
        "\n",
        "Uses modern Langchain-Qdrant patterns and Jaccard similarity for semantic chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, TypedDict\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "# Qdrant imports - using modern import pattern from latest documentation\n",
        "from qdrant_client import QdrantClient, models\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "\n",
        "# Ragas imports - using correct imports from documentation\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy, \n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_correctness\n",
        ")\n",
        "from ragas import EvaluationDataset, evaluate, RunConfig\n",
        "\n",
        "# For semantic chunking - using only basic libraries\n",
        "import re\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading and Preparation: Setting the Foundation\n",
        "\n",
        "### The Starting Point: Understanding Our Knowledge Base\n",
        "\n",
        "Before we can evaluate different chunking strategies, we need a substantial corpus of real-world documents that will serve as our testing ground. This phase is critical because the characteristics of our source material‚Äîits structure, complexity, and content patterns‚Äîwill significantly influence how different chunking approaches perform.\n",
        "\n",
        "We're working with PDF documents from the `data/` directory, which likely contain structured information about financial aid, loans, and educational policies. These documents represent the kind of dense, formal text that RAG systems commonly encounter in enterprise applications.\n",
        "\n",
        "**Why This Step Matters:**\n",
        "- **Document Diversity**: PDF documents often contain varied formatting, tables, and complex structures that challenge chunking algorithms\n",
        "- **Real-World Relevance**: Using actual policy documents ensures our evaluation reflects genuine use cases\n",
        "- **Baseline Establishment**: Understanding our source material helps us interpret why certain chunking strategies succeed or fail\n",
        "\n",
        "The loading process uses PyMuPDFLoader, which excels at extracting clean text from PDF documents while preserving important structural information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load documents from data directory\n",
        "path = \"data/\"\n",
        "# Note: PyMuPDFLoader handles PDF documents effectively\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "print(f\"Total characters: {sum(len(doc.page_content) for doc in docs)}\")\n",
        "\n",
        "# Show first document metadata for verification\n",
        "if docs:\n",
        "    print(f\"Sample document metadata: {docs[0].metadata}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Generate Synthetic Test Data with Ragas: Creating Our Evaluation Arsenal\n",
        "\n",
        "### The Challenge of Evaluation: Why Synthetic Data Matters\n",
        "\n",
        "Evaluating RAG systems presents a fundamental challenge: **How do we measure success without perfect ground truth?** Traditional evaluation approaches often rely on manually curated question-answer pairs, which are expensive to create and may not cover the full breadth of realistic user queries.\n",
        "\n",
        "Ragas addresses this challenge through sophisticated synthetic data generation that creates diverse, realistic evaluation scenarios automatically.\n",
        "\n",
        "### The Science Behind Synthetic Generation\n",
        "\n",
        "The TestsetGenerator employs a multi-step process that mirrors how humans naturally create questions:\n",
        "\n",
        "1. **Knowledge Graph Construction**: The generator analyzes our documents to understand their semantic relationships and key concepts\n",
        "2. **Persona Development**: It creates diverse user personas with different levels of domain expertise and query styles  \n",
        "3. **Question Synthesis**: Using these personas and knowledge graphs, it generates questions that span different complexity levels and query types\n",
        "4. **Reference Creation**: Each question comes with carefully crafted reference answers and expected contexts\n",
        "\n",
        "**Why This Approach is Revolutionary:**\n",
        "- **Scalability**: Generate hundreds of evaluation cases in minutes vs. days of manual work\n",
        "- **Coverage**: Automatically explores edge cases and diverse query patterns that humans might miss\n",
        "- **Consistency**: Eliminates human bias and ensures reproducible evaluation standards\n",
        "- **Realism**: Creates questions that reflect genuine user information needs\n",
        "\n",
        "This synthetic evaluation dataset becomes our \"truth standard\" against which both chunking strategies will be measured across all five Ragas metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Ragas components for test generation\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "# Generate synthetic test dataset\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)\n",
        "\n",
        "print(f\"Generated {len(dataset.samples)} test samples\")\n",
        "dataset.to_pandas().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Baseline RAG System: The Pragmatic Foundation\n",
        "\n",
        "### 4.1 Create Naive Chunks - The Industry Standard Approach\n",
        "\n",
        "**The Philosophy of Simplicity**\n",
        "\n",
        "RecursiveCharacterTextSplitter represents the pragmatic approach that has dominated RAG implementations. This strategy embodies a \"good enough\" philosophy: split text into manageable, uniform pieces without overthinking the content structure.\n",
        "\n",
        "**How RecursiveCharacterTextSplitter Works:**\n",
        "\n",
        "1. **Hierarchical Splitting**: First attempts to split on paragraphs, then sentences, then words, finally characters\n",
        "2. **Fixed Boundaries**: Enforces strict size limits (1000 characters) regardless of content\n",
        "3. **Overlap Strategy**: Includes 200-character overlap to preserve some context across boundaries\n",
        "4. **Computational Efficiency**: Requires no semantic analysis‚Äîjust character counting\n",
        "\n",
        "**The Trade-offs We Accept:**\n",
        "\n",
        "‚úÖ **Advantages:**\n",
        "- **Predictable Performance**: Consistent chunk sizes enable predictable retrieval behavior\n",
        "- **Speed**: No computational overhead for similarity calculations\n",
        "- **Reliability**: Works identically across different content types and domains\n",
        "- **Memory Efficiency**: Uniform chunks facilitate efficient vector storage\n",
        "\n",
        "‚ö†Ô∏è **Limitations:**\n",
        "- **Semantic Blindness**: May split coherent thoughts arbitrarily\n",
        "- **Context Loss**: Important relationships between sentences can be severed\n",
        "- **Retrieval Noise**: Fragments without complete context can confuse the generation process\n",
        "\n",
        "This baseline will reveal whether our sophisticated semantic approach can overcome these fundamental limitations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create naive chunks using RecursiveCharacterTextSplitter\n",
        "naive_text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, \n",
        "    chunk_overlap=200\n",
        ")\n",
        "naive_chunks = naive_text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Created {len(naive_chunks)} naive chunks\")\n",
        "print(f\"Average chunk size: {np.mean([len(chunk.page_content) for chunk in naive_chunks]):.0f} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. The Art and Science of Semantic Chunking\n",
        "\n",
        "### Beyond Arbitrary Boundaries: A More Thoughtful Approach\n",
        "\n",
        "Traditional text splitting treats documents like logs to be sawed‚Äîcutting wherever the size limit dictates, regardless of where ideas begin and end. Consider a scenario where a crucial explanation spans across two chunks: \"The Federal Pell Grant provides need-based aid to students. [CHUNK BOUNDARY] This aid does not need to be repaid and can cover up to $7,000 per year.\" The connection between the grant and its non-repayable nature is severed, potentially degrading retrieval quality.\n",
        "\n",
        "### The Semantic Solution: Jaccard Similarity\n",
        "\n",
        "Our semantic chunking implementation addresses this challenge through a sophisticated yet computationally efficient approach using **Jaccard similarity**‚Äîa measure of word set overlap that captures topical coherence without requiring expensive neural embeddings.\n",
        "\n",
        "#### The Algorithm's Intelligence\n",
        "\n",
        "The strategy operates on four key principles:\n",
        "\n",
        "1. **Sentence-Level Awareness**: Text is split at natural sentence boundaries using regex patterns `[.!?]+`, respecting the fundamental units of human communication\n",
        "\n",
        "2. **Similarity-Driven Grouping**: Consecutive sentences are evaluated for word overlap:\n",
        "   ```\n",
        "   Jaccard(A,B) = |words_A ‚à© words_B| / |words_A ‚à™ words_B|\n",
        "   ```\n",
        "\n",
        "3. **Threshold-Based Decisions**: When similarity ‚â• 0.7, sentences are grouped together, preserving topical coherence while maintaining manageable chunk sizes\n",
        "\n",
        "4. **Size Constraints**: Respects practical limits (50-1000 characters) to balance semantic preservation with retrieval efficiency\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "This approach embodies a fundamental principle of information science: **meaning should guide structure, not arbitrary size limits**. By keeping semantically related sentences together, we preserve the contextual relationships that make information truly useful for question-answering systems.\n",
        "\n",
        "The beauty lies in its simplicity‚Äîno external dependencies, no complex neural models, yet sophisticated enough to capture the semantic relationships that matter most for retrieval quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SemanticChunker:\n",
        "    \"\"\"Semantic chunking strategy that groups semantically similar sentences based on text similarity.\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 similarity_threshold: float = 0.7,\n",
        "                 max_chunk_size: int = 1000,\n",
        "                 min_chunk_size: int = 50):\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "    \n",
        "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents using semantic chunking strategy.\"\"\"\n",
        "        all_chunks = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            chunks = self._chunk_document(doc)\n",
        "            all_chunks.extend(chunks)\n",
        "        \n",
        "        return all_chunks\n",
        "    \n",
        "    def _chunk_document(self, document: Document) -> List[Document]:\n",
        "        \"\"\"Chunk a single document semantically.\"\"\"\n",
        "        text = document.page_content\n",
        "        \n",
        "        # Split into sentences using simple regex\n",
        "        sentences = self._split_into_sentences(text)\n",
        "        if not sentences:\n",
        "            return [document]\n",
        "        \n",
        "        # Group sentences semantically using text similarity\n",
        "        chunks = self._group_sentences(sentences)\n",
        "        \n",
        "        # Convert to Document objects\n",
        "        chunk_docs = []\n",
        "        for chunk_text in chunks:\n",
        "            if len(chunk_text.strip()) >= self.min_chunk_size:\n",
        "                chunk_doc = Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata=document.metadata.copy()\n",
        "                )\n",
        "                chunk_docs.append(chunk_doc)\n",
        "        \n",
        "        return chunk_docs if chunk_docs else [document]\n",
        "    \n",
        "    def _split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple sentence splitting using regex.\"\"\"\n",
        "        # Basic sentence splitting on periods, exclamation marks, question marks\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        # Clean up and filter empty sentences\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        return sentences\n",
        "    \n",
        "    def _calculate_text_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Calculate simple text similarity using word overlap (Jaccard similarity).\"\"\"\n",
        "        # Convert to lowercase and split into words\n",
        "        words1 = set(text1.lower().translate(str.maketrans('', '', string.punctuation)).split())\n",
        "        words2 = set(text2.lower().translate(str.maketrans('', '', string.punctuation)).split())\n",
        "        \n",
        "        # Calculate Jaccard similarity\n",
        "        intersection = len(words1.intersection(words2))\n",
        "        union = len(words1.union(words2))\n",
        "        \n",
        "        if union == 0:\n",
        "            return 0.0\n",
        "        return intersection / union\n",
        "    \n",
        "    def _group_sentences(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"Group sentences based on text similarity.\"\"\"\n",
        "        if len(sentences) == 1:\n",
        "            return sentences\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = [sentences[0]]\n",
        "        current_length = len(sentences[0])\n",
        "        \n",
        "        for i in range(1, len(sentences)):\n",
        "            sentence = sentences[i]\n",
        "            sentence_length = len(sentence)\n",
        "            \n",
        "            # Check if adding this sentence would exceed max chunk size\n",
        "            if current_length + sentence_length > self.max_chunk_size:\n",
        "                # Finalize current chunk\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "                continue\n",
        "            \n",
        "            # Calculate text similarity with previous sentence\n",
        "            prev_sentence = sentences[i-1]\n",
        "            similarity = self._calculate_text_similarity(prev_sentence, sentence)\n",
        "            \n",
        "            # Group if similar enough\n",
        "            if similarity >= self.similarity_threshold:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_length + 1  # +1 for space\n",
        "            else:\n",
        "                # Start new chunk\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "        \n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "print(\"Semantic chunker implemented using text-based similarity (Jaccard)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 5.1 Create Semantic Chunks\n",
        "\n",
        "Our SemanticChunker implementation uses:\n",
        "- **0.7 similarity threshold** for Jaccard word overlap\n",
        "- **50-1000 character chunks** respecting sentence boundaries  \n",
        "- **No external dependencies** - pure Python with regex sentence splitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create semantic chunks\n",
        "semantic_chunker = SemanticChunker(\n",
        "    similarity_threshold=0.7,\n",
        "    max_chunk_size=1000,\n",
        "    min_chunk_size=50\n",
        ")\n",
        "\n",
        "semantic_chunks = semantic_chunker.split_documents(docs)\n",
        "\n",
        "print(f\"Created {len(semantic_chunks)} semantic chunks\")\n",
        "print(f\"Average chunk size: {np.mean([len(chunk.page_content) for chunk in semantic_chunks]):.0f} characters\")\n",
        "\n",
        "# Compare chunk size distributions\n",
        "naive_sizes = [len(chunk.page_content) for chunk in naive_chunks]\n",
        "semantic_sizes = [len(chunk.page_content) for chunk in semantic_chunks]\n",
        "\n",
        "print(f\"\\nChunk Size Comparison:\")\n",
        "print(f\"Naive - Min: {min(naive_sizes)}, Max: {max(naive_sizes)}, Std: {np.std(naive_sizes):.0f}\")\n",
        "print(f\"Semantic - Min: {min(semantic_sizes)}, Max: {max(semantic_sizes)}, Std: {np.std(semantic_sizes):.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Build RAG Systems: From Chunks to Intelligence\n",
        "\n",
        "### 6.1 Create Vector Stores and Retrievers\n",
        "\n",
        "Convert chunks to 1536-dimensional vectors using OpenAI embeddings, stored in separate Qdrant collections with cosine similarity and k=5 retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup embeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Modern Qdrant vector store creation following latest documentation patterns\n",
        "\n",
        "print(\"Creating vector stores using modern Qdrant patterns...\")\n",
        "\n",
        "# === NAIVE CHUNKS VECTOR STORE ===\n",
        "print(\"Initializing naive chunks vector store...\")\n",
        "\n",
        "# Create Qdrant client for naive chunks\n",
        "naive_client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Clean up any existing collection (development best practice)\n",
        "collection_name_naive = \"naive_chunks\"\n",
        "try:\n",
        "    if naive_client.collection_exists(collection_name_naive):\n",
        "        naive_client.delete_collection(collection_name_naive)\n",
        "        print(f\"Cleaned up existing collection: {collection_name_naive}\")\n",
        "except Exception as e:\n",
        "    print(f\"Collection cleanup note: {e}\")\n",
        "\n",
        "# Create collection with modern configuration pattern\n",
        "naive_client.create_collection(\n",
        "    collection_name=collection_name_naive,\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=1536,  # Matching text-embedding-3-small dimensions\n",
        "        distance=models.Distance.COSINE  # Optimal for semantic similarity\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize vector store using modern QdrantVectorStore pattern\n",
        "naive_vector_store = QdrantVectorStore(\n",
        "    client=naive_client,\n",
        "    collection_name=collection_name_naive,\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Add documents with improved error handling\n",
        "print(f\"Adding {len(naive_chunks)} naive chunks to vector store...\")\n",
        "naive_ids = naive_vector_store.add_documents(documents=naive_chunks)\n",
        "naive_retriever = naive_vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"‚úÖ Naive vector store created successfully\")\n",
        "\n",
        "# === SEMANTIC CHUNKS VECTOR STORE ===\n",
        "print(\"Initializing semantic chunks vector store...\")\n",
        "\n",
        "# Create Qdrant client for semantic chunks  \n",
        "semantic_client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Clean up any existing collection\n",
        "collection_name_semantic = \"semantic_chunks\"\n",
        "try:\n",
        "    if semantic_client.collection_exists(collection_name_semantic):\n",
        "        semantic_client.delete_collection(collection_name_semantic)\n",
        "        print(f\"Cleaned up existing collection: {collection_name_semantic}\")\n",
        "except Exception as e:\n",
        "    print(f\"Collection cleanup note: {e}\")\n",
        "\n",
        "# Create collection with identical configuration for fair comparison\n",
        "semantic_client.create_collection(\n",
        "    collection_name=collection_name_semantic,\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=1536,  # Matching text-embedding-3-small dimensions\n",
        "        distance=models.Distance.COSINE  # Identical to naive setup\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize semantic vector store with identical configuration\n",
        "semantic_vector_store = QdrantVectorStore(\n",
        "    client=semantic_client,\n",
        "    collection_name=collection_name_semantic,\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Add documents with improved logging\n",
        "print(f\"Adding {len(semantic_chunks)} semantic chunks to vector store...\")\n",
        "semantic_ids = semantic_vector_store.add_documents(documents=semantic_chunks)\n",
        "semantic_retriever = semantic_vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"‚úÖ Semantic vector store created successfully\")\n",
        "\n",
        "print(\"\\nüöÄ Vector store infrastructure ready!\")\n",
        "print(f\"üìä Collections created:\")\n",
        "print(f\"   ‚Ä¢ {collection_name_naive}: {len(naive_chunks)} chunks\")\n",
        "print(f\"   ‚Ä¢ {collection_name_semantic}: {len(semantic_chunks)} chunks\")\n",
        "print(f\"üìê Vector configuration: 1536-dimensional with cosine similarity\")\n",
        "print(f\"üîÑ Retrieval setting: k=5 chunks per query\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 6.2 Build LangGraph RAG Applications - The Orchestration Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 6.2 Build LangGraph RAG Applications\n",
        "\n",
        "LangGraph orchestrates retrieval and generation with state management. Both systems use identical prompts and LLM configuration - only the chunk quality differs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define state for LangGraph\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    response: str\n",
        "\n",
        "# Create RAG prompt\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful assistant who answers questions based on provided context. \n",
        "You must only use the provided context, and cannot use your own knowledge.\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Context\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define nodes for RAG systems\n",
        "def naive_retrieve(state):\n",
        "    retrieved_docs = naive_retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def semantic_retrieve(state):\n",
        "    retrieved_docs = semantic_retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"response\": response.content}\n",
        "\n",
        "# Build naive RAG graph\n",
        "naive_graph_builder = StateGraph(RAGState).add_sequence([naive_retrieve, generate])\n",
        "naive_graph_builder.add_edge(START, \"naive_retrieve\")\n",
        "naive_graph = naive_graph_builder.compile()\n",
        "\n",
        "# Build semantic RAG graph\n",
        "semantic_graph_builder = StateGraph(RAGState).add_sequence([semantic_retrieve, generate])\n",
        "semantic_graph_builder.add_edge(START, \"semantic_retrieve\")\n",
        "semantic_graph = semantic_graph_builder.compile()\n",
        "\n",
        "print(\"LangGraph RAG applications created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Evaluation Setup and Execution: The Scientific Method in Action\n",
        "\n",
        "### Rigorous Measurement in the Age of AI\n",
        "\n",
        "Evaluation represents the most critical phase of our investigation‚Äîwhere subjective intuitions about chunking quality meet objective, quantifiable metrics. The Ragas framework provides a sophisticated evaluation apparatus that goes far beyond simple accuracy measurements.\n",
        "\n",
        "**The Multi-Dimensional Assessment Strategy:**\n",
        "\n",
        "Traditional evaluation approaches often rely on single metrics that miss the nuanced ways AI systems can fail or succeed. Our five-metric evaluation strategy captures different failure modes:\n",
        "\n",
        "- **Faithfulness**: Guards against hallucination and ensures factual grounding\n",
        "- **Answer Relevancy**: Measures whether the system addresses user intent\n",
        "- **Context Precision**: Evaluates the signal-to-noise ratio in retrieval\n",
        "- **Context Recall**: Assesses completeness of information gathering\n",
        "- **Answer Correctness**: Provides holistic accuracy measurement\n",
        "\n",
        "**The Experimental Design Principles:**\n",
        "\n",
        "1. **Controlled Variables**: Identical evaluation LLM (gpt-4o-mini) for consistent judging\n",
        "2. **Isolated Testing**: Each system evaluated against identical question sets\n",
        "3. **Reproducible Methods**: Fixed random seeds and evaluation parameters\n",
        "4. **Statistical Validity**: Multiple test samples provide robust performance estimates\n",
        "\n",
        "**Why This Evaluation Approach is Revolutionary:**\n",
        "\n",
        "Unlike traditional metrics that require extensive human annotation, Ragas leverages LLM-as-a-judge techniques that scale infinitely while maintaining consistency. This approach enables comprehensive evaluation across dimensions that would be prohibitively expensive to assess manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup evaluation LLM and metrics according to Ragas documentation\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "\n",
        "# Use pre-instantiated metrics from Ragas (as shown in documentation)\n",
        "metrics = [faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness]\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "print(\"Evaluation metrics initialized using pre-instantiated Ragas metrics\")\n",
        "print(f\"Metrics: {[m.__class__.__name__ for m in metrics]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rag_system(graph, system_name: str, test_dataset):\n",
        "    \"\"\"Evaluate a RAG system using Ragas metrics.\"\"\"\n",
        "    print(f\"\\nEvaluating {system_name} system...\")\n",
        "    \n",
        "    # Run the RAG system on test questions\n",
        "    for test_row in test_dataset:\n",
        "        question = test_row.eval_sample.user_input\n",
        "        response = graph.invoke({\"question\": question})\n",
        "        \n",
        "        # Update test row with response and context\n",
        "        test_row.eval_sample.response = response[\"response\"]\n",
        "        test_row.eval_sample.retrieved_contexts = [\n",
        "            context.page_content for context in response[\"context\"]\n",
        "        ]\n",
        "    \n",
        "    # Convert to evaluation dataset\n",
        "    evaluation_dataset = EvaluationDataset.from_pandas(test_dataset.to_pandas())\n",
        "    \n",
        "    # Evaluate with Ragas\n",
        "    result = evaluate(\n",
        "        dataset=evaluation_dataset,\n",
        "        metrics=metrics,\n",
        "        llm=evaluator_llm,\n",
        "        run_config=custom_run_config\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"Evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.1 Evaluate Baseline (Naive) RAG System - Establishing the Benchmark\n",
        "\n",
        "**The Foundation of Comparison**\n",
        "\n",
        "Before we can claim victory for semantic approaches, we must thoroughly understand the performance characteristics of the naive baseline. This evaluation establishes the \"to-beat\" scores that will determine whether our sophisticated approach delivers meaningful improvements.\n",
        "\n",
        "**What We're Measuring:**\n",
        "\n",
        "Each test question flows through the naive RAG system, generating:\n",
        "1. **Retrieved Context**: The 5 most similar chunks based on vector similarity\n",
        "2. **Generated Response**: The LLM's answer grounded in retrieved context\n",
        "3. **Performance Metrics**: Five comprehensive Ragas scores measuring different quality dimensions\n",
        "\n",
        "**The Evaluation Process:**\n",
        "\n",
        "For each synthetic question, we:\n",
        "- Execute the naive RAG pipeline end-to-end\n",
        "- Capture both intermediate results (context) and final outputs (responses)\n",
        "- Feed these into the Ragas evaluation framework\n",
        "- Generate comprehensive metric scores across all evaluation dimensions\n",
        "\n",
        "**Why This Step is Critical:**\n",
        "\n",
        "The baseline results will reveal the strengths and weaknesses of industry-standard approaches. Strong baseline performance would suggest that semantic chunking faces a high bar for improvement, while weak baseline results might indicate significant opportunities for enhancement.\n",
        "\n",
        "**Anticipated Baseline Characteristics:**\n",
        "\n",
        "Based on our understanding of naive chunking limitations, we expect:\n",
        "- **Moderate Faithfulness**: Some hallucination due to fragmented context\n",
        "- **Variable Relevancy**: Inconsistent focus due to incomplete thought preservation\n",
        "- **Mixed Precision**: Some irrelevant fragments alongside useful information\n",
        "- **Incomplete Recall**: Missing context pieces scattered across chunk boundaries\n",
        "\n",
        "These baseline metrics will provide the quantitative foundation for assessing whether semantic intelligence translates into measurable system improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Create a copy of the dataset for naive evaluation\n",
        "naive_dataset = copy.deepcopy(dataset)\n",
        "naive_results = evaluate_rag_system(naive_graph, \"Naive Chunking\", naive_dataset)\n",
        "\n",
        "print(\"\\n=== NAIVE RAG RESULTS ===\")\n",
        "print(naive_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.2 Evaluate Semantic RAG System - The Moment of Truth\n",
        "\n",
        "**Testing the Semantic Hypothesis**\n",
        "\n",
        "With baseline performance established, we now subject our semantic chunking approach to the same rigorous evaluation. This phase will definitively answer whether preserving semantic coherence translates into measurable improvements across our evaluation dimensions.\n",
        "\n",
        "**The Stakes of This Evaluation:**\n",
        "\n",
        "This is where our theoretical framework faces empirical reality. Will the additional complexity of semantic analysis justify its computational cost? Can Jaccard similarity effectively capture the semantic relationships that matter for RAG performance?\n",
        "\n",
        "**What We're Comparing:**\n",
        "\n",
        "The semantic system processes identical questions through:\n",
        "1. **Enhanced Retrieval**: Chunks that preserve complete thoughts and topical coherence\n",
        "2. **Identical Generation**: Same LLM and prompting strategy to isolate chunking effects\n",
        "3. **Rigorous Assessment**: Identical Ragas evaluation to ensure fair comparison\n",
        "\n",
        "**Expected Semantic Advantages:**\n",
        "\n",
        "If our hypothesis is correct, semantic chunking should demonstrate:\n",
        "- **Improved Faithfulness**: More complete context reduces hallucination risk\n",
        "- **Enhanced Relevancy**: Topically coherent chunks improve answer focus\n",
        "- **Better Precision**: Semantic grouping reduces retrieval noise\n",
        "- **Maintained Recall**: Intelligent boundaries preserve information completeness\n",
        "- **Higher Correctness**: Overall improvement in answer quality\n",
        "\n",
        "**The Critical Questions:**\n",
        "\n",
        "- Will semantic coherence overcome the challenge of variable chunk sizes?\n",
        "- Can our simple Jaccard similarity approach compete with sophisticated neural embeddings?\n",
        "- Do the benefits of semantic awareness justify the additional implementation complexity?\n",
        "\n",
        "**Potential Surprises:**\n",
        "\n",
        "The evaluation might reveal unexpected results:\n",
        "- Semantic chunking could excel in some dimensions while underperforming in others\n",
        "- The 0.7 similarity threshold might prove suboptimal for our specific content\n",
        "- Variable chunk sizes might introduce new failure modes we hadn't anticipated\n",
        "\n",
        "This evaluation will provide definitive evidence about the true value of semantic awareness in RAG systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy of the dataset for semantic evaluation\n",
        "semantic_dataset = copy.deepcopy(dataset)\n",
        "semantic_results = evaluate_rag_system(semantic_graph, \"Semantic Chunking\", semantic_dataset)\n",
        "\n",
        "print(\"\\n=== SEMANTIC RAG RESULTS ===\")\n",
        "print(semantic_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. The Moment of Truth: Deciphering the Evidence\n",
        "\n",
        "### What the Numbers Tell Us About Chunking Intelligence\n",
        "\n",
        "After subjecting both systems to the rigorous Ragas evaluation battery, we now face the critical question: **Did semantic awareness translate into measurable performance gains?** The results that follow represent more than just numbers‚Äîthey reveal fundamental insights about how information structure affects the quality of AI-driven question answering.\n",
        "\n",
        "Each metric tells a specific story about system behavior:\n",
        "- **Faithfulness** reveals whether the system stays anchored to reality or drifts into hallucination\n",
        "- **Answer Relevancy** indicates if the system truly understands what users are asking\n",
        "- **Context Precision** measures the signal-to-noise ratio in retrieved information\n",
        "- **Context Recall** evaluates completeness‚Äîdid we find all the pieces of the puzzle?\n",
        "- **Answer Correctness** provides the ultimate judgment: accuracy in the final response\n",
        "\n",
        "The comparative analysis below will illuminate whether our hypothesis‚Äîthat semantic coherence improves RAG performance‚Äîholds water when subjected to empirical scrutiny.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract results for comparison\n",
        "naive_scores = {\n",
        "    'faithfulness': naive_results['faithfulness'],\n",
        "    'answer_relevancy': naive_results['answer_relevancy'], \n",
        "    'context_precision': naive_results['context_precision'],\n",
        "    'context_recall': naive_results['context_recall'],\n",
        "    'answer_correctness': naive_results['answer_correctness']\n",
        "}\n",
        "\n",
        "semantic_scores = {\n",
        "    'faithfulness': semantic_results['faithfulness'],\n",
        "    'answer_relevancy': semantic_results['answer_relevancy'],\n",
        "    'context_precision': semantic_results['context_precision'], \n",
        "    'context_recall': semantic_results['context_recall'],\n",
        "    'answer_correctness': semantic_results['answer_correctness']\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Naive Chunking': naive_scores,\n",
        "    'Semantic Chunking': semantic_scores\n",
        "})\n",
        "\n",
        "# Calculate improvements\n",
        "comparison_df['Improvement'] = comparison_df['Semantic Chunking'] - comparison_df['Naive Chunking']\n",
        "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Naive Chunking'] * 100).round(2)\n",
        "\n",
        "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
        "print(comparison_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== DETAILED ANALYSIS ===\")\n",
        "print(f\"\\nüìä Chunk Statistics:\")\n",
        "print(f\"‚Ä¢ Naive Chunks: {len(naive_chunks)} (avg: {np.mean(naive_sizes):.0f} chars)\")\n",
        "print(f\"‚Ä¢ Semantic Chunks: {len(semantic_chunks)} (avg: {np.mean(semantic_sizes):.0f} chars)\")\n",
        "\n",
        "print(f\"\\nüéØ Metric Analysis:\")\n",
        "for metric in comparison_df.index:\n",
        "    naive_score = comparison_df.loc[metric, 'Naive Chunking']\n",
        "    semantic_score = comparison_df.loc[metric, 'Semantic Chunking']\n",
        "    improvement = comparison_df.loc[metric, 'Improvement %']\n",
        "    \n",
        "    if improvement > 0:\n",
        "        status = \"‚úÖ IMPROVED\"\n",
        "    elif improvement < 0:\n",
        "        status = \"‚ùå DECLINED\"\n",
        "    else:\n",
        "        status = \"‚ûñ UNCHANGED\"\n",
        "    \n",
        "    print(f\"‚Ä¢ {metric.replace('_', ' ').title()}: {naive_score:.3f} ‚Üí {semantic_score:.3f} ({improvement:+.1f}%) {status}\")\n",
        "\n",
        "# Overall assessment\n",
        "total_improvements = sum(1 for imp in comparison_df['Improvement'] if imp > 0)\n",
        "avg_improvement = comparison_df['Improvement %'].mean()\n",
        "\n",
        "print(f\"\\nüèÜ Overall Assessment:\")\n",
        "print(f\"‚Ä¢ Metrics Improved: {total_improvements}/5\")\n",
        "print(f\"‚Ä¢ Average Improvement: {avg_improvement:+.1f}%\")\n",
        "\n",
        "if avg_improvement > 5:\n",
        "    conclusion = \"üéâ Semantic chunking shows significant improvements!\"\n",
        "elif avg_improvement > 0:\n",
        "    conclusion = \"üëç Semantic chunking shows modest improvements.\"\n",
        "elif avg_improvement > -5:\n",
        "    conclusion = \"ü§î Results are mixed between approaches.\"\n",
        "else:\n",
        "    conclusion = \"‚ö†Ô∏è Naive chunking performed better overall.\"\n",
        "\n",
        "print(f\"‚Ä¢ Conclusion: {conclusion}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.1 Decoding the Performance Signatures: What Each Metric Reveals\n",
        "\n",
        "#### The Psychology of AI Systems Under Different Chunking Regimes\n",
        "\n",
        "Understanding these results requires appreciating that each metric captures a different aspect of how chunking strategy influences AI behavior. Like examining different vital signs of a patient, each measurement reveals something unique about system health and capability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"=== THE DEEPER STORY: WHAT THESE METRICS REVEAL ===\")\n",
        "print(\"\"\"\n",
        "üîç **Faithfulness: The Hallucination Detector**\n",
        "   This metric exposes whether our chunking strategy helps or hinders the AI's ability to stay \n",
        "   grounded in factual reality. Semantic chunks, by preserving complete thoughts, may provide \n",
        "   stronger anchors against the AI's tendency to fabricate plausible-sounding but false information.\n",
        "   \n",
        "   Consider this scenario: A fragmented chunk containing \"...provides aid to students. The grant \n",
        "   program offers...\" might lead to hallucinated details about eligibility. A complete semantic \n",
        "   chunk preserving the full context would provide stronger factual grounding.\n",
        "   \n",
        "   *The Question*: Does semantic coherence create stronger \"guardrails\" against hallucination?\n",
        "\n",
        "üéØ **Answer Relevancy: The Focus Meter** \n",
        "   Here we measure whether the system truly grasps user intent. Semantic chunking's preservation \n",
        "   of topical coherence should theoretically improve the system's ability to maintain focus on \n",
        "   the actual question, rather than getting distracted by tangentially related information.\n",
        "   \n",
        "   When chunks contain complete thoughts about specific topics, the retrieval process is more \n",
        "   likely to surface directly relevant information rather than tangentially related fragments.\n",
        "   \n",
        "   *The Question*: Does semantic grouping help the AI \"stay on topic\"?\n",
        "\n",
        "üìç **Context Precision: The Signal-to-Noise Ratio**\n",
        "   This reveals the quality of information retrieval. Semantic chunks, by clustering related \n",
        "   concepts, should reduce the retrieval of irrelevant fragments that confuse the generation \n",
        "   process. However, variable chunk sizes might introduce new retrieval challenges.\n",
        "   \n",
        "   The precision metric will reveal whether our semantic grouping strategy successfully filters \n",
        "   out the \"noise\" of irrelevant fragments that plague naive chunking approaches.\n",
        "   \n",
        "   *The Question*: Does semantic clustering improve the \"wheat-to-chaff\" ratio?\n",
        "\n",
        "üìä **Context Recall: The Completeness Test**\n",
        "   The critical trade-off emerges here. While semantic chunks preserve coherence, they might \n",
        "   miss relevant information scattered across different topical sections. This metric reveals \n",
        "   whether our quest for coherence comes at the cost of comprehensiveness.\n",
        "   \n",
        "   This is where our approach faces its greatest challenge: ensuring that semantic boundaries \n",
        "   don't inadvertently exclude important information that naive overlap strategies would capture.\n",
        "   \n",
        "   *The Question*: Do we sacrifice completeness for coherence?\n",
        "\n",
        "‚úÖ **Answer Correctness: The Ultimate Verdict**\n",
        "   This metric synthesizes factual accuracy with semantic appropriateness‚Äîthe final judgment \n",
        "   on whether our chunking strategy actually helps users get better answers to their questions.\n",
        "   \n",
        "   All the theoretical elegance means nothing if users don't get better, more accurate answers. \n",
        "   This metric cuts through the complexity to the fundamental question: does it work?\n",
        "   \n",
        "   *The Question*: Does all this sophistication actually matter for end users?\n",
        "\n",
        "üß† **The Semantic Chunking Hypothesis**:\n",
        "   By respecting the natural boundaries of human thought and language, semantic chunking should \n",
        "   provide AI systems with more contextually rich and coherent information, leading to more \n",
        "   accurate and relevant responses. But theory must meet empirical reality.\n",
        "\n",
        "‚öñÔ∏è **The Inevitable Trade-offs**:\n",
        "   ‚Ä¢ **Computational Cost**: Similarity calculations vs. simple character counting\n",
        "   ‚Ä¢ **Consistency**: Variable chunk sizes vs. predictable uniform chunks  \n",
        "   ‚Ä¢ **Tuning Complexity**: Threshold optimization vs. \"set and forget\" simplicity\n",
        "   ‚Ä¢ **Coverage Risk**: Semantic boundaries vs. guaranteed overlap patterns\n",
        "   ‚Ä¢ **Scalability**: Text-based similarity vs. neural embedding approaches\n",
        "\n",
        "The results above will reveal which forces dominate in this fascinating tension between \n",
        "computational efficiency and semantic intelligence, and whether the pursuit of semantic \n",
        "coherence yields measurable improvements in real-world RAG performance.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. The Verdict: Lessons from the Chunking Laboratory\n",
        "\n",
        "Our head-to-head comparison reveals how chunking strategy impacts RAG performance across five critical dimensions. The results show whether semantic coherence can overcome the simplicity of naive approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== THE FINAL CHAPTER: WHAT WE'VE DISCOVERED ===\")\n",
        "print(f\"\"\"\n",
        "üî¨ **The Empirical Reality**\n",
        "\n",
        "After subjecting both approaches to rigorous evaluation, we now have concrete evidence about \n",
        "the impact of chunking strategy on RAG system performance. The numbers tell a story that goes \n",
        "beyond simple performance metrics‚Äîthey reveal fundamental insights about how AI systems \n",
        "interact with differently structured information.\n",
        "\n",
        "**The Tale of Two Systems:**\n",
        "‚Ä¢ üìä Naive RAG: {len(naive_chunks)} uniform chunks averaging {np.mean(naive_sizes):.0f} characters\n",
        "‚Ä¢ üß† Semantic RAG: {len(semantic_chunks)} variable chunks averaging {np.mean(semantic_sizes):.0f} characters\n",
        "‚Ä¢ üìà Overall Performance Delta: {avg_improvement:+.1f}% change\n",
        "‚Ä¢ üéØ Metrics That Improved: {total_improvements} out of 5 dimensions\n",
        "\n",
        "**The Semantic Chunking Innovation:**\n",
        "Simple yet effective approach using Jaccard similarity for word overlap:\n",
        "   \n",
        "   Jaccard similarity = |shared_words| / |total_unique_words|\n",
        "   \n",
        "- Sentence-level splitting with 0.7 similarity threshold\n",
        "- No external dependencies, modern Qdrant integration patterns\n",
        "- Balances semantic coherence with practical constraints\n",
        "\n",
        "**Strategic Decision Framework:**\n",
        "   \n",
        "   if similarity ‚â• 0.7 AND size_constraint_satisfied:\n",
        "       preserve_semantic_coherence()\n",
        "   else:\n",
        "       respect_practical_boundaries()\n",
        "\n",
        "{conclusion}\n",
        "\n",
        "**Key Insight:**\n",
        "How we structure information shapes how AI systems understand and use it. The future of RAG \n",
        "depends not just on better models, but on smarter information organization.\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
