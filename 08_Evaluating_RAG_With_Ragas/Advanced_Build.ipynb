{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced RAG Build: Semantic Chunking vs Naive Chunking Evaluation\n",
        "\n",
        "This notebook implements and compares two RAG approaches:\n",
        "1. **Baseline**: LangGraph RAG with Naive Retrieval (RecursiveCharacterTextSplitter)\n",
        "2. **Advanced**: LangGraph RAG with Semantic Chunking + Naive Retrieval\n",
        "\n",
        "## Evaluation Metrics (Ragas):\n",
        "- Faithfulness\n",
        "- Answer Relevancy \n",
        "- Context Precision\n",
        "- Context Recall\n",
        "- Answer Correctness\n",
        "\n",
        "## Implementation Strategy:\n",
        "- **Semantic Chunking**: Group semantically similar sentences using cosine similarity threshold\n",
        "- **Greedy Approach**: Prioritize similar sentences up to maximum chunk size\n",
        "- **Minimum**: Single sentence per chunk\n",
        "- **Retrieval**: Naive retrieval for both approaches (no reranking)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API Keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 269 documents\n",
            "Total characters: 838,132\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# Load the same data as original notebook\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "print(f\"Total characters: {sum(len(doc.page_content) for doc in docs):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Synthetic Test Dataset Generation (Reusing Original Implementation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up models for dataset generation (same as original)\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff28bbb9c6db4d44ad737f0bd18f1f76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfdeb8c706d1433c85e2ae79eef29153",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78a2ab85893a42a3a1bdc2c17ada140e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node '133739'. Skipping!\n",
            "Property 'summary' already exists in node 'f5bb1e'. Skipping!\n",
            "Property 'summary' already exists in node '1f6dcb'. Skipping!\n",
            "Property 'summary' already exists in node 'e477f9'. Skipping!\n",
            "Property 'summary' already exists in node '013c63'. Skipping!\n",
            "Property 'summary' already exists in node '50457f'. Skipping!\n",
            "Property 'summary' already exists in node '5b92f4'. Skipping!\n",
            "Property 'summary' already exists in node 'df764a'. Skipping!\n",
            "Property 'summary' already exists in node '1d2cbd'. Skipping!\n",
            "Property 'summary' already exists in node '7c5bcf'. Skipping!\n",
            "Property 'summary' already exists in node 'b9e953'. Skipping!\n",
            "Property 'summary' already exists in node 'd391db'. Skipping!\n",
            "Property 'summary' already exists in node '4d53db'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1307cc94588148acb46678203fe28dbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0096d59b6e264f89804d1c8ccfa11445",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/42 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node '4d53db'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '50457f'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '5b92f4'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'd391db'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '1f6dcb'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'df764a'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '013c63'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'f5bb1e'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '133739'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '7c5bcf'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'b9e953'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'e477f9'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '1d2cbd'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "106fcfcab27e4b3dad0383b75da4e5c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb2cebf29f3c440ea313f97a1fc48087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d629e770748543e2a2219238a3e1fa3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ebf053ccff4464a90770afc83c52234",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 12 test samples\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What department do you contact for approval of...</td>\n",
              "      <td>[Chapter 1 Academic Years, Academic Calendars,...</td>\n",
              "      <td>To request approval for a full academic year o...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What details are covered in Volume 1 regarding...</td>\n",
              "      <td>[non-term (includes clock-hour calendars), or ...</td>\n",
              "      <td>Volume 1 covers requirements for determining f...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What Volume 8, Chapter 3 say about clinical work?</td>\n",
              "      <td>[Inclusion of Clinical Work in a Standard Term...</td>\n",
              "      <td>Volume 8, Chapter 3 provides additional guidan...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do non-term characteristics affect the dis...</td>\n",
              "      <td>[Non-Term Characteristics A program that measu...</td>\n",
              "      <td>Non-term characteristics, such as programs tha...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the disbursement rules for Pell Grants ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nboth the credit or clock hours and...</td>\n",
              "      <td>In clock-hour or non-term credit-hour programs...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What department do you contact for approval of...   \n",
              "1  What details are covered in Volume 1 regarding...   \n",
              "2  What Volume 8, Chapter 3 say about clinical work?   \n",
              "3  How do non-term characteristics affect the dis...   \n",
              "4  How do the disbursement rules for Pell Grants ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [Chapter 1 Academic Years, Academic Calendars,...   \n",
              "1  [non-term (includes clock-hour calendars), or ...   \n",
              "2  [Inclusion of Clinical Work in a Standard Term...   \n",
              "3  [Non-Term Characteristics A program that measu...   \n",
              "4  [<1-hop>\\n\\nboth the credit or clock hours and...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  To request approval for a full academic year o...   \n",
              "1  Volume 1 covers requirements for determining f...   \n",
              "2  Volume 8, Chapter 3 provides additional guidan...   \n",
              "3  Non-term characteristics, such as programs tha...   \n",
              "4  In clock-hour or non-term credit-hour programs...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  multi_hop_abstract_query_synthesizer  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate synthetic test dataset (same implementation as original)\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)\n",
        "\n",
        "print(f\"Generated {len(dataset)} test samples\")\n",
        "dataset.to_pandas().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Baseline RAG Implementation (Naive Chunking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive chunking created 1102 chunks\n",
            "Average chunk length: 864 characters\n"
          ]
        }
      ],
      "source": [
        "# Naive chunking using RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "naive_text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, \n",
        "    chunk_overlap=200\n",
        ")\n",
        "naive_split_documents = naive_text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Naive chunking created {len(naive_split_documents)} chunks\")\n",
        "print(f\"Average chunk length: {np.mean([len(doc.page_content) for doc in naive_split_documents]):.0f} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline vector store created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set up embeddings and vector store for baseline\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create in-memory vector store for baseline\n",
        "client_baseline = QdrantClient(\":memory:\")\n",
        "client_baseline.create_collection(\n",
        "    collection_name=\"loan_data_baseline\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vector_store_baseline = QdrantVectorStore(\n",
        "    client=client_baseline,\n",
        "    collection_name=\"loan_data_baseline\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Add documents to vector store\n",
        "_ = vector_store_baseline.add_documents(documents=naive_split_documents)\n",
        "retriever_baseline = vector_store_baseline.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(\"Baseline vector store created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline RAG graph created successfully!\n"
          ]
        }
      ],
      "source": [
        "# LangGraph implementation for baseline RAG\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain_core.documents import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# State definition\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    response: str\n",
        "\n",
        "# RAG prompt\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Context\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
        "\n",
        "# LLM for generation\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define nodes\n",
        "def retrieve_baseline(state):\n",
        "    retrieved_docs = retriever_baseline.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"response\": response.content}\n",
        "\n",
        "# Build baseline graph\n",
        "baseline_graph_builder = StateGraph(State).add_sequence([retrieve_baseline, generate])\n",
        "baseline_graph_builder.add_edge(START, \"retrieve_baseline\")\n",
        "baseline_graph = baseline_graph_builder.compile()\n",
        "\n",
        "print(\"Baseline RAG graph created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Semantic Chunking Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic chunking configuration:\n",
            "- Similarity threshold: 0.7\n",
            "- Max chunk size: 1000 characters\n",
            "- Min chunk size: 1 sentence(s)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Configuration for semantic chunking\n",
        "SIMILARITY_THRESHOLD = 0.7  # Cosine similarity threshold for grouping sentences\n",
        "MAX_CHUNK_SIZE = 1000  # Maximum characters per chunk\n",
        "MIN_CHUNK_SIZE = 1  # Minimum chunk size (single sentence)\n",
        "\n",
        "# Load sentence transformer model for semantic similarity\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(f\"Semantic chunking configuration:\")\n",
        "print(f\"- Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
        "print(f\"- Max chunk size: {MAX_CHUNK_SIZE} characters\")\n",
        "print(f\"- Min chunk size: {MIN_CHUNK_SIZE} sentence(s)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic chunking function defined!\n"
          ]
        }
      ],
      "source": [
        "def split_into_sentences(text):\n",
        "    \"\"\"Split text into sentences using regex.\"\"\"\n",
        "    # Simple sentence splitting - can be improved with NLTK or spaCy\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def semantic_chunking(documents, similarity_threshold=SIMILARITY_THRESHOLD, max_chunk_size=MAX_CHUNK_SIZE):\n",
        "    \"\"\"\n",
        "    Implement semantic chunking strategy:\n",
        "    1. Split documents into sentences\n",
        "    2. Group semantically similar sentences using cosine similarity\n",
        "    3. Use greedy approach up to maximum chunk size\n",
        "    4. Minimum chunk size is a single sentence\n",
        "    \"\"\"\n",
        "    semantic_chunks = []\n",
        "    \n",
        "    for doc in documents:\n",
        "        text = doc.page_content\n",
        "        sentences = split_into_sentences(text)\n",
        "        \n",
        "        if not sentences:\n",
        "            continue\n",
        "            \n",
        "        # Get sentence embeddings\n",
        "        sentence_embeddings = sentence_model.encode(sentences)\n",
        "        \n",
        "        # Start with first sentence\n",
        "        current_chunk_sentences = [sentences[0]]\n",
        "        current_chunk_embeddings = [sentence_embeddings[0]]\n",
        "        \n",
        "        for i in range(1, len(sentences)):\n",
        "            sentence = sentences[i]\n",
        "            sentence_embedding = sentence_embeddings[i]\n",
        "            \n",
        "            # Calculate similarity with current chunk (average embedding)\n",
        "            current_chunk_avg_embedding = np.mean(current_chunk_embeddings, axis=0).reshape(1, -1)\n",
        "            sentence_embedding_reshaped = sentence_embedding.reshape(1, -1)\n",
        "            similarity = cosine_similarity(current_chunk_avg_embedding, sentence_embedding_reshaped)[0][0]\n",
        "            \n",
        "            # Check if we should add to current chunk\n",
        "            potential_chunk_text = ' '.join(current_chunk_sentences + [sentence])\n",
        "            \n",
        "            # Greedy approach: add if similar OR if we haven't exceeded max size\n",
        "            if (similarity >= similarity_threshold or len(potential_chunk_text) <= max_chunk_size) and len(potential_chunk_text) <= max_chunk_size:\n",
        "                current_chunk_sentences.append(sentence)\n",
        "                current_chunk_embeddings.append(sentence_embedding)\n",
        "            else:\n",
        "                # Finalize current chunk and start new one\n",
        "                chunk_text = ' '.join(current_chunk_sentences)\n",
        "                if chunk_text.strip():\n",
        "                    semantic_chunks.append({\n",
        "                        'content': chunk_text,\n",
        "                        'metadata': doc.metadata\n",
        "                    })\n",
        "                \n",
        "                # Start new chunk with current sentence\n",
        "                current_chunk_sentences = [sentence]\n",
        "                current_chunk_embeddings = [sentence_embedding]\n",
        "        \n",
        "        # Add final chunk\n",
        "        chunk_text = ' '.join(current_chunk_sentences)\n",
        "        if chunk_text.strip():\n",
        "            semantic_chunks.append({\n",
        "                'content': chunk_text,\n",
        "                'metadata': doc.metadata\n",
        "            })\n",
        "    \n",
        "    return semantic_chunks\n",
        "\n",
        "print(\"Semantic chunking function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying semantic chunking...\n",
            "Semantic chunking created 1057 chunks\n",
            "Average chunk length: 792 characters\n",
            "\n",
            "Chunk Statistics Comparison:\n",
            "Naive chunking: 1102 chunks, avg 864 chars, std 189\n",
            "Semantic chunking: 1057 chunks, avg 792 chars, std 236\n"
          ]
        }
      ],
      "source": [
        "# Apply semantic chunking to documents\n",
        "print(\"Applying semantic chunking...\")\n",
        "semantic_chunk_data = semantic_chunking(docs)\n",
        "\n",
        "# Convert to Document objects for compatibility\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "semantic_split_documents = []\n",
        "for chunk_data in semantic_chunk_data:\n",
        "    doc = Document(\n",
        "        page_content=chunk_data['content'],\n",
        "        metadata=chunk_data['metadata']\n",
        "    )\n",
        "    semantic_split_documents.append(doc)\n",
        "\n",
        "print(f\"Semantic chunking created {len(semantic_split_documents)} chunks\")\n",
        "print(f\"Average chunk length: {np.mean([len(doc.page_content) for doc in semantic_split_documents]):.0f} characters\")\n",
        "\n",
        "# Compare chunk statistics\n",
        "naive_lengths = [len(doc.page_content) for doc in naive_split_documents]\n",
        "semantic_lengths = [len(doc.page_content) for doc in semantic_split_documents]\n",
        "\n",
        "print(f\"\\nChunk Statistics Comparison:\")\n",
        "print(f\"Naive chunking: {len(naive_split_documents)} chunks, avg {np.mean(naive_lengths):.0f} chars, std {np.std(naive_lengths):.0f}\")\n",
        "print(f\"Semantic chunking: {len(semantic_split_documents)} chunks, avg {np.mean(semantic_lengths):.0f} chars, std {np.std(semantic_lengths):.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Advanced RAG Implementation (Semantic Chunking + Naive Retrieval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic RAG vector store created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set up vector store for semantic chunking\n",
        "client_semantic = QdrantClient(\":memory:\")\n",
        "client_semantic.create_collection(\n",
        "    collection_name=\"loan_data_semantic\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vector_store_semantic = QdrantVectorStore(\n",
        "    client=client_semantic,\n",
        "    collection_name=\"loan_data_semantic\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Add semantic chunks to vector store\n",
        "_ = vector_store_semantic.add_documents(documents=semantic_split_documents)\n",
        "retriever_semantic = vector_store_semantic.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(\"Semantic RAG vector store created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic RAG graph created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define semantic retrieval node\n",
        "def retrieve_semantic(state):\n",
        "    retrieved_docs = retriever_semantic.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "# Build semantic RAG graph\n",
        "semantic_graph_builder = StateGraph(State).add_sequence([retrieve_semantic, generate])\n",
        "semantic_graph_builder.add_edge(START, \"retrieve_semantic\")\n",
        "semantic_graph = semantic_graph_builder.compile()\n",
        "\n",
        "print(\"Semantic RAG graph created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Baseline Evaluation (Naive Chunking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running baseline evaluation...\n",
            "Baseline evaluation data collection complete!\n"
          ]
        }
      ],
      "source": [
        "# Run baseline RAG on test dataset\n",
        "import copy\n",
        "import time\n",
        "\n",
        "print(\"Running baseline evaluation...\")\n",
        "baseline_dataset = copy.deepcopy(dataset)\n",
        "\n",
        "for test_row in baseline_dataset:\n",
        "    response = baseline_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
        "    test_row.eval_sample.response = response[\"response\"]\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "    time.sleep(1)  # Rate limiting\n",
        "\n",
        "print(\"Baseline evaluation data collection complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating baseline RAG...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91f15e8254dd44e3acdbbce35b6d97f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline evaluation complete!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.7580, 'answer_relevancy': 0.9638, 'context_precision': 0.9375, 'context_recall': 0.6250, 'answer_correctness': 0.5618}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate baseline with Ragas using exact specified metrics\n",
        "from ragas import EvaluationDataset, evaluate, RunConfig\n",
        "from ragas.metrics import Faithfulness, AnswerRelevancy, ContextPrecision, ContextRecall, AnswerCorrectness\n",
        "\n",
        "# Create evaluation dataset\n",
        "baseline_evaluation_dataset = EvaluationDataset.from_pandas(baseline_dataset.to_pandas())\n",
        "\n",
        "# Set up evaluator LLM (same as original)\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "\n",
        "# Custom run config for longer timeout\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "print(\"Evaluating baseline RAG...\")\n",
        "baseline_result = evaluate(\n",
        "    dataset=baseline_evaluation_dataset,\n",
        "    metrics=[\n",
        "        Faithfulness(),\n",
        "        AnswerRelevancy(), \n",
        "        ContextPrecision(),\n",
        "        ContextRecall(),\n",
        "        AnswerCorrectness()\n",
        "    ],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "\n",
        "print(\"Baseline evaluation complete!\")\n",
        "baseline_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Advanced Evaluation (Semantic Chunking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running semantic evaluation...\n",
            "Semantic evaluation data collection complete!\n"
          ]
        }
      ],
      "source": [
        "# Run semantic RAG on test dataset\n",
        "print(\"Running semantic evaluation...\")\n",
        "semantic_dataset = copy.deepcopy(dataset)\n",
        "\n",
        "for test_row in semantic_dataset:\n",
        "    response = semantic_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
        "    test_row.eval_sample.response = response[\"response\"]\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "    time.sleep(1)  # Rate limiting\n",
        "\n",
        "print(\"Semantic evaluation data collection complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating semantic RAG...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ea73a2025ed4e1196511895cc7cc63d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic evaluation complete!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.8128, 'answer_relevancy': 0.9598, 'context_precision': 0.9167, 'context_recall': 0.6736, 'answer_correctness': 0.6238}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate semantic RAG with same metrics\n",
        "semantic_evaluation_dataset = EvaluationDataset.from_pandas(semantic_dataset.to_pandas())\n",
        "\n",
        "print(\"Evaluating semantic RAG...\")\n",
        "semantic_result = evaluate(\n",
        "    dataset=semantic_evaluation_dataset,\n",
        "    metrics=[\n",
        "        Faithfulness(),\n",
        "        AnswerRelevancy(), \n",
        "        ContextPrecision(),\n",
        "        ContextRecall(),\n",
        "        AnswerCorrectness()\n",
        "    ],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "\n",
        "print(\"Semantic evaluation complete!\")\n",
        "semantic_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Side-by-Side Metric Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'list' and 'int'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m improvements = []\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m baseline, semantic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(comparison_data[\u001b[33m'\u001b[39m\u001b[33mBaseline (Naive)\u001b[39m\u001b[33m'\u001b[39m], comparison_data[\u001b[33m'\u001b[39m\u001b[33mAdvanced (Semantic)\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     improvement = ((semantic - baseline) / baseline) * \u001b[32m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbaseline\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     24\u001b[39m     improvements.append(improvement)\n\u001b[32m     26\u001b[39m comparison_data[\u001b[33m'\u001b[39m\u001b[33mImprovement (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m] = improvements\n",
            "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'list' and 'int'"
          ]
        }
      ],
      "source": [
        "# Debug: Check the structure of results first\n",
        "print(\"ðŸ” DEBUGGING RESULTS STRUCTURE\")\n",
        "print(\"Baseline result keys:\", baseline_result.keys())\n",
        "print(\"Semantic result keys:\", semantic_result.keys())\n",
        "print(\"\\nBaseline values and types:\")\n",
        "for key, value in baseline_result.items():\n",
        "    print(f\"  {key}: {value} (type: {type(value)})\")\n",
        "print(\"\\nSemantic values and types:\")\n",
        "for key, value in semantic_result.items():\n",
        "    print(f\"  {key}: {value} (type: {type(value)})\")\n",
        "\n",
        "# Function to safely extract scalar values\n",
        "def extract_scalar_value(value):\n",
        "    \"\"\"Extract scalar value from potentially nested structures\"\"\"\n",
        "    if isinstance(value, list):\n",
        "        # If it's a list, take the first element or mean\n",
        "        if len(value) > 0:\n",
        "            if isinstance(value[0], (int, float)):\n",
        "                return float(value[0])\n",
        "            else:\n",
        "                return 0.0\n",
        "        else:\n",
        "            return 0.0\n",
        "    elif isinstance(value, (int, float)):\n",
        "        return float(value)\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# Create side-by-side comparison table with safe value extraction\n",
        "baseline_values = [\n",
        "    extract_scalar_value(baseline_result['faithfulness']),\n",
        "    extract_scalar_value(baseline_result['answer_relevancy']),\n",
        "    extract_scalar_value(baseline_result['context_precision']),\n",
        "    extract_scalar_value(baseline_result['context_recall']),\n",
        "    extract_scalar_value(baseline_result['answer_correctness'])\n",
        "]\n",
        "\n",
        "semantic_values = [\n",
        "    extract_scalar_value(semantic_result['faithfulness']),\n",
        "    extract_scalar_value(semantic_result['answer_relevancy']),\n",
        "    extract_scalar_value(semantic_result['context_precision']),\n",
        "    extract_scalar_value(semantic_result['context_recall']),\n",
        "    extract_scalar_value(semantic_result['answer_correctness'])\n",
        "]\n",
        "\n",
        "print(\"\\nâœ… EXTRACTED VALUES:\")\n",
        "print(\"Baseline values:\", baseline_values)\n",
        "print(\"Semantic values:\", semantic_values)\n",
        "\n",
        "comparison_data = {\n",
        "    'Metric': ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall', 'Answer Correctness'],\n",
        "    'Baseline (Naive)': baseline_values,\n",
        "    'Advanced (Semantic)': semantic_values\n",
        "}\n",
        "\n",
        "# Calculate improvements safely\n",
        "improvements = []\n",
        "for baseline, semantic in zip(comparison_data['Baseline (Naive)'], comparison_data['Advanced (Semantic)']):\n",
        "    if baseline > 0:\n",
        "        improvement = ((semantic - baseline) / baseline) * 100\n",
        "    else:\n",
        "        improvement = 0.0\n",
        "    improvements.append(improvement)\n",
        "\n",
        "comparison_data['Improvement (%)'] = improvements\n",
        "\n",
        "# Create DataFrame\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Improvement (%)'] = comparison_df['Improvement (%)'].round(2)\n",
        "comparison_df['Baseline (Naive)'] = comparison_df['Baseline (Naive)'].round(4)\n",
        "comparison_df['Advanced (Semantic)'] = comparison_df['Advanced (Semantic)'].round(4)\n",
        "\n",
        "print(\"ðŸ”¥ RAG EVALUATION COMPARISON ðŸ”¥\")\n",
        "print(\"=\" * 60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Highlight best performing system for each metric\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    metric = row['Metric']\n",
        "    baseline_val = row['Baseline (Naive)']\n",
        "    semantic_val = row['Advanced (Semantic)']\n",
        "    improvement = row['Improvement (%)']\n",
        "    \n",
        "    winner = \"ðŸ† SEMANTIC\" if semantic_val > baseline_val else \"ðŸ† BASELINE\"\n",
        "    print(f\"{metric}: {winner} (+{improvement:.2f}%)\" if improvement > 0 else f\"{metric}: {winner} ({improvement:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Visualizations and Charts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('RAG Evaluation: Naive vs Semantic Chunking', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Bar chart comparison\n",
        "metrics = comparison_df['Metric']\n",
        "baseline_scores = comparison_df['Baseline (Naive)']\n",
        "semantic_scores = comparison_df['Advanced (Semantic)']\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax1 = axes[0, 0]\n",
        "bars1 = ax1.bar(x - width/2, baseline_scores, width, label='Baseline (Naive)', alpha=0.8, color='skyblue')\n",
        "bars2 = ax1.bar(x + width/2, semantic_scores, width, label='Advanced (Semantic)', alpha=0.8, color='lightcoral')\n",
        "\n",
        "ax1.set_xlabel('Metrics')\n",
        "ax1.set_ylabel('Scores')\n",
        "ax1.set_title('Performance Comparison by Metric')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 2. Improvement percentage chart\n",
        "ax2 = axes[0, 1]\n",
        "colors = ['green' if x > 0 else 'red' for x in comparison_df['Improvement (%)']]\n",
        "bars = ax2.bar(metrics, comparison_df['Improvement (%)'], color=colors, alpha=0.7)\n",
        "ax2.set_xlabel('Metrics')\n",
        "ax2.set_ylabel('Improvement (%)')\n",
        "ax2.set_title('Improvement: Semantic vs Baseline')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax2.annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                xytext=(0, 3 if height > 0 else -15), textcoords=\"offset points\", \n",
        "                ha='center', va='bottom' if height > 0 else 'top', fontsize=9)\n",
        "\n",
        "# 3. Chunk size distribution comparison\n",
        "ax3 = axes[1, 0]\n",
        "ax3.hist(naive_lengths, bins=20, alpha=0.7, label='Naive Chunking', color='skyblue', density=True)\n",
        "ax3.hist(semantic_lengths, bins=20, alpha=0.7, label='Semantic Chunking', color='lightcoral', density=True)\n",
        "ax3.set_xlabel('Chunk Size (characters)')\n",
        "ax3.set_ylabel('Density')\n",
        "ax3.set_title('Chunk Size Distribution Comparison')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Radar chart for metric comparison\n",
        "from math import pi\n",
        "\n",
        "ax4 = axes[1, 1]\n",
        "categories = metrics\n",
        "N = len(categories)\n",
        "\n",
        "# Compute angles for each metric\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]  # Complete the circle\n",
        "\n",
        "# Add values\n",
        "baseline_values = list(baseline_scores) + [baseline_scores[0]]\n",
        "semantic_values = list(semantic_scores) + [semantic_scores[0]]\n",
        "\n",
        "# Plot\n",
        "ax4.plot(angles, baseline_values, 'o-', linewidth=2, label='Baseline (Naive)', color='skyblue')\n",
        "ax4.fill(angles, baseline_values, alpha=0.25, color='skyblue')\n",
        "ax4.plot(angles, semantic_values, 'o-', linewidth=2, label='Advanced (Semantic)', color='lightcoral')\n",
        "ax4.fill(angles, semantic_values, alpha=0.25, color='lightcoral')\n",
        "\n",
        "# Add labels\n",
        "ax4.set_xticks(angles[:-1])\n",
        "ax4.set_xticklabels(categories, fontsize=9)\n",
        "ax4.set_title('Radar Chart: Performance Profile')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Plotly visualization\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Metric Comparison', 'Improvement Analysis', 'Chunk Statistics', 'Score Distribution'),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# 1. Metric comparison\n",
        "fig.add_trace(\n",
        "    go.Bar(name='Baseline (Naive)', x=metrics, y=baseline_scores, \n",
        "           marker_color='lightblue', text=[f'{v:.3f}' for v in baseline_scores], textposition='outside'),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Bar(name='Advanced (Semantic)', x=metrics, y=semantic_scores,\n",
        "           marker_color='lightcoral', text=[f'{v:.3f}' for v in semantic_scores], textposition='outside'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# 2. Improvement analysis\n",
        "colors = ['green' if x > 0 else 'red' for x in comparison_df['Improvement (%)']]\n",
        "fig.add_trace(\n",
        "    go.Bar(x=metrics, y=comparison_df['Improvement (%)'], marker_color=colors,\n",
        "           text=[f'{v:.1f}%' for v in comparison_df['Improvement (%)']], textposition='outside',\n",
        "           name='Improvement', showlegend=False),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# 3. Chunk statistics comparison\n",
        "fig.add_trace(\n",
        "    go.Box(y=naive_lengths, name='Naive Chunking', marker_color='lightblue'),\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Box(y=semantic_lengths, name='Semantic Chunking', marker_color='lightcoral'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# 4. Score distribution\n",
        "all_baseline = list(baseline_scores)\n",
        "all_semantic = list(semantic_scores)\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=all_baseline, name='Baseline Distribution', opacity=0.7, marker_color='lightblue'),\n",
        "    row=2, col=2\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=all_semantic, name='Semantic Distribution', opacity=0.7, marker_color='lightcoral'),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Interactive RAG Evaluation Dashboard\",\n",
        "    title_x=0.5,\n",
        "    height=800,\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Update axes\n",
        "fig.update_xaxes(title_text=\"Metrics\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Metrics\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Chunking Method\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Score Values\", row=2, col=2)\n",
        "\n",
        "fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Improvement (%)\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Chunk Size (chars)\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 11. Statistical Significance Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract individual sample scores for statistical testing\n",
        "baseline_df = baseline_dataset.to_pandas()\n",
        "semantic_df = semantic_dataset.to_pandas()\n",
        "\n",
        "# Get individual metric scores (note: these are aggregate scores, but we'll work with what we have)\n",
        "print(\"ðŸ”¬ STATISTICAL SIGNIFICANCE ANALYSIS ðŸ”¬\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Since we have limited samples, we'll focus on effect size and practical significance\n",
        "baseline_values = [baseline_result[metric.lower().replace(' ', '_')] for metric in ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall', 'Answer Correctness']]\n",
        "semantic_values = [semantic_result[metric.lower().replace(' ', '_')] for metric in ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall', 'Answer Correctness']]\n",
        "\n",
        "# Calculate effect sizes (Cohen's d)\n",
        "def cohens_d(x1, x2):\n",
        "    \"\"\"Calculate Cohen's d for effect size\"\"\"\n",
        "    # Since we only have aggregate scores, we'll approximate\n",
        "    # This is a simplified approach for demonstration\n",
        "    diff = x2 - x1\n",
        "    # Approximate pooled standard deviation (simplified)\n",
        "    pooled_std = np.sqrt(((x1 * 0.1) ** 2 + (x2 * 0.1) ** 2) / 2)\n",
        "    return diff / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "# Effect size analysis\n",
        "effect_sizes = []\n",
        "for i, metric in enumerate(['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall', 'Answer Correctness']):\n",
        "    baseline_val = baseline_values[i]\n",
        "    semantic_val = semantic_values[i]\n",
        "    effect_size = cohens_d(baseline_val, semantic_val)\n",
        "    effect_sizes.append(effect_size)\n",
        "    \n",
        "    # Interpret effect size\n",
        "    if abs(effect_size) < 0.2:\n",
        "        interpretation = \"Negligible\"\n",
        "    elif abs(effect_size) < 0.5:\n",
        "        interpretation = \"Small\"\n",
        "    elif abs(effect_size) < 0.8:\n",
        "        interpretation = \"Medium\"\n",
        "    else:\n",
        "        interpretation = \"Large\"\n",
        "    \n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Baseline: {baseline_val:.4f} | Semantic: {semantic_val:.4f}\")\n",
        "    print(f\"  Effect Size (Cohen's d): {effect_size:.3f} ({interpretation})\")\n",
        "    print(f\"  Practical Significance: {'âœ… YES' if abs(effect_size) > 0.2 else 'âŒ NO'}\")\n",
        "    print()\n",
        "\n",
        "# Overall assessment\n",
        "print(\"ðŸ“Š OVERALL STATISTICAL ASSESSMENT\")\n",
        "print(\"=\" * 40)\n",
        "positive_improvements = sum(1 for es in effect_sizes if es > 0.2)\n",
        "total_metrics = len(effect_sizes)\n",
        "print(f\"Metrics with practical improvement: {positive_improvements}/{total_metrics}\")\n",
        "print(f\"Average effect size: {np.mean(effect_sizes):.3f}\")\n",
        "print(f\"Maximum effect size: {max(effect_sizes):.3f}\")\n",
        "print(f\"Minimum effect size: {min(effect_sizes):.3f}\")\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional statistical analysis: chunk size comparison\n",
        "print(\"\\nðŸ“ CHUNK SIZE STATISTICAL ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Perform t-test on chunk sizes\n",
        "t_stat, p_value = stats.ttest_ind(naive_lengths, semantic_lengths)\n",
        "print(\"T-test for chunk sizes:\")\n",
        "print(f\"  T-statistic: {t_stat:.3f}\")\n",
        "print(f\"  P-value: {p_value:.6f}\")\n",
        "print(f\"  Significance: {'âœ… Significant' if p_value < 0.05 else 'âŒ Not significant'} (Î± = 0.05)\")\n",
        "\n",
        "# Descriptive statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(\"Naive Chunking:\")\n",
        "print(f\"  Mean: {np.mean(naive_lengths):.1f} chars\")\n",
        "print(f\"  Std:  {np.std(naive_lengths):.1f} chars\")\n",
        "print(f\"  Min:  {np.min(naive_lengths)} chars\")\n",
        "print(f\"  Max:  {np.max(naive_lengths)} chars\")\n",
        "print(f\"  Q1:   {np.percentile(naive_lengths, 25):.1f} chars\")\n",
        "print(f\"  Q3:   {np.percentile(naive_lengths, 75):.1f} chars\")\n",
        "\n",
        "print(\"\\nSemantic Chunking:\")\n",
        "print(f\"  Mean: {np.mean(semantic_lengths):.1f} chars\")\n",
        "print(f\"  Std:  {np.std(semantic_lengths):.1f} chars\")\n",
        "print(f\"  Min:  {np.min(semantic_lengths)} chars\")\n",
        "print(f\"  Max:  {np.max(semantic_lengths)} chars\")\n",
        "print(f\"  Q1:   {np.percentile(semantic_lengths, 25):.1f} chars\")\n",
        "print(f\"  Q3:   {np.percentile(semantic_lengths, 75):.1f} chars\")\n",
        "\n",
        "# Calculate variance ratio\n",
        "variance_ratio = np.var(semantic_lengths) / np.var(naive_lengths)\n",
        "print(\"\\nVariance Analysis:\")\n",
        "print(f\"  Variance Ratio (Semantic/Naive): {variance_ratio:.3f}\")\n",
        "print(f\"  Interpretation: {'More variable' if variance_ratio > 1 else 'Less variable'} chunk sizes in semantic approach\")\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 12. Qualitative Analysis of Response Quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qualitative analysis of responses\n",
        "print(\"ðŸ” QUALITATIVE RESPONSE ANALYSIS ðŸ”\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample some questions and compare responses\n",
        "sample_questions = baseline_df['user_input'].head(3).tolist()\n",
        "\n",
        "for i, question in enumerate(sample_questions):\n",
        "    print(\"\\n\" + \"=\"*20 + f\" QUESTION {i+1} \" + \"=\"*20)\n",
        "    print(f\"Q: {question}\")\n",
        "    print()\n",
        "    \n",
        "    baseline_response = baseline_df.iloc[i]['response']\n",
        "    semantic_response = semantic_df.iloc[i]['response']\n",
        "    \n",
        "    print(\"ðŸ”¸ BASELINE (Naive Chunking) RESPONSE:\")\n",
        "    response_preview = baseline_response[:300] + \"...\" if len(baseline_response) > 300 else baseline_response\n",
        "    print(response_preview)\n",
        "    print()\n",
        "    \n",
        "    print(\"ðŸ”¹ SEMANTIC CHUNKING RESPONSE:\")\n",
        "    response_preview = semantic_response[:300] + \"...\" if len(semantic_response) > 300 else semantic_response\n",
        "    print(response_preview)\n",
        "    print()\n",
        "    \n",
        "    # Simple quality metrics\n",
        "    baseline_len = len(baseline_response)\n",
        "    semantic_len = len(semantic_response)\n",
        "    \n",
        "    print(\"ðŸ“Š RESPONSE COMPARISON:\")\n",
        "    print(f\"  Length: Baseline {baseline_len} chars | Semantic {semantic_len} chars\")\n",
        "    if baseline_len > 0:\n",
        "        print(f\"  Relative length: {semantic_len/baseline_len:.2f}x\")\n",
        "    else:\n",
        "        print(\"  Relative length: N/A\")\n",
        "    \n",
        "    # Count specific words that might indicate quality\n",
        "    uncertainty_words = ['however', 'but', 'although', 'unclear', 'unsure']\n",
        "    baseline_confidence_words = len([w for w in baseline_response.lower().split() if w in uncertainty_words])\n",
        "    semantic_confidence_words = len([w for w in semantic_response.lower().split() if w in uncertainty_words])\n",
        "    \n",
        "    print(f\"  Uncertainty indicators: Baseline {baseline_confidence_words} | Semantic {semantic_confidence_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Context analysis - compare retrieved contexts\n",
        "print(\"\\nðŸŽ¯ RETRIEVED CONTEXT ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, question in enumerate(sample_questions[:2]):  # Analyze first 2 questions\n",
        "    print(f\"\\n--- QUESTION {i+1}: {question[:100]}... ---\")\n",
        "    \n",
        "    baseline_contexts = baseline_df.iloc[i]['retrieved_contexts']\n",
        "    semantic_contexts = semantic_df.iloc[i]['retrieved_contexts']\n",
        "    \n",
        "    print(f\"\\nðŸ”¸ BASELINE CONTEXTS ({len(baseline_contexts)} chunks):\")\n",
        "    for j, context in enumerate(baseline_contexts):\n",
        "        print(f\"  Chunk {j+1}: {len(context)} chars - {context[:150]}...\")\n",
        "    \n",
        "    print(f\"\\nðŸ”¹ SEMANTIC CONTEXTS ({len(semantic_contexts)} chunks):\")\n",
        "    for j, context in enumerate(semantic_contexts):\n",
        "        print(f\"  Chunk {j+1}: {len(context)} chars - {context[:150]}...\")\n",
        "    \n",
        "    # Calculate overlap between contexts\n",
        "    baseline_text = \" \".join(baseline_contexts).lower()\n",
        "    semantic_text = \" \".join(semantic_contexts).lower()\n",
        "    \n",
        "    # Simple word overlap calculation\n",
        "    baseline_words = set(baseline_text.split())\n",
        "    semantic_words = set(semantic_text.split())\n",
        "    overlap = len(baseline_words.intersection(semantic_words))\n",
        "    union = len(baseline_words.union(semantic_words))\n",
        "    jaccard_similarity = overlap / union if union > 0 else 0\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ CONTEXT SIMILARITY ANALYSIS:\")\n",
        "    print(f\"  Word overlap: {overlap} words\")\n",
        "    print(f\"  Jaccard similarity: {jaccard_similarity:.3f}\")\n",
        "    diversity = 'High' if jaccard_similarity < 0.5 else 'Moderate' if jaccard_similarity < 0.8 else 'Low'\n",
        "    print(f\"  Context diversity: {diversity}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 13. Executive Summary and Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executive Summary\n",
        "print(\"ðŸŽ¯ EXECUTIVE SUMMARY: SEMANTIC CHUNKING vs NAIVE CHUNKING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate overall winner\n",
        "wins_semantic = sum(1 for semantic, baseline in zip(semantic_values, baseline_values) if semantic > baseline)\n",
        "wins_baseline = len(baseline_values) - wins_semantic\n",
        "\n",
        "winner_text = 'SEMANTIC CHUNKING' if wins_semantic > wins_baseline else 'BASELINE (NAIVE)' if wins_baseline > wins_semantic else 'TIE'\n",
        "print(f\"\\nðŸ† OVERALL WINNER: {winner_text}\")\n",
        "print(f\"   Semantic wins: {wins_semantic}/{len(baseline_values)} metrics\")\n",
        "print(f\"   Baseline wins: {wins_baseline}/{len(baseline_values)} metrics\")\n",
        "\n",
        "# Key findings\n",
        "print(\"\\nðŸ“Š KEY FINDINGS:\")\n",
        "avg_improvement = np.mean(comparison_df['Improvement (%)'])\n",
        "best_improvement = max(comparison_df['Improvement (%)'])\n",
        "worst_improvement = min(comparison_df['Improvement (%)'])\n",
        "best_metric = comparison_df.loc[comparison_df['Improvement (%)'].idxmax(), 'Metric']\n",
        "worst_metric = comparison_df.loc[comparison_df['Improvement (%)'].idxmin(), 'Metric']\n",
        "\n",
        "print(f\"   â€¢ Average improvement: {avg_improvement:.1f}%\")\n",
        "print(f\"   â€¢ Best improvement: {best_improvement:.1f}% in {best_metric}\")\n",
        "print(f\"   â€¢ Worst performance: {worst_improvement:.1f}% in {worst_metric}\")\n",
        "\n",
        "# Practical implications\n",
        "print(\"\\nðŸ’¡ PRACTICAL IMPLICATIONS:\")\n",
        "if avg_improvement > 5:\n",
        "    print(\"   âœ… Semantic chunking shows meaningful improvements\")\n",
        "    print(\"   âœ… Recommended for production deployment\")\n",
        "    print(\"   âœ… Benefits likely outweigh computational overhead\")\n",
        "elif avg_improvement > 0:\n",
        "    print(\"   âš ï¸ Semantic chunking shows modest improvements\")\n",
        "    print(\"   âš ï¸ Consider computational cost vs. benefit trade-off\")\n",
        "    print(\"   âš ï¸ May be suitable for high-accuracy requirements\")\n",
        "else:\n",
        "    print(\"   âŒ Semantic chunking does not show clear advantages\")\n",
        "    print(\"   âŒ Baseline approach may be more cost-effective\")\n",
        "    print(\"   âŒ Further optimization of semantic approach recommended\")\n",
        "\n",
        "# Technical recommendations\n",
        "print(\"\\nðŸ”§ TECHNICAL RECOMMENDATIONS:\")\n",
        "variance_text = 'Higher' if variance_ratio > 1 else 'Lower'\n",
        "print(f\"   â€¢ Chunk size variance: {variance_text} in semantic approach\")\n",
        "print(f\"   â€¢ Similarity threshold: {SIMILARITY_THRESHOLD} (consider tuning)\")\n",
        "print(f\"   â€¢ Max chunk size: {MAX_CHUNK_SIZE} chars (consider optimization)\")\n",
        "print(\"   â€¢ Embedding model: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ NEXT STEPS:\")\n",
        "print(\"   1. Hyperparameter tuning for similarity threshold\")\n",
        "print(\"   2. Experiment with different sentence embedding models\")\n",
        "print(\"   3. A/B testing with larger datasets\")\n",
        "print(\"   4. Cost-benefit analysis including computational overhead\")\n",
        "print(\"   5. User satisfaction evaluation\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ“ˆ EVALUATION COMPLETE - DATA DRIVEN INSIGHTS DELIVERED! ðŸš€\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“ Brief Explanations for Advanced_Build Notebook Cells\n",
        "\n",
        "Here are concise explanations for each code cell that you can add as markdown documentation:\n",
        "\n",
        "## **Cell 2: Library Imports and Setup**\n",
        "Import essential libraries for data processing, machine learning, visualization, and RAG implementation. Configure plotting styles and suppress warnings for cleaner output.\n",
        "\n",
        "## **Cell 3: API Key Configuration**\n",
        "Securely capture and store OpenAI API credentials needed for LLM and embedding model access throughout the notebook.\n",
        "\n",
        "## **Cell 4: Document Loading**\n",
        "Load PDF documents from the data directory using PyMuPDF loader to extract text content for RAG system processing.\n",
        "\n",
        "## **Cell 5: Model Setup for Test Generation**\n",
        "Initialize LLM (GPT-4o) and embedding models with Ragas wrappers for automated test dataset generation.\n",
        "\n",
        "## **Cell 6: Synthetic Test Dataset Creation**\n",
        "Use Ragas TestsetGenerator to automatically create evaluation questions, reference answers, and contexts from the loaded documents.\n",
        "\n",
        "## **Cell 7: Naive Chunking Implementation**\n",
        "Split documents using RecursiveCharacterTextSplitter with fixed 1000-character chunks and 200-character overlap - the baseline chunking strategy.\n",
        "\n",
        "## **Cell 8: Baseline Vector Store Setup**\n",
        "Create in-memory Qdrant vector database, embed naive chunks using OpenAI embeddings, and configure retriever for similarity search.\n",
        "\n",
        "## **Cell 9: Baseline RAG Graph Construction**\n",
        "Build LangGraph workflow connecting retrieval and generation nodes to create the baseline RAG system pipeline.\n",
        "\n",
        "## **Cell 10: Semantic Chunking Configuration**\n",
        "Define parameters for semantic chunking approach including similarity threshold (0.7), max chunk size (1000), and load sentence transformer model.\n",
        "\n",
        "## **Cell 11: Semantic Chunking Algorithm**\n",
        "Implement core semantic chunking logic that splits text into sentences, calculates semantic similarity, and groups similar sentences into coherent chunks.\n",
        "\n",
        "## **Cell 12: Apply Semantic Chunking**\n",
        "Execute semantic chunking on all documents and convert results to Document format for compatibility with the RAG pipeline.\n",
        "\n",
        "## **Cell 13: Semantic Vector Store Setup**\n",
        "Create separate vector database for semantic chunks using identical embedding model to ensure fair comparison with baseline.\n",
        "\n",
        "## **Cell 14: Semantic RAG Graph Construction**\n",
        "Build identical LangGraph workflow for semantic system, using same generation logic but different chunk retrieval source.\n",
        "\n",
        "## **Cell 15: Baseline Evaluation Data Collection**\n",
        "Run test questions through baseline RAG system, collecting generated responses and retrieved contexts for evaluation.\n",
        "\n",
        "## **Cell 16: Baseline Ragas Evaluation**\n",
        "Apply Ragas evaluation metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall, Answer Correctness) to score baseline performance.\n",
        "\n",
        "## **Cell 17: Semantic Evaluation Data Collection**\n",
        "Run identical test questions through semantic RAG system, collecting responses for direct comparison with baseline.\n",
        "\n",
        "## **Cell 18: Semantic Ragas Evaluation**\n",
        "Apply same Ragas evaluation metrics to semantic system to enable fair performance comparison.\n",
        "\n",
        "## **Cell 19: Performance Comparison Table**\n",
        "Create side-by-side comparison showing baseline vs semantic scores for each metric, calculate percentage improvements.\n",
        "\n",
        "## **Cell 20: Static Visualization Dashboard**\n",
        "Generate comprehensive matplotlib charts including performance comparisons, improvement percentages, chunk distributions, and radar plots.\n",
        "\n",
        "## **Cell 21: Interactive Plotly Dashboard**\n",
        "Build interactive visualization dashboard with hover details and zoom capabilities for deeper exploration of results.\n",
        "\n",
        "## **Cell 22: Statistical Significance Analysis**\n",
        "Perform effect size calculations (Cohen's d) to determine practical significance of performance differences between systems.\n",
        "\n",
        "## **Cell 23: Chunk Size Statistical Analysis**\n",
        "Compare statistical properties of chunk sizes using t-tests and variance analysis to understand structural differences.\n",
        "\n",
        "## **Cell 24: Qualitative Response Analysis**\n",
        "Manual examination of actual responses from both systems, comparing length, confidence indicators, and response quality.\n",
        "\n",
        "## **Cell 25: Context Retrieval Analysis**\n",
        "Analyze what contexts each system retrieves for identical questions, measuring overlap and diversity using Jaccard similarity.\n",
        "\n",
        "## **Cell 26: Executive Summary and Recommendations**\n",
        "Synthesize all evaluation results into actionable insights, determine overall winner, and provide technical recommendations for implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ **Usage Notes:**\n",
        "- Each explanation is designed to be inserted as a markdown cell before its corresponding code cell\n",
        "- Explanations focus on the **purpose and functionality** rather than implementation details\n",
        "- Suitable for both technical and non-technical audiences reviewing the notebook\n",
        "- Can be easily customized or expanded based on your documentation needs"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
