{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced RAG Evaluation: The Great Chunking Debate\n",
        "\n",
        "## When Size Doesn't Matter (But Semantic Coherence Does)\n",
        "\n",
        "In the rapidly evolving landscape of Retrieval-Augmented Generation (RAG), one fundamental question continues to challenge practitioners: **How should we divide our knowledge into digestible pieces?** This notebook ventures into the heart of this question by conducting a rigorous empirical comparison between two fundamentally different approaches to document chunking.\n",
        "\n",
        "The conventional wisdom suggests that splitting text at arbitrary character boundaries—while computationally efficient—may fracture the semantic coherence that makes information truly useful. Yet, does this intuition hold up under scrutiny? Can semantic-aware chunking strategies deliver measurable improvements that justify their additional complexity?\n",
        "\n",
        "## The Experimental Design\n",
        "\n",
        "This investigation implements and evaluates two competing paradigms:\n",
        "\n",
        "### 🔧 **Baseline System**: The Pragmatic Approach\n",
        "- **Strategy**: RecursiveCharacterTextSplitter with fixed boundaries\n",
        "- **Philosophy**: Simple, fast, and widely adopted\n",
        "- **Characteristics**: 1000-character chunks with 200-character overlap\n",
        "\n",
        "### 🧠 **Advanced System**: The Semantic Pioneer  \n",
        "- **Strategy**: Jaccard similarity-based sentence grouping\n",
        "- **Philosophy**: Preserve meaning boundaries, optimize for coherence\n",
        "- **Characteristics**: Variable-sized chunks respecting semantic relationships\n",
        "\n",
        "## The Stakes\n",
        "\n",
        "Both systems face the same rigorous evaluation battery using **five comprehensive Ragas metrics**:\n",
        "- **Faithfulness** - Does the system hallucinate or stay grounded?\n",
        "- **Answer Relevancy** - Does it actually answer what was asked?\n",
        "- **Context Precision** - Is the retrieved information truly relevant?\n",
        "- **Context Recall** - Does it find all the necessary pieces?\n",
        "- **Answer Correctness** - Is the final response accurate?\n",
        "\n",
        "This comparison will reveal not just which approach performs better, but *why* certain chunking strategies succeed or fail in different dimensions of RAG performance. The results may challenge our assumptions about the trade-offs between computational efficiency and semantic intelligence in information retrieval systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup Dependencies and API Keys\n",
        "\n",
        "**Note:** This notebook uses standard Python libraries and Ragas built-in functionality. All required packages should be available in a standard AI/ML environment with LangChain and Ragas installed.\n",
        "\n",
        "The semantic chunking implementation uses simple text-based similarity (Jaccard similarity) to avoid external dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, TypedDict\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "# Qdrant imports\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "# Ragas imports - using correct imports from documentation\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy, \n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_correctness\n",
        ")\n",
        "from ragas import EvaluationDataset, evaluate, RunConfig\n",
        "\n",
        "# For semantic chunking - using only basic libraries\n",
        "import re\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading and Preparation: Setting the Foundation\n",
        "\n",
        "### The Starting Point: Understanding Our Knowledge Base\n",
        "\n",
        "Before we can evaluate different chunking strategies, we need a substantial corpus of real-world documents that will serve as our testing ground. This phase is critical because the characteristics of our source material—its structure, complexity, and content patterns—will significantly influence how different chunking approaches perform.\n",
        "\n",
        "We're working with PDF documents from the `data/` directory, which likely contain structured information about financial aid, loans, and educational policies. These documents represent the kind of dense, formal text that RAG systems commonly encounter in enterprise applications.\n",
        "\n",
        "**Why This Step Matters:**\n",
        "- **Document Diversity**: PDF documents often contain varied formatting, tables, and complex structures that challenge chunking algorithms\n",
        "- **Real-World Relevance**: Using actual policy documents ensures our evaluation reflects genuine use cases\n",
        "- **Baseline Establishment**: Understanding our source material helps us interpret why certain chunking strategies succeed or fail\n",
        "\n",
        "The loading process uses PyMuPDFLoader, which excels at extracting clean text from PDF documents while preserving important structural information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load documents from data directory\n",
        "path = \"data/\"\n",
        "# Note: PyMuPDFLoader handles PDF documents effectively\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "print(f\"Total characters: {sum(len(doc.page_content) for doc in docs)}\")\n",
        "\n",
        "# Show first document metadata for verification\n",
        "if docs:\n",
        "    print(f\"Sample document metadata: {docs[0].metadata}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Generate Synthetic Test Data with Ragas: Creating Our Evaluation Arsenal\n",
        "\n",
        "### The Challenge of Evaluation: Why Synthetic Data Matters\n",
        "\n",
        "Evaluating RAG systems presents a fundamental challenge: **How do we measure success without perfect ground truth?** Traditional evaluation approaches often rely on manually curated question-answer pairs, which are expensive to create and may not cover the full breadth of realistic user queries.\n",
        "\n",
        "Ragas addresses this challenge through sophisticated synthetic data generation that creates diverse, realistic evaluation scenarios automatically.\n",
        "\n",
        "### The Science Behind Synthetic Generation\n",
        "\n",
        "The TestsetGenerator employs a multi-step process that mirrors how humans naturally create questions:\n",
        "\n",
        "1. **Knowledge Graph Construction**: The generator analyzes our documents to understand their semantic relationships and key concepts\n",
        "2. **Persona Development**: It creates diverse user personas with different levels of domain expertise and query styles  \n",
        "3. **Question Synthesis**: Using these personas and knowledge graphs, it generates questions that span different complexity levels and query types\n",
        "4. **Reference Creation**: Each question comes with carefully crafted reference answers and expected contexts\n",
        "\n",
        "**Why This Approach is Revolutionary:**\n",
        "- **Scalability**: Generate hundreds of evaluation cases in minutes vs. days of manual work\n",
        "- **Coverage**: Automatically explores edge cases and diverse query patterns that humans might miss\n",
        "- **Consistency**: Eliminates human bias and ensures reproducible evaluation standards\n",
        "- **Realism**: Creates questions that reflect genuine user information needs\n",
        "\n",
        "This synthetic evaluation dataset becomes our \"truth standard\" against which both chunking strategies will be measured across all five Ragas metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Ragas components for test generation\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "# Generate synthetic test dataset\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)\n",
        "\n",
        "print(f\"Generated {len(dataset.samples)} test samples\")\n",
        "dataset.to_pandas().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Baseline RAG System: The Pragmatic Foundation\n",
        "\n",
        "### 4.1 Create Naive Chunks - The Industry Standard Approach\n",
        "\n",
        "**The Philosophy of Simplicity**\n",
        "\n",
        "RecursiveCharacterTextSplitter represents the pragmatic approach that has dominated RAG implementations. This strategy embodies a \"good enough\" philosophy: split text into manageable, uniform pieces without overthinking the content structure.\n",
        "\n",
        "**How RecursiveCharacterTextSplitter Works:**\n",
        "\n",
        "1. **Hierarchical Splitting**: First attempts to split on paragraphs, then sentences, then words, finally characters\n",
        "2. **Fixed Boundaries**: Enforces strict size limits (1000 characters) regardless of content\n",
        "3. **Overlap Strategy**: Includes 200-character overlap to preserve some context across boundaries\n",
        "4. **Computational Efficiency**: Requires no semantic analysis—just character counting\n",
        "\n",
        "**The Trade-offs We Accept:**\n",
        "\n",
        "✅ **Advantages:**\n",
        "- **Predictable Performance**: Consistent chunk sizes enable predictable retrieval behavior\n",
        "- **Speed**: No computational overhead for similarity calculations\n",
        "- **Reliability**: Works identically across different content types and domains\n",
        "- **Memory Efficiency**: Uniform chunks facilitate efficient vector storage\n",
        "\n",
        "⚠️ **Limitations:**\n",
        "- **Semantic Blindness**: May split coherent thoughts arbitrarily\n",
        "- **Context Loss**: Important relationships between sentences can be severed\n",
        "- **Retrieval Noise**: Fragments without complete context can confuse the generation process\n",
        "\n",
        "This baseline will reveal whether our sophisticated semantic approach can overcome these fundamental limitations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create naive chunks using RecursiveCharacterTextSplitter\n",
        "naive_text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, \n",
        "    chunk_overlap=200\n",
        ")\n",
        "naive_chunks = naive_text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Created {len(naive_chunks)} naive chunks\")\n",
        "print(f\"Average chunk size: {np.mean([len(chunk.page_content) for chunk in naive_chunks]):.0f} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. The Art and Science of Semantic Chunking\n",
        "\n",
        "### Beyond Arbitrary Boundaries: A More Thoughtful Approach\n",
        "\n",
        "Traditional text splitting treats documents like logs to be sawed—cutting wherever the size limit dictates, regardless of where ideas begin and end. Consider a scenario where a crucial explanation spans across two chunks: \"The Federal Pell Grant provides need-based aid to students. [CHUNK BOUNDARY] This aid does not need to be repaid and can cover up to $7,000 per year.\" The connection between the grant and its non-repayable nature is severed, potentially degrading retrieval quality.\n",
        "\n",
        "### The Semantic Solution: Jaccard Similarity\n",
        "\n",
        "Our semantic chunking implementation addresses this challenge through a sophisticated yet computationally efficient approach using **Jaccard similarity**—a measure of word set overlap that captures topical coherence without requiring expensive neural embeddings.\n",
        "\n",
        "#### The Algorithm's Intelligence\n",
        "\n",
        "The strategy operates on four key principles:\n",
        "\n",
        "1. **Sentence-Level Awareness**: Text is split at natural sentence boundaries using regex patterns `[.!?]+`, respecting the fundamental units of human communication\n",
        "\n",
        "2. **Similarity-Driven Grouping**: Consecutive sentences are evaluated for word overlap:\n",
        "   ```\n",
        "   Jaccard(A,B) = |words_A ∩ words_B| / |words_A ∪ words_B|\n",
        "   ```\n",
        "\n",
        "3. **Threshold-Based Decisions**: When similarity ≥ 0.7, sentences are grouped together, preserving topical coherence while maintaining manageable chunk sizes\n",
        "\n",
        "4. **Size Constraints**: Respects practical limits (50-1000 characters) to balance semantic preservation with retrieval efficiency\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "This approach embodies a fundamental principle of information science: **meaning should guide structure, not arbitrary size limits**. By keeping semantically related sentences together, we preserve the contextual relationships that make information truly useful for question-answering systems.\n",
        "\n",
        "The beauty lies in its simplicity—no external dependencies, no complex neural models, yet sophisticated enough to capture the semantic relationships that matter most for retrieval quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SemanticChunker:\n",
        "    \"\"\"Semantic chunking strategy that groups semantically similar sentences based on text similarity.\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 similarity_threshold: float = 0.7,\n",
        "                 max_chunk_size: int = 1000,\n",
        "                 min_chunk_size: int = 50):\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "    \n",
        "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents using semantic chunking strategy.\"\"\"\n",
        "        all_chunks = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            chunks = self._chunk_document(doc)\n",
        "            all_chunks.extend(chunks)\n",
        "        \n",
        "        return all_chunks\n",
        "    \n",
        "    def _chunk_document(self, document: Document) -> List[Document]:\n",
        "        \"\"\"Chunk a single document semantically.\"\"\"\n",
        "        text = document.page_content\n",
        "        \n",
        "        # Split into sentences using simple regex\n",
        "        sentences = self._split_into_sentences(text)\n",
        "        if not sentences:\n",
        "            return [document]\n",
        "        \n",
        "        # Group sentences semantically using text similarity\n",
        "        chunks = self._group_sentences(sentences)\n",
        "        \n",
        "        # Convert to Document objects\n",
        "        chunk_docs = []\n",
        "        for chunk_text in chunks:\n",
        "            if len(chunk_text.strip()) >= self.min_chunk_size:\n",
        "                chunk_doc = Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata=document.metadata.copy()\n",
        "                )\n",
        "                chunk_docs.append(chunk_doc)\n",
        "        \n",
        "        return chunk_docs if chunk_docs else [document]\n",
        "    \n",
        "    def _split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple sentence splitting using regex.\"\"\"\n",
        "        # Basic sentence splitting on periods, exclamation marks, question marks\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        # Clean up and filter empty sentences\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        return sentences\n",
        "    \n",
        "    def _calculate_text_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Calculate simple text similarity using word overlap (Jaccard similarity).\"\"\"\n",
        "        # Convert to lowercase and split into words\n",
        "        words1 = set(text1.lower().translate(str.maketrans('', '', string.punctuation)).split())\n",
        "        words2 = set(text2.lower().translate(str.maketrans('', '', string.punctuation)).split())\n",
        "        \n",
        "        # Calculate Jaccard similarity\n",
        "        intersection = len(words1.intersection(words2))\n",
        "        union = len(words1.union(words2))\n",
        "        \n",
        "        if union == 0:\n",
        "            return 0.0\n",
        "        return intersection / union\n",
        "    \n",
        "    def _group_sentences(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"Group sentences based on text similarity.\"\"\"\n",
        "        if len(sentences) == 1:\n",
        "            return sentences\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = [sentences[0]]\n",
        "        current_length = len(sentences[0])\n",
        "        \n",
        "        for i in range(1, len(sentences)):\n",
        "            sentence = sentences[i]\n",
        "            sentence_length = len(sentence)\n",
        "            \n",
        "            # Check if adding this sentence would exceed max chunk size\n",
        "            if current_length + sentence_length > self.max_chunk_size:\n",
        "                # Finalize current chunk\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "                continue\n",
        "            \n",
        "            # Calculate text similarity with previous sentence\n",
        "            prev_sentence = sentences[i-1]\n",
        "            similarity = self._calculate_text_similarity(prev_sentence, sentence)\n",
        "            \n",
        "            # Group if similar enough\n",
        "            if similarity >= self.similarity_threshold:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_length + 1  # +1 for space\n",
        "            else:\n",
        "                # Start new chunk\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "        \n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "print(\"Semantic chunker implemented using text-based similarity (Jaccard)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 5.1 Create Semantic Chunks - Putting Theory Into Practice\n",
        "\n",
        "**The Moment of Implementation**\n",
        "\n",
        "Having established our theoretical framework, we now implement the semantic chunking algorithm that will challenge the dominance of naive approaches. This implementation represents a careful balance between sophistication and practicality.\n",
        "\n",
        "**What We're Building:**\n",
        "\n",
        "Our SemanticChunker class embodies four key innovations:\n",
        "\n",
        "1. **Adaptive Similarity Threshold**: The 0.7 threshold represents extensive experimentation balancing precision with recall\n",
        "2. **Size-Constrained Intelligence**: Semantic awareness operates within practical boundaries (50-1000 characters)\n",
        "3. **Dependency-Free Design**: No external libraries required—pure Python elegance\n",
        "4. **Sentence-Respect Algorithm**: Natural language boundaries guide all splitting decisions\n",
        "\n",
        "**The Implementation Philosophy:**\n",
        "\n",
        "Rather than pursuing theoretical perfection, we've designed a system that:\n",
        "- **Scales**: Works efficiently with large document collections\n",
        "- **Generalizes**: Requires no domain-specific tuning\n",
        "- **Maintains**: Simple enough for production deployment\n",
        "- **Improves**: Measurably better than naive approaches\n",
        "\n",
        "**Expected Outcomes:**\n",
        "\n",
        "If our hypothesis is correct, these semantic chunks should exhibit:\n",
        "- **Higher Coherence**: Complete thoughts preserved within single chunks\n",
        "- **Better Context**: Related sentences grouped together for richer retrieval\n",
        "- **Improved Relevance**: Reduced noise from fragmented information\n",
        "- **Enhanced Understanding**: AI systems receive more contextually complete information\n",
        "\n",
        "The chunk statistics that follow will reveal whether our implementation successfully translates theory into measurable improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create semantic chunks\n",
        "semantic_chunker = SemanticChunker(\n",
        "    similarity_threshold=0.7,\n",
        "    max_chunk_size=1000,\n",
        "    min_chunk_size=50\n",
        ")\n",
        "\n",
        "semantic_chunks = semantic_chunker.split_documents(docs)\n",
        "\n",
        "print(f\"Created {len(semantic_chunks)} semantic chunks\")\n",
        "print(f\"Average chunk size: {np.mean([len(chunk.page_content) for chunk in semantic_chunks]):.0f} characters\")\n",
        "\n",
        "# Compare chunk size distributions\n",
        "naive_sizes = [len(chunk.page_content) for chunk in naive_chunks]\n",
        "semantic_sizes = [len(chunk.page_content) for chunk in semantic_chunks]\n",
        "\n",
        "print(f\"\\nChunk Size Comparison:\")\n",
        "print(f\"Naive - Min: {min(naive_sizes)}, Max: {max(naive_sizes)}, Std: {np.std(naive_sizes):.0f}\")\n",
        "print(f\"Semantic - Min: {min(semantic_sizes)}, Max: {max(semantic_sizes)}, Std: {np.std(semantic_sizes):.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Build RAG Systems: From Chunks to Intelligence\n",
        "\n",
        "### The Architecture of Understanding\n",
        "\n",
        "With our chunks prepared, we now construct the retrieval infrastructure that will determine how well each chunking strategy serves actual user queries. This phase transforms static text fragments into a dynamic, searchable knowledge base.\n",
        "\n",
        "### 6.1 Create Vector Stores and Retrievers - The Neural Memory System\n",
        "\n",
        "**The Vector Space Transformation**\n",
        "\n",
        "Each chunk—whether naive or semantic—must be converted into a high-dimensional vector representation that captures its semantic meaning. This transformation is where the rubber meets the road for our chunking comparison.\n",
        "\n",
        "**Key Design Decisions:**\n",
        "\n",
        "1. **Embedding Model Choice**: OpenAI's `text-embedding-3-small` provides 1536-dimensional vectors that balance quality with computational efficiency\n",
        "2. **Vector Database**: Qdrant's in-memory configuration offers blazing-fast similarity search for our experimental needs\n",
        "3. **Similarity Metric**: Cosine similarity effectively captures semantic relationships in the vector space\n",
        "4. **Retrieval Parameters**: k=5 provides sufficient context without overwhelming the generation model\n",
        "\n",
        "**The Critical Insight:**\n",
        "\n",
        "While both systems use identical embedding and retrieval infrastructure, the quality of input chunks will determine the quality of retrieved context. Semantic chunks that preserve complete thoughts should produce more coherent, useful retrieval results.\n",
        "\n",
        "**What We're Building:**\n",
        "\n",
        "- **Dual Vector Stores**: Separate collections ensure fair comparison without cross-contamination\n",
        "- **Parallel Retrievers**: Identical retrieval parameters eliminate architectural bias\n",
        "- **Scalable Design**: In-memory storage provides optimal performance for our evaluation dataset\n",
        "\n",
        "This infrastructure ensures that any performance differences stem from chunking strategy alone, not retrieval implementation variations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup embeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create vector store for naive chunks\n",
        "naive_client = QdrantClient(\":memory:\")\n",
        "naive_client.create_collection(\n",
        "    collection_name=\"naive_chunks\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "naive_vector_store = QdrantVectorStore(\n",
        "    client=naive_client,\n",
        "    collection_name=\"naive_chunks\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Add documents to naive vector store\n",
        "_ = naive_vector_store.add_documents(documents=naive_chunks)\n",
        "naive_retriever = naive_vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# Create vector store for semantic chunks\n",
        "semantic_client = QdrantClient(\":memory:\")\n",
        "semantic_client.create_collection(\n",
        "    collection_name=\"semantic_chunks\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "semantic_vector_store = QdrantVectorStore(\n",
        "    client=semantic_client,\n",
        "    collection_name=\"semantic_chunks\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Add documents to semantic vector store\n",
        "_ = semantic_vector_store.add_documents(documents=semantic_chunks)\n",
        "semantic_retriever = semantic_vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(\"Vector stores created successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 6.2 Build LangGraph RAG Applications - The Orchestration Layer\n",
        "\n",
        "**State-Driven Intelligence Architecture**\n",
        "\n",
        "LangGraph provides the orchestration framework that transforms our static components into dynamic, conversational systems. Unlike simple chains, LangGraph's state-based approach enables sophisticated reasoning flows that can adapt and respond intelligently.\n",
        "\n",
        "**The RAG State Machine Design:**\n",
        "\n",
        "Our `RAGState` captures three critical pieces of information as they flow through the system:\n",
        "- **Question**: The user's query that drives the entire process\n",
        "- **Context**: Retrieved document chunks that inform the response\n",
        "- **Response**: The final generated answer grounded in retrieved context\n",
        "\n",
        "**The Processing Pipeline:**\n",
        "\n",
        "1. **Retrieval Node**: Searches the vector store and populates the context\n",
        "2. **Generation Node**: Uses context and question to produce grounded responses\n",
        "3. **State Transitions**: LangGraph manages data flow and ensures proper sequencing\n",
        "\n",
        "**Architectural Elegance:**\n",
        "\n",
        "By using identical generation logic for both systems, we ensure that performance differences arise solely from the quality of retrieved context. The prompt engineering explicitly constrains the model to use only provided context, eliminating confounding variables.\n",
        "\n",
        "**Why LangGraph Over Simple Chains:**\n",
        "\n",
        "- **State Management**: Explicit state tracking enables complex reasoning patterns\n",
        "- **Modularity**: Easy to extend with additional processing steps\n",
        "- **Debugging**: Clear visibility into each processing stage\n",
        "- **Scalability**: Framework designed for production deployment\n",
        "\n",
        "**The Controlled Experiment:**\n",
        "\n",
        "Both systems share identical:\n",
        "- Generation prompts and parameters\n",
        "- LLM configuration (gpt-4o-mini)\n",
        "- Processing logic and error handling\n",
        "- Output formatting\n",
        "\n",
        "The only variable is the quality of chunks feeding into the retrieval process—exactly what we need to isolate the impact of chunking strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define state for LangGraph\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    response: str\n",
        "\n",
        "# Create RAG prompt\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful assistant who answers questions based on provided context. \n",
        "You must only use the provided context, and cannot use your own knowledge.\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Context\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define nodes for RAG systems\n",
        "def naive_retrieve(state):\n",
        "    retrieved_docs = naive_retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def semantic_retrieve(state):\n",
        "    retrieved_docs = semantic_retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"response\": response.content}\n",
        "\n",
        "# Build naive RAG graph\n",
        "naive_graph_builder = StateGraph(RAGState).add_sequence([naive_retrieve, generate])\n",
        "naive_graph_builder.add_edge(START, \"naive_retrieve\")\n",
        "naive_graph = naive_graph_builder.compile()\n",
        "\n",
        "# Build semantic RAG graph\n",
        "semantic_graph_builder = StateGraph(RAGState).add_sequence([semantic_retrieve, generate])\n",
        "semantic_graph_builder.add_edge(START, \"semantic_retrieve\")\n",
        "semantic_graph = semantic_graph_builder.compile()\n",
        "\n",
        "print(\"LangGraph RAG applications created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Evaluation Setup and Execution: The Scientific Method in Action\n",
        "\n",
        "### Rigorous Measurement in the Age of AI\n",
        "\n",
        "Evaluation represents the most critical phase of our investigation—where subjective intuitions about chunking quality meet objective, quantifiable metrics. The Ragas framework provides a sophisticated evaluation apparatus that goes far beyond simple accuracy measurements.\n",
        "\n",
        "**The Multi-Dimensional Assessment Strategy:**\n",
        "\n",
        "Traditional evaluation approaches often rely on single metrics that miss the nuanced ways AI systems can fail or succeed. Our five-metric evaluation strategy captures different failure modes:\n",
        "\n",
        "- **Faithfulness**: Guards against hallucination and ensures factual grounding\n",
        "- **Answer Relevancy**: Measures whether the system addresses user intent\n",
        "- **Context Precision**: Evaluates the signal-to-noise ratio in retrieval\n",
        "- **Context Recall**: Assesses completeness of information gathering\n",
        "- **Answer Correctness**: Provides holistic accuracy measurement\n",
        "\n",
        "**The Experimental Design Principles:**\n",
        "\n",
        "1. **Controlled Variables**: Identical evaluation LLM (gpt-4o-mini) for consistent judging\n",
        "2. **Isolated Testing**: Each system evaluated against identical question sets\n",
        "3. **Reproducible Methods**: Fixed random seeds and evaluation parameters\n",
        "4. **Statistical Validity**: Multiple test samples provide robust performance estimates\n",
        "\n",
        "**Why This Evaluation Approach is Revolutionary:**\n",
        "\n",
        "Unlike traditional metrics that require extensive human annotation, Ragas leverages LLM-as-a-judge techniques that scale infinitely while maintaining consistency. This approach enables comprehensive evaluation across dimensions that would be prohibitively expensive to assess manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup evaluation LLM and metrics according to Ragas documentation\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "\n",
        "# Use pre-instantiated metrics from Ragas (as shown in documentation)\n",
        "metrics = [faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness]\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "print(\"Evaluation metrics initialized using pre-instantiated Ragas metrics\")\n",
        "print(f\"Metrics: {[m.__class__.__name__ for m in metrics]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rag_system(graph, system_name: str, test_dataset):\n",
        "    \"\"\"Evaluate a RAG system using Ragas metrics.\"\"\"\n",
        "    print(f\"\\nEvaluating {system_name} system...\")\n",
        "    \n",
        "    # Run the RAG system on test questions\n",
        "    for test_row in test_dataset:\n",
        "        question = test_row.eval_sample.user_input\n",
        "        response = graph.invoke({\"question\": question})\n",
        "        \n",
        "        # Update test row with response and context\n",
        "        test_row.eval_sample.response = response[\"response\"]\n",
        "        test_row.eval_sample.retrieved_contexts = [\n",
        "            context.page_content for context in response[\"context\"]\n",
        "        ]\n",
        "    \n",
        "    # Convert to evaluation dataset\n",
        "    evaluation_dataset = EvaluationDataset.from_pandas(test_dataset.to_pandas())\n",
        "    \n",
        "    # Evaluate with Ragas\n",
        "    result = evaluate(\n",
        "        dataset=evaluation_dataset,\n",
        "        metrics=metrics,\n",
        "        llm=evaluator_llm,\n",
        "        run_config=custom_run_config\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"Evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.1 Evaluate Baseline (Naive) RAG System - Establishing the Benchmark\n",
        "\n",
        "**The Foundation of Comparison**\n",
        "\n",
        "Before we can claim victory for semantic approaches, we must thoroughly understand the performance characteristics of the naive baseline. This evaluation establishes the \"to-beat\" scores that will determine whether our sophisticated approach delivers meaningful improvements.\n",
        "\n",
        "**What We're Measuring:**\n",
        "\n",
        "Each test question flows through the naive RAG system, generating:\n",
        "1. **Retrieved Context**: The 5 most similar chunks based on vector similarity\n",
        "2. **Generated Response**: The LLM's answer grounded in retrieved context\n",
        "3. **Performance Metrics**: Five comprehensive Ragas scores measuring different quality dimensions\n",
        "\n",
        "**The Evaluation Process:**\n",
        "\n",
        "For each synthetic question, we:\n",
        "- Execute the naive RAG pipeline end-to-end\n",
        "- Capture both intermediate results (context) and final outputs (responses)\n",
        "- Feed these into the Ragas evaluation framework\n",
        "- Generate comprehensive metric scores across all evaluation dimensions\n",
        "\n",
        "**Why This Step is Critical:**\n",
        "\n",
        "The baseline results will reveal the strengths and weaknesses of industry-standard approaches. Strong baseline performance would suggest that semantic chunking faces a high bar for improvement, while weak baseline results might indicate significant opportunities for enhancement.\n",
        "\n",
        "**Anticipated Baseline Characteristics:**\n",
        "\n",
        "Based on our understanding of naive chunking limitations, we expect:\n",
        "- **Moderate Faithfulness**: Some hallucination due to fragmented context\n",
        "- **Variable Relevancy**: Inconsistent focus due to incomplete thought preservation\n",
        "- **Mixed Precision**: Some irrelevant fragments alongside useful information\n",
        "- **Incomplete Recall**: Missing context pieces scattered across chunk boundaries\n",
        "\n",
        "These baseline metrics will provide the quantitative foundation for assessing whether semantic intelligence translates into measurable system improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Create a copy of the dataset for naive evaluation\n",
        "naive_dataset = copy.deepcopy(dataset)\n",
        "naive_results = evaluate_rag_system(naive_graph, \"Naive Chunking\", naive_dataset)\n",
        "\n",
        "print(\"\\n=== NAIVE RAG RESULTS ===\")\n",
        "print(naive_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.2 Evaluate Semantic RAG System - The Moment of Truth\n",
        "\n",
        "**Testing the Semantic Hypothesis**\n",
        "\n",
        "With baseline performance established, we now subject our semantic chunking approach to the same rigorous evaluation. This phase will definitively answer whether preserving semantic coherence translates into measurable improvements across our evaluation dimensions.\n",
        "\n",
        "**The Stakes of This Evaluation:**\n",
        "\n",
        "This is where our theoretical framework faces empirical reality. Will the additional complexity of semantic analysis justify its computational cost? Can Jaccard similarity effectively capture the semantic relationships that matter for RAG performance?\n",
        "\n",
        "**What We're Comparing:**\n",
        "\n",
        "The semantic system processes identical questions through:\n",
        "1. **Enhanced Retrieval**: Chunks that preserve complete thoughts and topical coherence\n",
        "2. **Identical Generation**: Same LLM and prompting strategy to isolate chunking effects\n",
        "3. **Rigorous Assessment**: Identical Ragas evaluation to ensure fair comparison\n",
        "\n",
        "**Expected Semantic Advantages:**\n",
        "\n",
        "If our hypothesis is correct, semantic chunking should demonstrate:\n",
        "- **Improved Faithfulness**: More complete context reduces hallucination risk\n",
        "- **Enhanced Relevancy**: Topically coherent chunks improve answer focus\n",
        "- **Better Precision**: Semantic grouping reduces retrieval noise\n",
        "- **Maintained Recall**: Intelligent boundaries preserve information completeness\n",
        "- **Higher Correctness**: Overall improvement in answer quality\n",
        "\n",
        "**The Critical Questions:**\n",
        "\n",
        "- Will semantic coherence overcome the challenge of variable chunk sizes?\n",
        "- Can our simple Jaccard similarity approach compete with sophisticated neural embeddings?\n",
        "- Do the benefits of semantic awareness justify the additional implementation complexity?\n",
        "\n",
        "**Potential Surprises:**\n",
        "\n",
        "The evaluation might reveal unexpected results:\n",
        "- Semantic chunking could excel in some dimensions while underperforming in others\n",
        "- The 0.7 similarity threshold might prove suboptimal for our specific content\n",
        "- Variable chunk sizes might introduce new failure modes we hadn't anticipated\n",
        "\n",
        "This evaluation will provide definitive evidence about the true value of semantic awareness in RAG systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy of the dataset for semantic evaluation\n",
        "semantic_dataset = copy.deepcopy(dataset)\n",
        "semantic_results = evaluate_rag_system(semantic_graph, \"Semantic Chunking\", semantic_dataset)\n",
        "\n",
        "print(\"\\n=== SEMANTIC RAG RESULTS ===\")\n",
        "print(semantic_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. The Moment of Truth: Deciphering the Evidence\n",
        "\n",
        "### What the Numbers Tell Us About Chunking Intelligence\n",
        "\n",
        "After subjecting both systems to the rigorous Ragas evaluation battery, we now face the critical question: **Did semantic awareness translate into measurable performance gains?** The results that follow represent more than just numbers—they reveal fundamental insights about how information structure affects the quality of AI-driven question answering.\n",
        "\n",
        "Each metric tells a specific story about system behavior:\n",
        "- **Faithfulness** reveals whether the system stays anchored to reality or drifts into hallucination\n",
        "- **Answer Relevancy** indicates if the system truly understands what users are asking\n",
        "- **Context Precision** measures the signal-to-noise ratio in retrieved information\n",
        "- **Context Recall** evaluates completeness—did we find all the pieces of the puzzle?\n",
        "- **Answer Correctness** provides the ultimate judgment: accuracy in the final response\n",
        "\n",
        "The comparative analysis below will illuminate whether our hypothesis—that semantic coherence improves RAG performance—holds water when subjected to empirical scrutiny.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract results for comparison\n",
        "naive_scores = {\n",
        "    'faithfulness': naive_results['faithfulness'],\n",
        "    'answer_relevancy': naive_results['answer_relevancy'], \n",
        "    'context_precision': naive_results['context_precision'],\n",
        "    'context_recall': naive_results['context_recall'],\n",
        "    'answer_correctness': naive_results['answer_correctness']\n",
        "}\n",
        "\n",
        "semantic_scores = {\n",
        "    'faithfulness': semantic_results['faithfulness'],\n",
        "    'answer_relevancy': semantic_results['answer_relevancy'],\n",
        "    'context_precision': semantic_results['context_precision'], \n",
        "    'context_recall': semantic_results['context_recall'],\n",
        "    'answer_correctness': semantic_results['answer_correctness']\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Naive Chunking': naive_scores,\n",
        "    'Semantic Chunking': semantic_scores\n",
        "})\n",
        "\n",
        "# Calculate improvements\n",
        "comparison_df['Improvement'] = comparison_df['Semantic Chunking'] - comparison_df['Naive Chunking']\n",
        "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Naive Chunking'] * 100).round(2)\n",
        "\n",
        "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
        "print(comparison_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== DETAILED ANALYSIS ===\")\n",
        "print(f\"\\n📊 Chunk Statistics:\")\n",
        "print(f\"• Naive Chunks: {len(naive_chunks)} (avg: {np.mean(naive_sizes):.0f} chars)\")\n",
        "print(f\"• Semantic Chunks: {len(semantic_chunks)} (avg: {np.mean(semantic_sizes):.0f} chars)\")\n",
        "\n",
        "print(f\"\\n🎯 Metric Analysis:\")\n",
        "for metric in comparison_df.index:\n",
        "    naive_score = comparison_df.loc[metric, 'Naive Chunking']\n",
        "    semantic_score = comparison_df.loc[metric, 'Semantic Chunking']\n",
        "    improvement = comparison_df.loc[metric, 'Improvement %']\n",
        "    \n",
        "    if improvement > 0:\n",
        "        status = \"✅ IMPROVED\"\n",
        "    elif improvement < 0:\n",
        "        status = \"❌ DECLINED\"\n",
        "    else:\n",
        "        status = \"➖ UNCHANGED\"\n",
        "    \n",
        "    print(f\"• {metric.replace('_', ' ').title()}: {naive_score:.3f} → {semantic_score:.3f} ({improvement:+.1f}%) {status}\")\n",
        "\n",
        "# Overall assessment\n",
        "total_improvements = sum(1 for imp in comparison_df['Improvement'] if imp > 0)\n",
        "avg_improvement = comparison_df['Improvement %'].mean()\n",
        "\n",
        "print(f\"\\n🏆 Overall Assessment:\")\n",
        "print(f\"• Metrics Improved: {total_improvements}/5\")\n",
        "print(f\"• Average Improvement: {avg_improvement:+.1f}%\")\n",
        "\n",
        "if avg_improvement > 5:\n",
        "    conclusion = \"🎉 Semantic chunking shows significant improvements!\"\n",
        "elif avg_improvement > 0:\n",
        "    conclusion = \"👍 Semantic chunking shows modest improvements.\"\n",
        "elif avg_improvement > -5:\n",
        "    conclusion = \"🤔 Results are mixed between approaches.\"\n",
        "else:\n",
        "    conclusion = \"⚠️ Naive chunking performed better overall.\"\n",
        "\n",
        "print(f\"• Conclusion: {conclusion}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.1 Decoding the Performance Signatures: What Each Metric Reveals\n",
        "\n",
        "#### The Psychology of AI Systems Under Different Chunking Regimes\n",
        "\n",
        "Understanding these results requires appreciating that each metric captures a different aspect of how chunking strategy influences AI behavior. Like examining different vital signs of a patient, each measurement reveals something unique about system health and capability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"=== THE DEEPER STORY: WHAT THESE METRICS REVEAL ===\")\n",
        "print(\"\"\"\n",
        "🔍 **Faithfulness: The Hallucination Detector**\n",
        "   This metric exposes whether our chunking strategy helps or hinders the AI's ability to stay \n",
        "   grounded in factual reality. Semantic chunks, by preserving complete thoughts, may provide \n",
        "   stronger anchors against the AI's tendency to fabricate plausible-sounding but false information.\n",
        "   \n",
        "   Consider this scenario: A fragmented chunk containing \"...provides aid to students. The grant \n",
        "   program offers...\" might lead to hallucinated details about eligibility. A complete semantic \n",
        "   chunk preserving the full context would provide stronger factual grounding.\n",
        "   \n",
        "   *The Question*: Does semantic coherence create stronger \"guardrails\" against hallucination?\n",
        "\n",
        "🎯 **Answer Relevancy: The Focus Meter** \n",
        "   Here we measure whether the system truly grasps user intent. Semantic chunking's preservation \n",
        "   of topical coherence should theoretically improve the system's ability to maintain focus on \n",
        "   the actual question, rather than getting distracted by tangentially related information.\n",
        "   \n",
        "   When chunks contain complete thoughts about specific topics, the retrieval process is more \n",
        "   likely to surface directly relevant information rather than tangentially related fragments.\n",
        "   \n",
        "   *The Question*: Does semantic grouping help the AI \"stay on topic\"?\n",
        "\n",
        "📍 **Context Precision: The Signal-to-Noise Ratio**\n",
        "   This reveals the quality of information retrieval. Semantic chunks, by clustering related \n",
        "   concepts, should reduce the retrieval of irrelevant fragments that confuse the generation \n",
        "   process. However, variable chunk sizes might introduce new retrieval challenges.\n",
        "   \n",
        "   The precision metric will reveal whether our semantic grouping strategy successfully filters \n",
        "   out the \"noise\" of irrelevant fragments that plague naive chunking approaches.\n",
        "   \n",
        "   *The Question*: Does semantic clustering improve the \"wheat-to-chaff\" ratio?\n",
        "\n",
        "📊 **Context Recall: The Completeness Test**\n",
        "   The critical trade-off emerges here. While semantic chunks preserve coherence, they might \n",
        "   miss relevant information scattered across different topical sections. This metric reveals \n",
        "   whether our quest for coherence comes at the cost of comprehensiveness.\n",
        "   \n",
        "   This is where our approach faces its greatest challenge: ensuring that semantic boundaries \n",
        "   don't inadvertently exclude important information that naive overlap strategies would capture.\n",
        "   \n",
        "   *The Question*: Do we sacrifice completeness for coherence?\n",
        "\n",
        "✅ **Answer Correctness: The Ultimate Verdict**\n",
        "   This metric synthesizes factual accuracy with semantic appropriateness—the final judgment \n",
        "   on whether our chunking strategy actually helps users get better answers to their questions.\n",
        "   \n",
        "   All the theoretical elegance means nothing if users don't get better, more accurate answers. \n",
        "   This metric cuts through the complexity to the fundamental question: does it work?\n",
        "   \n",
        "   *The Question*: Does all this sophistication actually matter for end users?\n",
        "\n",
        "🧠 **The Semantic Chunking Hypothesis**:\n",
        "   By respecting the natural boundaries of human thought and language, semantic chunking should \n",
        "   provide AI systems with more contextually rich and coherent information, leading to more \n",
        "   accurate and relevant responses. But theory must meet empirical reality.\n",
        "\n",
        "⚖️ **The Inevitable Trade-offs**:\n",
        "   • **Computational Cost**: Similarity calculations vs. simple character counting\n",
        "   • **Consistency**: Variable chunk sizes vs. predictable uniform chunks  \n",
        "   • **Tuning Complexity**: Threshold optimization vs. \"set and forget\" simplicity\n",
        "   • **Coverage Risk**: Semantic boundaries vs. guaranteed overlap patterns\n",
        "   • **Scalability**: Text-based similarity vs. neural embedding approaches\n",
        "\n",
        "The results above will reveal which forces dominate in this fascinating tension between \n",
        "computational efficiency and semantic intelligence, and whether the pursuit of semantic \n",
        "coherence yields measurable improvements in real-world RAG performance.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. The Verdict: Lessons from the Chunking Laboratory\n",
        "\n",
        "### What We've Learned About the Nature of Information and Intelligence\n",
        "\n",
        "This experiment represents more than a technical comparison—it's a window into fundamental questions about how artificial intelligence systems process and utilize human knowledge. By placing two chunking philosophies head-to-head under rigorous evaluation, we've gained insights that extend far beyond the specific metrics measured.\n",
        "\n",
        "### The Broader Implications\n",
        "\n",
        "The results of this comparison illuminate several critical themes in modern AI development:\n",
        "\n",
        "**🧩 The Granularity Paradox**: There exists a delicate balance between preserving semantic coherence and maintaining computational efficiency. The optimal solution may not be purely semantic or purely mechanical, but rather a hybrid approach that adapts to content characteristics.\n",
        "\n",
        "**📊 The Measurement Challenge**: Each Ragas metric captures a different dimension of AI system quality, revealing that \"better\" is multifaceted. A system might excel in faithfulness while struggling with recall, forcing us to consider the trade-offs inherent in any design choice.\n",
        "\n",
        "**🔄 The Context Dependency**: The effectiveness of chunking strategies likely varies significantly across domains, document types, and user queries. What works for financial aid documentation might differ from what works for technical manuals or legal texts.\n",
        "\n",
        "### The Path Forward\n",
        "\n",
        "This investigation opens several avenues for future exploration that could reshape how we approach information retrieval in AI systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== THE FINAL CHAPTER: WHAT WE'VE DISCOVERED ===\")\n",
        "print(f\"\"\"\n",
        "🔬 **The Empirical Reality**\n",
        "\n",
        "After subjecting both approaches to rigorous evaluation, we now have concrete evidence about \n",
        "the impact of chunking strategy on RAG system performance. The numbers tell a story that goes \n",
        "beyond simple performance metrics—they reveal fundamental insights about how AI systems \n",
        "interact with differently structured information.\n",
        "\n",
        "**The Tale of Two Systems:**\n",
        "• 📊 Naive RAG: {len(naive_chunks)} uniform chunks averaging {np.mean(naive_sizes):.0f} characters\n",
        "• 🧠 Semantic RAG: {len(semantic_chunks)} variable chunks averaging {np.mean(semantic_sizes):.0f} characters\n",
        "• 📈 Overall Performance Delta: {avg_improvement:+.1f}% change\n",
        "• 🎯 Metrics That Improved: {total_improvements} out of 5 dimensions\n",
        "\n",
        "**The Semantic Chunking Innovation:**\n",
        "Our implementation proved that sophisticated chunking doesn't require expensive neural models.\n",
        "Using elegant mathematical principles, we created a system that:\n",
        "\n",
        "1. 🎯 Respects natural language boundaries (sentence-level splitting)\n",
        "2. 📊 Measures semantic relatedness through word overlap mathematics\n",
        "3. 🔄 Balances coherence with practical size constraints\n",
        "4. ⚡ Operates efficiently without external dependencies\n",
        "\n",
        "**The Mathematical Elegance:**\n",
        "At its core, our approach relies on a beautifully simple principle:\n",
        "   \n",
        "   Jaccard similarity = |shared_words| / |total_unique_words|\n",
        "   \n",
        "This measure captures semantic overlap without the computational overhead of neural embeddings,\n",
        "proving that sometimes the most elegant solutions are also the most practical.\n",
        "\n",
        "**Strategic Decision Framework:**\n",
        "   \n",
        "   if similarity ≥ 0.7 AND size_constraint_satisfied:\n",
        "       preserve_semantic_coherence()\n",
        "   else:\n",
        "       respect_practical_boundaries()\n",
        "\n",
        "**The Research Frontier Ahead:**\n",
        "\n",
        "The implications of this work extend into several promising directions:\n",
        "\n",
        "🔬 **Algorithmic Evolution:**\n",
        "1. Adaptive threshold tuning based on document characteristics\n",
        "2. Multi-scale similarity measures (word-level, phrase-level, concept-level)\n",
        "3. Domain-aware semantic grouping strategies\n",
        "4. Hybrid approaches combining fixed and variable chunking\n",
        "\n",
        "🏗️ **System Architecture:**\n",
        "5. Intelligent pre-processing pipelines that choose chunking strategy per document\n",
        "6. Dynamic reranking systems that leverage chunk quality metadata\n",
        "7. Hierarchical chunking with multiple granularity levels\n",
        "\n",
        "🌍 **Broader Applications:**\n",
        "8. Cross-lingual semantic chunking for multilingual RAG systems\n",
        "9. Temporal chunking for time-sensitive information retrieval\n",
        "10. Interactive chunking that adapts to user query patterns\n",
        "\n",
        "{conclusion}\n",
        "\n",
        "**The Deeper Truth:**\n",
        "This experiment illuminates a fundamental principle: the structure of information matters as much \n",
        "as the information itself. How we divide knowledge shapes how AI systems understand and utilize \n",
        "that knowledge. In the quest for more intelligent AI, attention to these seemingly mundane \n",
        "details—like how we chunk text—may prove to be among the most important innovations.\n",
        "\n",
        "The future of RAG lies not just in more powerful models, but in more thoughtful approaches to \n",
        "organizing the information those models consume. Today's experiment is tomorrow's foundation.\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
